{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/georgelamb/Documents/GitHub/chempropBayes\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import copy\n",
    "import gpytorch\n",
    "\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "from tqdm import trange\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch_geometric.datasets import QM9\n",
    "\n",
    "# cd to chempropBayes\n",
    "%cd /Users/georgelamb/Documents/GitHub/chempropBayes\n",
    "\n",
    "# import from chempropBayes\n",
    "from chemprop.train.evaluate import evaluate, evaluate_predictions\n",
    "from chemprop.train.predict import predict\n",
    "from chemprop.train.train import train\n",
    "from chemprop.args import TrainArgs\n",
    "from chemprop.data import StandardScaler, MoleculeDataLoader\n",
    "from chemprop.data.utils import get_class_sizes, get_data, get_task_names, split_data\n",
    "from chemprop.models import MoleculeModel\n",
    "from chemprop.nn_utils import param_count\n",
    "from chemprop.utils import build_optimizer, build_lr_scheduler, get_loss_func, get_metric_func, load_checkpoint,\\\n",
    "    makedirs, save_checkpoint, save_smiles_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### args\n",
    "\n",
    "# instantiate args class and load from dict\n",
    "args = TrainArgs()\n",
    "args.from_dict({\n",
    "    'dataset_type': 'regression',\n",
    "    'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv'\n",
    "})\n",
    "\n",
    "# location for model checkpoints to be saved\n",
    "args.save_dir = '/Users/georgelamb/Documents/GitHub/chempropBayes/log'\n",
    "\n",
    "### args (non-model)\n",
    "\n",
    "# seed for splitting and loading data\n",
    "args.seed = 0\n",
    "\n",
    "# data\n",
    "args.max_data_size = 10000\n",
    "args.features_path = None\n",
    "args.features_generator = None\n",
    "\n",
    "# splitting data\n",
    "args.split_type = 'random'\n",
    "args.split_sizes = (0.8, 0.1, 0.1)\n",
    "\n",
    "# evaluation metric\n",
    "args.metric = 'mae'\n",
    "\n",
    "### args (model)\n",
    "\n",
    "# seed for random initial weights\n",
    "args.pytorch_seed = 0\n",
    "\n",
    "# message passing\n",
    "args.atom_messages = False\n",
    "args.undirected = False\n",
    "args.bias = False\n",
    "args.hidden_size = 100\n",
    "args.depth = 2\n",
    "\n",
    "# FFN\n",
    "args.ffn_hidden_size = args.hidden_size\n",
    "args.ffn_num_layers = 2\n",
    "\n",
    "# shared\n",
    "args.activation = 'ReLU'\n",
    "\n",
    "# batch size\n",
    "args.batch_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.gp = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9114it [00:00, 43769.76it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 678327.75it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 16695.26it/s]\n"
     ]
    }
   ],
   "source": [
    "logger = None\n",
    "torch.manual_seed(args.pytorch_seed)\n",
    "args.task_names = args.target_columns or get_task_names(args.data_path)\n",
    "data = get_data(path=args.data_path, args=args, logger=logger)\n",
    "args.num_tasks = data.num_tasks()\n",
    "args.features_size = data.features_size()\n",
    "\n",
    "# split data\n",
    "train_data, val_data, test_data = split_data(\n",
    "    data=data, split_type=args.split_type, sizes=args.split_sizes, seed=args.seed, args=args, logger=logger)\n",
    "\n",
    "if args.features_scaling:\n",
    "    features_scaler = train_data.normalize_features(replace_nan_token=0)\n",
    "    val_data.normalize_features(features_scaler)\n",
    "    test_data.normalize_features(features_scaler)\n",
    "else:\n",
    "    features_scaler = None\n",
    "\n",
    "args.train_data_size = len(train_data)\n",
    "\n",
    "if args.dataset_type == 'regression':\n",
    "    train_smiles, train_targets = train_data.smiles(), train_data.targets()\n",
    "    scaler = StandardScaler().fit(train_targets)\n",
    "    scaled_targets = scaler.transform(train_targets).tolist()\n",
    "    train_data.set_targets(scaled_targets)\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "#loss_func = get_loss_func(args)\n",
    "metric_func = get_metric_func(metric=args.metric)\n",
    "\n",
    "# Automatically determine whether to cache\n",
    "if len(data) <= args.cache_cutoff:\n",
    "    cache = True\n",
    "    num_workers = 0\n",
    "else:\n",
    "    cache = False\n",
    "    num_workers = args.num_workers\n",
    "\n",
    "# Create data loaders\n",
    "train_data_loader = MoleculeDataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=num_workers,\n",
    "    cache=cache,\n",
    "    class_balance=args.class_balance,\n",
    "    shuffle=True,\n",
    "    seed=args.seed\n",
    ")\n",
    "val_data_loader = MoleculeDataLoader(\n",
    "        dataset=val_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points, num_dim):\n",
    "        \n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            num_inducing_points = inducing_points.size(-2),\n",
    "            batch_shape = torch.Size([num_dim])\n",
    "        )\n",
    "\n",
    "        \n",
    "        # We have to wrap the VariationalStrategy in a MultitaskVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = gpytorch.variational.MultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, \n",
    "                inducing_points, \n",
    "                variational_distribution, \n",
    "                learn_inducing_locations=True\n",
    "            ), num_tasks=num_dim\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_dim]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_dim])),\n",
    "            batch_shape=torch.Size([num_dim])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    \n",
    "    def __init__(self, feature_extractor, gp_layer):\n",
    "        \n",
    "        super(DKLModel, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.gp_layer = gp_layer\n",
    "\n",
    "    def forward(self, *input):\n",
    "        \n",
    "        features = self.feature_extractor(*input)\n",
    "        res = self.gp_layer(features)\n",
    "        \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [5, 6]],\n",
       "\n",
       "        [[0, 1],\n",
       "         [5, 6]],\n",
       "\n",
       "        [[0, 1],\n",
       "         [5, 6]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0,1],[5,6]])\n",
    "x = x.repeat(3,1,1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2000, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inducing_points = []\n",
    "for batch in train_data_loader:\n",
    "    mol_batch = batch.batch_graph()\n",
    "    inducing_points.extend(feature_extractor(mol_batch))\n",
    "inducing_points = torch.stack(inducing_points)[:2000]\n",
    "inducing_points = inducing_points.repeat(10,1,1)\n",
    "\n",
    "# need to turn this into output x m x input\n",
    "inducing_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = MoleculeModel(args, featurizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of the inducing points should be (2 x m x 2) - so that we learn different inducing points per output\n",
    "torch.manual_seed(0)\n",
    "inducing_points = torch.rand(12, 100, args.hidden_size)\n",
    "gp_layer = MultitaskGPModel(inducing_points, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DKLModel(feature_extractor, gp_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use a multitask likeihood with this model\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3045.948022842407\n",
      "Validation mae = 30.413390\n",
      "3012.2813262939453\n",
      "Validation mae = 30.415102\n",
      "2981.9948749542236\n",
      "Validation mae = 30.413786\n",
      "2954.9243297576904\n",
      "Validation mae = 30.412523\n",
      "2930.8706817626953\n",
      "Validation mae = 30.411033\n",
      "2909.574275970459\n",
      "Validation mae = 30.412925\n",
      "2890.752908706665\n",
      "Validation mae = 30.413056\n",
      "2874.130563735962\n",
      "Validation mae = 30.409716\n",
      "2859.4554080963135\n",
      "Validation mae = 30.410089\n",
      "2846.4586391448975\n",
      "Validation mae = 30.408022\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.gp_layer.hyperparameters()},\n",
    "    {'params': model.gp_layer.variational_parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.001)\n",
    "\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=args.train_data_size)\n",
    "\n",
    "for i in range(10):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    loss_epoch = 0\n",
    "    for batch in train_data_loader:\n",
    "        \n",
    "        mol_batch, target_batch = batch.batch_graph(), batch.targets()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(mol_batch)\n",
    "        loss = -mll(output, torch.tensor(target_batch))\n",
    "        loss_epoch += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss_epoch)\n",
    "    \n",
    "    val_scores = evaluate(\n",
    "                model=model,\n",
    "                data_loader=val_data_loader,\n",
    "                args=args,\n",
    "                num_tasks=args.num_tasks,\n",
    "                metric_func=metric_func,\n",
    "                dataset_type=args.dataset_type,\n",
    "                scaler=scaler,\n",
    "                logger=logger\n",
    "            )\n",
    "    avg_val_score = np.nanmean(val_scores)\n",
    "    print(f'Validation {args.metric} = {avg_val_score:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2905, 1.2743, 1.2754, 1.2665, 1.2683, 1.2724, 1.2689, 1.2703, 1.2766,\n",
       "        1.2766, 1.2766, 1.2766], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood(model(mol_batch)).variance[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1287, 0.1282, 0.1281, 0.1279, 0.1280, 0.1280, 0.1280, 0.1281, 0.1282,\n",
       "        0.1282, 0.1282, 0.1282], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(mol_batch).variance[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6932], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932],\n",
       "        [0.6932]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=12, rank=0, batch_shape=torch.Size([12])).noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultitaskGaussianLikelihood' object has no attribute 'log_noise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultitaskGaussianLikelihood' object has no attribute 'log_noise'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-3583265e01c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 594\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultitaskGaussianLikelihood' object has no attribute 'log_noise'"
     ]
    }
   ],
   "source": [
    "likelihood.log_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
