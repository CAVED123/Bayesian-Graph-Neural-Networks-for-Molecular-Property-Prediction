{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains and evaluates a prototype D-MPNN on QM9 (regression)\n",
    "\n",
    "The code is based on run_training.py\n",
    "\n",
    "D-MPNN settings:\n",
    "- fff\n",
    "- fff\n",
    "- fff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "- Why do only two weight matrices have a bias switch?\n",
    "- Why are lists used rather than numpy arrays for data objects?\n",
    "- Loss function.. are we not dividing by number of molecules twice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate:\n",
    "\n",
    "- Why does the property use_input_features not work? Need to look at how args/parsing works.\n",
    "- only moving onto cuda in message passing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "from tqdm import trange\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch_geometric.datasets import QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/georgelamb/Documents/GitHub/chempropBayes\n"
     ]
    }
   ],
   "source": [
    "# cd to chempropBayes\n",
    "%cd /Users/georgelamb/Documents/GitHub/chempropBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from chempropBayes\n",
    "from chemprop.train.evaluate import evaluate, evaluate_predictions\n",
    "from chemprop.train.predict import predict\n",
    "from chemprop.train.train import train\n",
    "from chemprop.args import TrainArgs\n",
    "from chemprop.data import StandardScaler, MoleculeDataLoader\n",
    "from chemprop.data.utils import get_class_sizes, get_data, get_task_names, split_data\n",
    "from chemprop.models import MoleculeModel\n",
    "from chemprop.nn_utils import param_count\n",
    "from chemprop.utils import build_optimizer, build_lr_scheduler, get_loss_func, get_metric_func, load_checkpoint,\\\n",
    "    makedirs, save_checkpoint, save_smiles_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainArgs()\n",
    "args.from_dict({\n",
    "    'dataset_type': 'regression',\n",
    "    'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv'\n",
    "})\n",
    "logger = None\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pytorch seed for random initial weights\n",
    "args.pytorch_seed = 0\n",
    "torch.manual_seed(args.pytorch_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16209it [00:00, 52220.24it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 620463.76it/s]\n",
      "100%|██████████| 20000/20000 [00:01<00:00, 16269.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "\n",
    "# specify relevant arguments\n",
    "args.target_columns = ['mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'cv', 'u0', 'u298', 'h298', 'g298']\n",
    "args.task_names = args.target_columns # set task name to 'mu'\n",
    "args.features_path = None # path to load additional features\n",
    "args.features_generator = None # generator for additional features (cannot specify generator and path)\n",
    "args.max_data_size = 20000 # number of data points to load\n",
    "\n",
    "# use get_data function to convert QM9 csv -> MoleculeDataset\n",
    "# a MoleculeDataset contains a list of molecules and their associated features and targets\n",
    "data = get_data(path=args.data_path, args=args)\n",
    "\n",
    "# call MoleculeDataset methods\n",
    "args.num_tasks = data.num_tasks() # returns number of prediction tasks (number of targets)\n",
    "args.features_size = data.features_size() # returns size of features array for each molecule (>0 or None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, val, test\n",
    "\n",
    "# specify relevant arguments\n",
    "# split type options: 'random', 'scaffold_balanced', 'predetermined', 'crossval', 'index_predetermined'\n",
    "args.split_type = 'random' # specify split type\n",
    "args.split_sizes = (0.8, 0.1, 0.1) # tuple of train/val/test proportions\n",
    "args.seed = 0 # random seed for splitting\n",
    "\n",
    "# split_data returns three MoleculeDatasets\n",
    "train_data, val_data, test_data = split_data(data=data, split_type=args.split_type, \n",
    "                                             sizes=args.split_sizes, seed=args.seed, args=args, logger=logger)\n",
    "# specify training data size\n",
    "args.train_data_size = len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise training targets (zero mean, unit variance)\n",
    "\n",
    "args.dataset_type = 'regression'\n",
    "if args.dataset_type == 'regression':\n",
    "    train_smiles, train_targets = train_data.smiles(), train_data.targets() # return training targets\n",
    "    scaler = StandardScaler().fit(train_targets) # fit transform to training targets\n",
    "    scaled_targets = scaler.transform(train_targets).tolist() # apply transform\n",
    "    train_data.set_targets(scaled_targets) # replace targets in MoleculeDataset with standardised targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get loss and metric functions\n",
    "\n",
    "loss_func = get_loss_func(args) # returns loss function based on args.dataset_type (MSE for regression)\n",
    "args.metric = 'mae' # set metric\n",
    "metric_func = get_metric_func(metric=args.metric) # returns callable metric function based on args.metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up array to house test scores (num molecules X num tasks)\n",
    "\n",
    "test_smiles, test_targets = test_data.smiles(), test_data.targets() # return test smiles and targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "\n",
    "# specify relevant arguments\n",
    "args.cache_cutoff = 10000 # max number of molecules for caching\n",
    "args.batch_size = 50 # default batch size is 50\n",
    "args.num_workers = 8 # number of workers for parallel data loading\n",
    "args.class_balance = False # equal number of pos and neg in each batch (for classification)\n",
    "args.seed = 0 # seed for data loader shuffle\n",
    "\n",
    "# determine whether to cache graph featurisations when running batch_graph method\n",
    "# caching = saving mol_graphs in SMILES_TO_GRAPH\n",
    "if len(data) <= args.cache_cutoff:\n",
    "    cache = True\n",
    "    num_workers = 0\n",
    "else: # if we don't cache we perform parallel data loading\n",
    "    cache = False\n",
    "    num_workers = args.num_workers\n",
    "    \n",
    "# instantiate data loaders\n",
    "# MoleculeDataLoader: an iterable over a MoleculeDataSet which calls MoleculeSampler\n",
    "# data loader returns a MoleculeDataSet with BatchMolGraph computed using batch_graph method\n",
    "train_data_loader = MoleculeDataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache,\n",
    "        class_balance=args.class_balance,\n",
    "        shuffle=True,\n",
    "        seed=args.seed\n",
    ")\n",
    "val_data_loader = MoleculeDataLoader(\n",
    "        dataset=val_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache\n",
    ")\n",
    "test_data_loader = MoleculeDataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.use_input_features:\n",
    "    print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "\n",
    "# args: additional features\n",
    "args.features_only = False # if True only the 'additional' features feed the FFN\n",
    "#args.use_input_features = (args.features_generator is not None or args.features_path is not None)\n",
    "\n",
    "# args: message passing\n",
    "args.atom_messages = False # False means bond-centred i.e. D-MPNN\n",
    "args.undirected = False # if messages are bond-centred we can choose directed or undirected\n",
    "args.hidden_size = 300 # hidden layer size\n",
    "args.depth = 3 # number of message passing steps\n",
    "args.bias = False # only affects W_i and W_h\n",
    "args.device = torch.device('cpu')\n",
    "\n",
    "# args: FFN\n",
    "args.ffn_hidden_size = 300\n",
    "args.ffn_num_layers = 2\n",
    "\n",
    "# args: both message passing and FFN\n",
    "args.dropout = 0.0 # dropout probability\n",
    "args.activation = 'ReLU'\n",
    "\n",
    "# MoleculeModel: message passing network followed by feed-forward layers\n",
    "model = MoleculeModel(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build optimiser and lr scheduler\n",
    "\n",
    "# optimiser\n",
    "args.init_lr = 1e-4\n",
    "optimizer = build_optimizer(model, args)\n",
    "\n",
    "# lr scheduler (Noam)\n",
    "# linear increase from init_lr to max_lr during warmup_steps (warmup_epochs * steps_per_epoch)\n",
    "# steps_per_epoch = train_data_size // batch_size\n",
    "# exponential decay from max_lr to final_lr over remaining steps\n",
    "# total epochs = args.epochs * args.num_lrs\n",
    "args.max_lr = 1e-3\n",
    "args.final_lr = 1e-4\n",
    "\n",
    "args.warmup_epochs = 2.0\n",
    "args.epochs = 30\n",
    "args.num_lrs = 1 # number of learning rates\n",
    "\n",
    "scheduler = build_lr_scheduler(optimizer, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training\n",
    "\n",
    "save_dir = '/Users/georgelamb/Documents/GitHub/chempropBayes/saved_models'\n",
    "args.log_frequency = 320 # number of batches between each logging of the training loss\n",
    "args.show_individual_scores = False\n",
    "args.minimize_score = True\n",
    "\n",
    "best_score = float('inf') if args.minimize_score else -float('inf')\n",
    "best_epoch, n_iter = 0, 0\n",
    "for epoch in range(args.epochs):\n",
    "    \n",
    "    # train for one epoch\n",
    "    n_iter = train(\n",
    "        model=model,\n",
    "        data_loader=train_data_loader,\n",
    "        loss_func=loss_func,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        args=args,\n",
    "        n_iter=n_iter,\n",
    "        logger=logger,\n",
    "        writer=writer\n",
    "    )\n",
    "    \n",
    "    # evaluation on val set for one epoch\n",
    "    val_scores = evaluate(\n",
    "        model=model,\n",
    "        data_loader=val_data_loader,\n",
    "        num_tasks=args.num_tasks,\n",
    "        metric_func=metric_func,\n",
    "        dataset_type=args.dataset_type,\n",
    "        scaler=scaler,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    # average validation score\n",
    "    avg_val_score = np.nanmean(val_scores)\n",
    "    print(f'Validation AVG {args.metric} = {avg_val_score:.6f}')\n",
    "    \n",
    "    # show individual validation scores\n",
    "    if args.show_individual_scores:\n",
    "        for task_name, val_score in zip(args.task_names, val_scores):\n",
    "            print(f'Validation {task_name} {args.metric} = {val_score:.6f}')\n",
    "            \n",
    "    # save model if improved validation score\n",
    "    if args.minimize_score and avg_val_score < best_score or \\\n",
    "            not args.minimize_score and avg_val_score > best_score:\n",
    "        best_score, best_epoch = avg_val_score, epoch\n",
    "        best_model = copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "\n",
    "print(f'Best validation {args.metric} = {best_score:.6f} on epoch {best_epoch}')\n",
    "\n",
    "test_preds = predict(\n",
    "    model=best_model,\n",
    "    data_loader=test_data_loader,\n",
    "    scaler=scaler\n",
    ")\n",
    "test_scores = evaluate_predictions(\n",
    "    preds=test_preds,\n",
    "    targets=test_targets,\n",
    "    num_tasks=args.num_tasks,\n",
    "    metric_func=metric_func,\n",
    "    dataset_type=args.dataset_type,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Average test score\n",
    "avg_test_score = np.nanmean(test_scores)\n",
    "print(f'Test {args.metric} = {avg_test_score:.6f}')\n",
    "\n",
    "# Individual test scores\n",
    "if args.show_individual_scores:\n",
    "    for task_name, test_score in zip(args.task_names, test_scores):\n",
    "        print(f'Test {task_name} {args.metric} = {test_score:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
