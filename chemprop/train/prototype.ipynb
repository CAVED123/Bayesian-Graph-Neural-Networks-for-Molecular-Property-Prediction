{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains and evaluates a prototype D-MPNN on QM9 (regression)\n",
    "\n",
    "The code is based on run_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate:\n",
    "\n",
    "- Why do only two learned weight matrices have a bias switch?\n",
    "- Loss function... why are we dividing by number of molecules twice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "from tqdm import trange\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch_geometric.datasets import QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/georgelamb/Documents/GitHub/chempropBayes\n"
     ]
    }
   ],
   "source": [
    "# cd to chempropBayes\n",
    "%cd /Users/georgelamb/Documents/GitHub/chempropBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from chempropBayes\n",
    "from chemprop.train.evaluate import evaluate, evaluate_predictions\n",
    "from chemprop.train.predict import predict\n",
    "from chemprop.train.train import train\n",
    "from chemprop.args import TrainArgs\n",
    "from chemprop.data import StandardScaler, MoleculeDataLoader\n",
    "from chemprop.data.utils import get_class_sizes, get_data, get_task_names, split_data\n",
    "from chemprop.models import MoleculeModel\n",
    "from chemprop.nn_utils import param_count\n",
    "from chemprop.utils import build_optimizer, build_lr_scheduler, get_loss_func, get_metric_func, load_checkpoint,\\\n",
    "    makedirs, save_checkpoint, save_smiles_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate args class and load from dict\n",
    "args = TrainArgs()\n",
    "args.from_dict({\n",
    "    'dataset_type': 'regression',\n",
    "    'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv'\n",
    "})\n",
    "\n",
    "# define logger or writer\n",
    "logger = None\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.target_columns = ['mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'cv', 'u0', 'u298', 'h298', 'g298']\n",
    "args.task_names = args.target_columns\n",
    "args.features_path = None # path to load additional features\n",
    "args.features_generator = None # generator for additional features (cannot specify generator and path)\n",
    "args.max_data_size = 20000 # number of data points to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14356it [00:00, 52034.67it/s] \n",
      "100%|██████████| 20000/20000 [00:00<00:00, 593661.00it/s]\n",
      "100%|██████████| 20000/20000 [00:01<00:00, 15939.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# use get_data function to convert QM9 csv -> MoleculeDataset\n",
    "# a MoleculeDataset contains a list of molecules and their associated features and targets\n",
    "data = get_data(path=args.data_path, args=args)\n",
    "\n",
    "# call MoleculeDataset methods\n",
    "args.num_tasks = data.num_tasks() # returns number of prediction tasks (number of targets)\n",
    "args.features_size = data.features_size() # returns size of features array for each molecule (>0 or None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "# split type options: 'random', 'scaffold_balanced', 'predetermined', 'crossval', 'index_predetermined'\n",
    "args.split_type = 'random' # specify split type \n",
    "args.split_sizes = (0.8, 0.1, 0.1) # tuple of train/val/test proportions\n",
    "args.seed = 0 # random seed for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_data returns three MoleculeDatasets\n",
    "train_data, val_data, test_data = split_data(data=data, split_type=args.split_type, \n",
    "                                             sizes=args.split_sizes, seed=args.seed, args=args, logger=logger)\n",
    "# specify training data size\n",
    "args.train_data_size = len(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise training targets (zero mean, unit variance)\n",
    "if args.dataset_type == 'regression':\n",
    "    train_smiles, train_targets = train_data.smiles(), train_data.targets() # return training targets\n",
    "    scaler = StandardScaler().fit(train_targets) # fit transform to training targets\n",
    "    scaled_targets = scaler.transform(train_targets).tolist() # apply transform\n",
    "    train_data.set_targets(scaled_targets) # replace targets in MoleculeDataset with standardised targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.cache_cutoff = 10000 # max number of molecules for caching\n",
    "args.batch_size = 50 # default batch size is 50\n",
    "args.num_workers = 8 # number of workers for parallel data loading\n",
    "args.class_balance = False # equal number of pos and neg in each batch (for classification)\n",
    "args.seed = 0 # seed for data loader shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine whether to cache graph featurisations when running batch_graph method\n",
    "# caching = saving mol_graphs in SMILES_TO_GRAPH\n",
    "if len(data) <= args.cache_cutoff:\n",
    "    cache = True\n",
    "    num_workers = 0\n",
    "else: # if we don't cache we perform parallel data loading\n",
    "    cache = False\n",
    "    num_workers = args.num_workers\n",
    "    \n",
    "# instantiate data loaders\n",
    "# MoleculeDataLoader: an iterable over a MoleculeDataSet which calls MoleculeSampler\n",
    "# data loader returns a MoleculeDataSet with BatchMolGraph computed using batch_graph method\n",
    "train_data_loader = MoleculeDataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache,\n",
    "        class_balance=args.class_balance,\n",
    "        shuffle=True,\n",
    "        seed=args.seed\n",
    ")\n",
    "val_data_loader = MoleculeDataLoader(\n",
    "        dataset=val_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache\n",
    ")\n",
    "test_data_loader = MoleculeDataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss and metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.metric = 'mae' # set metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = get_loss_func(args) # returns loss function based on args.dataset_type (MSE for regression)\n",
    "metric_func = get_metric_func(metric=args.metric) # returns callable metric function based on args.metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS: seed\n",
    "args.pytorch_seed = 0\n",
    "\n",
    "# ARGS: additional features\n",
    "args.features_only = False # if True only the 'additional' features feed the FFN\n",
    "\n",
    "# ARGS: message passing\n",
    "args.atom_messages = False # False means bond-centred i.e. D-MPNN\n",
    "args.undirected = False # if messages are bond-centred we can choose directed or undirected\n",
    "args.hidden_size = 300 # hidden layer size\n",
    "args.depth = 3 # number of message passing steps\n",
    "args.bias = False # only affects W_i and W_h\n",
    "args.device = torch.device('cpu')\n",
    "\n",
    "# ARGS: FFN\n",
    "args.ffn_hidden_size = 300\n",
    "args.ffn_num_layers = 2\n",
    "\n",
    "# ARGS: both message passing and FFN\n",
    "args.dropout = 0.0 # dropout probability\n",
    "args.activation = 'ReLU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pytorch seed for random initial weights\n",
    "torch.manual_seed(args.pytorch_seed);\n",
    "\n",
    "# MoleculeModel: message passing network followed by feed-forward layers\n",
    "model = MoleculeModel(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build optimiser and lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "\n",
    "# lr scheduler (Noam)\n",
    "# linear increase from init_lr to max_lr during warmup_steps (warmup_epochs * steps_per_epoch)\n",
    "# steps_per_epoch = train_data_size // batch_size\n",
    "# exponential decay from max_lr to final_lr over remaining steps\n",
    "# total epochs = args.epochs * args.num_lrs\n",
    "\n",
    "args.init_lr = 1e-4\n",
    "args.max_lr = 1e-3\n",
    "args.final_lr = 1e-4\n",
    "\n",
    "args.warmup_epochs = 2.0\n",
    "args.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build optimiser\n",
    "optimizer = build_optimizer(model, args)\n",
    "\n",
    "# build scheduler\n",
    "scheduler = build_lr_scheduler(optimizer, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and save model with best val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.log_frequency = 320 # number of batches between each logging of the training loss\n",
    "args.show_individual_scores = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 9.3878e-03, PNorm = 34.9440, GNorm = 2.2418, lr_0 = 5.5141e-04\n",
      "Validation AVG mae = 15.163658\n",
      "Loss = 4.9756e-03, PNorm = 35.9501, GNorm = 1.3781, lr_0 = 9.9974e-04\n",
      "Validation AVG mae = 11.791326\n",
      "Loss = 3.9404e-03, PNorm = 36.7471, GNorm = 1.1471, lr_0 = 9.2082e-04\n",
      "Validation AVG mae = 10.546208\n",
      "Loss = 3.3758e-03, PNorm = 37.4242, GNorm = 1.2346, lr_0 = 8.4812e-04\n",
      "Validation AVG mae = 11.337531\n",
      "Loss = 2.9279e-03, PNorm = 38.0750, GNorm = 1.5341, lr_0 = 7.8117e-04\n",
      "Validation AVG mae = 8.504389\n",
      "Loss = 2.5165e-03, PNorm = 38.6695, GNorm = 0.5595, lr_0 = 7.1950e-04\n",
      "Validation AVG mae = 8.037916\n",
      "Loss = 2.2428e-03, PNorm = 39.1973, GNorm = 1.0680, lr_0 = 6.6270e-04\n",
      "Validation AVG mae = 7.029600\n",
      "Loss = 1.9738e-03, PNorm = 39.6241, GNorm = 0.8361, lr_0 = 6.1038e-04\n",
      "Validation AVG mae = 6.751574\n",
      "Loss = 1.8715e-03, PNorm = 40.0332, GNorm = 0.9515, lr_0 = 5.6220e-04\n",
      "Validation AVG mae = 7.616514\n",
      "Loss = 1.7034e-03, PNorm = 40.3852, GNorm = 0.4129, lr_0 = 5.1781e-04\n",
      "Validation AVG mae = 6.427191\n",
      "Loss = 1.5792e-03, PNorm = 40.7084, GNorm = 0.7483, lr_0 = 4.7694e-04\n",
      "Validation AVG mae = 6.645649\n",
      "Loss = 1.4660e-03, PNorm = 40.9799, GNorm = 0.8983, lr_0 = 4.3928e-04\n",
      "Validation AVG mae = 5.935448\n",
      "Loss = 1.3571e-03, PNorm = 41.2198, GNorm = 0.5071, lr_0 = 4.0461e-04\n",
      "Validation AVG mae = 5.791906\n",
      "Loss = 1.2911e-03, PNorm = 41.4469, GNorm = 0.7294, lr_0 = 3.7266e-04\n",
      "Validation AVG mae = 5.874291\n",
      "Loss = 1.2137e-03, PNorm = 41.6482, GNorm = 0.6762, lr_0 = 3.4324e-04\n",
      "Validation AVG mae = 5.100887\n",
      "Loss = 1.1789e-03, PNorm = 41.8453, GNorm = 0.4808, lr_0 = 3.1615e-04\n",
      "Validation AVG mae = 5.102564\n",
      "Loss = 1.0928e-03, PNorm = 42.0051, GNorm = 0.5319, lr_0 = 2.9119e-04\n",
      "Validation AVG mae = 5.688285\n",
      "Loss = 1.0588e-03, PNorm = 42.1598, GNorm = 0.8563, lr_0 = 2.6820e-04\n",
      "Validation AVG mae = 4.674614\n",
      "Loss = 1.0158e-03, PNorm = 42.2958, GNorm = 0.9059, lr_0 = 2.4703e-04\n",
      "Validation AVG mae = 4.863602\n",
      "Loss = 9.6582e-04, PNorm = 42.4194, GNorm = 0.4365, lr_0 = 2.2753e-04\n",
      "Validation AVG mae = 4.775673\n",
      "Loss = 9.3153e-04, PNorm = 42.5290, GNorm = 0.8177, lr_0 = 2.0956e-04\n",
      "Validation AVG mae = 4.906900\n",
      "Loss = 9.0519e-04, PNorm = 42.6394, GNorm = 0.4713, lr_0 = 1.9302e-04\n",
      "Validation AVG mae = 4.455228\n",
      "Loss = 8.5642e-04, PNorm = 42.7309, GNorm = 0.9287, lr_0 = 1.7778e-04\n",
      "Validation AVG mae = 4.407882\n",
      "Loss = 8.3556e-04, PNorm = 42.8154, GNorm = 0.4543, lr_0 = 1.6375e-04\n",
      "Validation AVG mae = 4.768172\n",
      "Loss = 8.1600e-04, PNorm = 42.8981, GNorm = 0.2598, lr_0 = 1.5082e-04\n",
      "Validation AVG mae = 4.339168\n",
      "Loss = 7.8971e-04, PNorm = 42.9712, GNorm = 0.3115, lr_0 = 1.3891e-04\n",
      "Validation AVG mae = 4.249009\n",
      "Loss = 7.6857e-04, PNorm = 43.0367, GNorm = 0.7444, lr_0 = 1.2795e-04\n",
      "Validation AVG mae = 4.231848\n",
      "Loss = 7.5819e-04, PNorm = 43.0990, GNorm = 0.6821, lr_0 = 1.1785e-04\n",
      "Validation AVG mae = 4.198008\n",
      "Loss = 7.2547e-04, PNorm = 43.1539, GNorm = 0.5188, lr_0 = 1.0854e-04\n",
      "Validation AVG mae = 4.220353\n",
      "Loss = 7.2566e-04, PNorm = 43.2070, GNorm = 0.4152, lr_0 = 1.0000e-04\n",
      "Validation AVG mae = 4.353608\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "best_score = float('inf') if args.minimize_score else -float('inf')\n",
    "best_epoch, n_iter = 0, 0\n",
    "for epoch in range(args.epochs):\n",
    "    \n",
    "    # train for one epoch\n",
    "    n_iter = train(\n",
    "        model=model,\n",
    "        data_loader=train_data_loader,\n",
    "        loss_func=loss_func,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        args=args,\n",
    "        n_iter=n_iter,\n",
    "        logger=logger,\n",
    "        writer=writer\n",
    "    )\n",
    "    \n",
    "    # evaluation on val set for one epoch\n",
    "    val_scores = evaluate(\n",
    "        model=model,\n",
    "        data_loader=val_data_loader,\n",
    "        num_tasks=args.num_tasks,\n",
    "        metric_func=metric_func,\n",
    "        dataset_type=args.dataset_type,\n",
    "        scaler=scaler,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    # average validation score\n",
    "    avg_val_score = np.nanmean(val_scores)\n",
    "    print(f'Validation AVG {args.metric} = {avg_val_score:.6f}')\n",
    "    \n",
    "    # show individual validation scores\n",
    "    if args.show_individual_scores:\n",
    "        for task_name, val_score in zip(args.task_names, val_scores):\n",
    "            print(f'Validation {task_name} {args.metric} = {val_score:.6f}')\n",
    "            \n",
    "    # save model if improved validation score\n",
    "    if args.minimize_score and avg_val_score < best_score or \\\n",
    "            not args.minimize_score and avg_val_score > best_score:\n",
    "        best_score, best_epoch = avg_val_score, epoch\n",
    "        best_model = copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.show_individual_scores = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation mae = 4.198008 on epoch 27\n"
     ]
    }
   ],
   "source": [
    "# print performance of best model\n",
    "print(f'Best validation {args.metric} = {best_score:.6f} on epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mae = 4.280236\n",
      "Test mu mae = 0.483251\n",
      "Test alpha mae = 0.932755\n",
      "Test homo mae = 0.004504\n",
      "Test lumo mae = 0.004795\n",
      "Test gap mae = 0.006307\n",
      "Test r2 mae = 33.991091\n",
      "Test zpve mae = 0.002149\n",
      "Test cv mae = 0.415957\n",
      "Test u0 mae = 3.848183\n",
      "Test u298 mae = 3.878890\n",
      "Test h298 mae = 3.902034\n",
      "Test g298 mae = 3.892922\n"
     ]
    }
   ],
   "source": [
    "# return test targets\n",
    "test_targets = test_data.targets()\n",
    "\n",
    "# compute predictions and scores on test set\n",
    "test_preds = predict(\n",
    "    model=best_model,\n",
    "    data_loader=test_data_loader,\n",
    "    scaler=scaler\n",
    ")\n",
    "test_scores = evaluate_predictions(\n",
    "    preds=test_preds,\n",
    "    targets=test_targets,\n",
    "    num_tasks=args.num_tasks,\n",
    "    metric_func=metric_func,\n",
    "    dataset_type=args.dataset_type,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# average test score\n",
    "avg_test_score = np.nanmean(test_scores)\n",
    "print(f'Test {args.metric} = {avg_test_score:.6f}')\n",
    "\n",
    "# individual test scores\n",
    "if args.show_individual_scores:\n",
    "    for task_name, test_score in zip(args.task_names, test_scores):\n",
    "        print(f'Test {task_name} {args.metric} = {test_score:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mae = 25.805655\n",
      "Test mu mae = 1.192260\n",
      "Test alpha mae = 6.272279\n",
      "Test homo mae = 0.016396\n",
      "Test lumo mae = 0.038681\n",
      "Test gap mae = 0.040090\n",
      "Test r2 mae = 178.012015\n",
      "Test zpve mae = 0.025054\n",
      "Test cv mae = 3.111326\n",
      "Test u0 mae = 30.239929\n",
      "Test u298 mae = 30.239719\n",
      "Test h298 mae = 30.239719\n",
      "Test g298 mae = 30.240388\n"
     ]
    }
   ],
   "source": [
    "# compare with means as predictions of each target\n",
    "\n",
    "test_targets_array = np.array(test_targets)\n",
    "target_means = np.mean(test_targets_array,0)\n",
    "MAE = np.mean(abs(test_targets_array - target_means),0)\n",
    "\n",
    "avg_test_score = np.nanmean(MAE)\n",
    "print(f'Test {args.metric} = {avg_test_score:.6f}')\n",
    "\n",
    "for task_name, test_score in zip(args.task_names, MAE):\n",
    "    print(f'Test {task_name} {args.metric} = {test_score:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
