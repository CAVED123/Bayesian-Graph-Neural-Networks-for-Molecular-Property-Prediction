{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains and evaluates a prototype D-MPNN on QM9 (regression)\n",
    "\n",
    "The code is based on run_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate:\n",
    "\n",
    "- Why do only two learned weight matrices have a bias switch?\n",
    "- Loss function... why are we dividing by number of molecules twice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "from tqdm import trange\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch_geometric.datasets import QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/georgelamb/Documents/GitHub/chempropBayes\n"
     ]
    }
   ],
   "source": [
    "# cd to chempropBayes\n",
    "%cd /Users/georgelamb/Documents/GitHub/chempropBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from chempropBayes\n",
    "from chemprop.train.evaluate import evaluate, evaluate_predictions\n",
    "from chemprop.train.predict import predict\n",
    "from chemprop.train.train import train\n",
    "from chemprop.args import TrainArgs\n",
    "from chemprop.data import StandardScaler, MoleculeDataLoader\n",
    "from chemprop.data.utils import get_class_sizes, get_data, get_task_names, split_data\n",
    "from chemprop.models import MoleculeModel\n",
    "from chemprop.nn_utils import param_count\n",
    "from chemprop.utils import build_optimizer, build_lr_scheduler, get_loss_func, get_metric_func, load_checkpoint,\\\n",
    "    makedirs, save_checkpoint, save_smiles_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate args class and load from dict\n",
    "args = TrainArgs()\n",
    "args.from_dict({\n",
    "    'dataset_type': 'regression',\n",
    "    'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv'\n",
    "})\n",
    "\n",
    "# define logger or writer\n",
    "logger = None\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.target_columns = ['mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'cv', 'u0', 'u298', 'h298', 'g298']\n",
    "args.task_names = args.target_columns\n",
    "args.features_path = None # path to load additional features\n",
    "args.features_generator = None # generator for additional features (cannot specify generator and path)\n",
    "args.max_data_size = 20000 # number of data points to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14356it [00:00, 52034.67it/s] \n",
      "100%|██████████| 20000/20000 [00:00<00:00, 593661.00it/s]\n",
      "100%|██████████| 20000/20000 [00:01<00:00, 15939.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# use get_data function to convert QM9 csv -> MoleculeDataset\n",
    "# a MoleculeDataset contains a list of molecules and their associated features and targets\n",
    "data = get_data(path=args.data_path, args=args)\n",
    "\n",
    "# call MoleculeDataset methods\n",
    "args.num_tasks = data.num_tasks() # returns number of prediction tasks (number of targets)\n",
    "args.features_size = data.features_size() # returns size of features array for each molecule (>0 or None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "# split type options: 'random', 'scaffold_balanced', 'predetermined', 'crossval', 'index_predetermined'\n",
    "args.split_type = 'random' # specify split type \n",
    "args.split_sizes = (0.8, 0.1, 0.1) # tuple of train/val/test proportions\n",
    "args.seed = 0 # random seed for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_data returns three MoleculeDatasets\n",
    "train_data, val_data, test_data = split_data(data=data, split_type=args.split_type, \n",
    "                                             sizes=args.split_sizes, seed=args.seed, args=args, logger=logger)\n",
    "# specify training data size\n",
    "args.train_data_size = len(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise training targets (zero mean, unit variance)\n",
    "if args.dataset_type == 'regression':\n",
    "    train_smiles, train_targets = train_data.smiles(), train_data.targets() # return training targets\n",
    "    scaler = StandardScaler().fit(train_targets) # fit transform to training targets\n",
    "    scaled_targets = scaler.transform(train_targets).tolist() # apply transform\n",
    "    train_data.set_targets(scaled_targets) # replace targets in MoleculeDataset with standardised targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.cache_cutoff = 10000 # max number of molecules for caching\n",
    "args.batch_size = 50 # default batch size is 50\n",
    "args.num_workers = 8 # number of workers for parallel data loading\n",
    "args.class_balance = False # equal number of pos and neg in each batch (for classification)\n",
    "args.seed = 0 # seed for data loader shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine whether to cache graph featurisations when running batch_graph method\n",
    "# caching = saving mol_graphs in SMILES_TO_GRAPH\n",
    "if len(data) <= args.cache_cutoff:\n",
    "    cache = True\n",
    "    num_workers = 0\n",
    "else: # if we don't cache we perform parallel data loading\n",
    "    cache = False\n",
    "    num_workers = args.num_workers\n",
    "    \n",
    "# instantiate data loaders\n",
    "# MoleculeDataLoader: an iterable over a MoleculeDataSet which calls MoleculeSampler\n",
    "# data loader returns a MoleculeDataSet with BatchMolGraph computed using batch_graph method\n",
    "train_data_loader = MoleculeDataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache,\n",
    "        class_balance=args.class_balance,\n",
    "        shuffle=True,\n",
    "        seed=args.seed\n",
    ")\n",
    "val_data_loader = MoleculeDataLoader(\n",
    "        dataset=val_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache\n",
    ")\n",
    "test_data_loader = MoleculeDataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        cache=cache\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss and metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.metric = 'mae' # set metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = get_loss_func(args) # returns loss function based on args.dataset_type (MSE for regression)\n",
    "metric_func = get_metric_func(metric=args.metric) # returns callable metric function based on args.metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS: seed\n",
    "args.pytorch_seed = 0\n",
    "\n",
    "# ARGS: additional features\n",
    "args.features_only = False # if True only the 'additional' features feed the FFN\n",
    "\n",
    "# ARGS: message passing\n",
    "args.atom_messages = False # False means bond-centred i.e. D-MPNN\n",
    "args.undirected = False # if messages are bond-centred we can choose directed or undirected\n",
    "args.hidden_size = 300 # hidden layer size\n",
    "args.depth = 3 # number of message passing steps\n",
    "args.bias = False # only affects W_i and W_h (only message passing layers where you turn it off)\n",
    "args.device = torch.device('cpu')\n",
    "\n",
    "# ARGS: FFN\n",
    "args.ffn_hidden_size = 300\n",
    "args.ffn_num_layers = 2\n",
    "\n",
    "# ARGS: both message passing and FFN\n",
    "args.dropout = 0.0 # dropout probability\n",
    "args.activation = 'ReLU'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set pytorch seed for random initial weights\n",
    "torch.manual_seed(args.pytorch_seed);\n",
    "\n",
    "# MoleculeModel: message passing network followed by feed-forward layers\n",
    "model = MoleculeModel(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build optimiser and lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "\n",
    "# lr scheduler (Noam)\n",
    "# linear increase from init_lr to max_lr during warmup_steps (warmup_epochs * steps_per_epoch)\n",
    "# steps_per_epoch = train_data_size // batch_size\n",
    "# exponential decay from max_lr to final_lr over remaining steps\n",
    "# total epochs = args.epochs * args.num_lrs\n",
    "\n",
    "args.init_lr = 1e-4\n",
    "args.max_lr = 1e-3\n",
    "args.final_lr = 1e-4\n",
    "\n",
    "args.warmup_epochs = 2.0\n",
    "args.epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build optimiser\n",
    "optimizer = build_optimizer(model, args)\n",
    "\n",
    "# build scheduler\n",
    "scheduler = build_lr_scheduler(optimizer, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and save model with best val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.log_frequency = 320 # number of batches between each logging of the training loss\n",
    "args.show_individual_scores = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 7.0826e-04, PNorm = 43.2558, GNorm = 0.4867, lr_0 = 1.0000e-04\n",
      "Validation AVG mae = 4.101726\n",
      "Validation mu mae = 0.459818\n",
      "Validation alpha mae = 0.906455\n",
      "Validation homo mae = 0.004575\n",
      "Validation lumo mae = 0.004463\n",
      "Validation gap mae = 0.006130\n",
      "Validation r2 mae = 32.500538\n",
      "Validation zpve mae = 0.002201\n",
      "Validation cv mae = 0.413761\n",
      "Validation u0 mae = 3.723235\n",
      "Validation u298 mae = 3.693065\n",
      "Validation h298 mae = 3.752473\n",
      "Validation g298 mae = 3.753995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-510:\n",
      "Process Process-512:\n",
      "Process Process-511:\n",
      "Process Process-507:\n",
      "Process Process-506:\n",
      "Process Process-509:\n",
      "Process Process-508:\n",
      "Process Process-505:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fcace7565f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2d0bda0a8ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/train/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, loss_func, optimizer, scheduler, args, n_iter, logger, writer)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# backward pass; update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "best_score = float('inf') if args.minimize_score else -float('inf')\n",
    "best_epoch, n_iter = 0, 0\n",
    "for epoch in range(args.epochs):\n",
    "    \n",
    "    # train for one epoch\n",
    "    n_iter = train(\n",
    "        model=model,\n",
    "        data_loader=train_data_loader,\n",
    "        loss_func=loss_func,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        args=args,\n",
    "        n_iter=n_iter,\n",
    "        logger=logger,\n",
    "        writer=writer\n",
    "    )\n",
    "    \n",
    "    # evaluation on val set for one epoch\n",
    "    val_scores = evaluate(\n",
    "        model=model,\n",
    "        data_loader=val_data_loader,\n",
    "        num_tasks=args.num_tasks,\n",
    "        metric_func=metric_func,\n",
    "        dataset_type=args.dataset_type,\n",
    "        scaler=scaler,\n",
    "        logger=logger\n",
    "    )\n",
    "    \n",
    "    # average validation score\n",
    "    avg_val_score = np.nanmean(val_scores)\n",
    "    print(f'Validation AVG {args.metric} = {avg_val_score:.6f}')\n",
    "    \n",
    "    # show individual validation scores\n",
    "    if args.show_individual_scores:\n",
    "        for task_name, val_score in zip(args.task_names, val_scores):\n",
    "            print(f'Validation {task_name} {args.metric} = {val_score:.6f}')\n",
    "            \n",
    "    # save model if improved validation score\n",
    "    if args.minimize_score and avg_val_score < best_score or \\\n",
    "            not args.minimize_score and avg_val_score > best_score:\n",
    "        best_score, best_epoch = avg_val_score, epoch\n",
    "        best_model = copy.deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGS\n",
    "args.show_individual_scores = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation mae = 4.198008 on epoch 27\n"
     ]
    }
   ],
   "source": [
    "# print performance of best model\n",
    "print(f'Best validation {args.metric} = {best_score:.6f} on epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mae = 4.280236\n",
      "Test mu mae = 0.483251\n",
      "Test alpha mae = 0.932755\n",
      "Test homo mae = 0.004504\n",
      "Test lumo mae = 0.004795\n",
      "Test gap mae = 0.006307\n",
      "Test r2 mae = 33.991091\n",
      "Test zpve mae = 0.002149\n",
      "Test cv mae = 0.415957\n",
      "Test u0 mae = 3.848183\n",
      "Test u298 mae = 3.878890\n",
      "Test h298 mae = 3.902034\n",
      "Test g298 mae = 3.892922\n"
     ]
    }
   ],
   "source": [
    "# return test targets\n",
    "test_targets = test_data.targets()\n",
    "\n",
    "# compute predictions and scores on test set\n",
    "test_preds = predict(\n",
    "    model=best_model,\n",
    "    data_loader=test_data_loader,\n",
    "    scaler=scaler\n",
    ")\n",
    "test_scores = evaluate_predictions(\n",
    "    preds=test_preds,\n",
    "    targets=test_targets,\n",
    "    num_tasks=args.num_tasks,\n",
    "    metric_func=metric_func,\n",
    "    dataset_type=args.dataset_type,\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# average test score\n",
    "avg_test_score = np.nanmean(test_scores)\n",
    "print(f'Test {args.metric} = {avg_test_score:.6f}')\n",
    "\n",
    "# individual test scores\n",
    "if args.show_individual_scores:\n",
    "    for task_name, test_score in zip(args.task_names, test_scores):\n",
    "        print(f'Test {task_name} {args.metric} = {test_score:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test mae = 25.805655\n",
      "Test mu mae = 1.192260\n",
      "Test alpha mae = 6.272279\n",
      "Test homo mae = 0.016396\n",
      "Test lumo mae = 0.038681\n",
      "Test gap mae = 0.040090\n",
      "Test r2 mae = 178.012015\n",
      "Test zpve mae = 0.025054\n",
      "Test cv mae = 3.111326\n",
      "Test u0 mae = 30.239929\n",
      "Test u298 mae = 30.239719\n",
      "Test h298 mae = 30.239719\n",
      "Test g298 mae = 30.240388\n"
     ]
    }
   ],
   "source": [
    "# compare with means as predictions of each target\n",
    "\n",
    "test_targets_array = np.array(test_targets)\n",
    "target_means = np.mean(test_targets_array,0)\n",
    "MAE = np.mean(abs(test_targets_array - target_means),0)\n",
    "\n",
    "avg_test_score = np.nanmean(MAE)\n",
    "print(f'Test {args.metric} = {avg_test_score:.6f}')\n",
    "\n",
    "for task_name, test_score in zip(args.task_names, MAE):\n",
    "    print(f'Test {task_name} {args.metric} = {test_score:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
