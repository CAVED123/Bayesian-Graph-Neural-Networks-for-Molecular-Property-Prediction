{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs prototype versions of the following methods:\n",
    "- MAP\n",
    "- MAP Ensemble\n",
    "- MC-Dropout\n",
    "- Featurisation + GP\n",
    "- SWAG-Diag\n",
    "- SWAG\n",
    "- MultiSWAG\n",
    "- SGLD\n",
    "- BBP\n",
    "- BBP + reparametrisation\n",
    "\n",
    "The methods are run with:\n",
    "- A smaller version of QM9 (50,000 examples)\n",
    "- A single split of the data (random)\n",
    "- A D-MPNN optimised through crude grid-search (hidden size 500, depth 5, layers 3)\n",
    "\n",
    "Please re-instantiate args before running each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "from tqdm import trange\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch_geometric.datasets import QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/georgelamb/Documents/GitHub/chempropBayes\n"
     ]
    }
   ],
   "source": [
    "# cd to chempropBayes\n",
    "%cd /Users/georgelamb/Documents/GitHub/chempropBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from chemprop.train.run_training import run_training\n",
    "from chemprop.args import TrainArgs\n",
    "from chemprop.data.utils import get_class_sizes, get_data, get_task_names, split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate args class and load from dict\n",
    "args = TrainArgs()\n",
    "args.from_dict({\n",
    "    'dataset_type': 'regression',\n",
    "    'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv'\n",
    "})\n",
    "\n",
    "# location for model checkpoints to be saved\n",
    "args.save_dir = '/Users/georgelamb/Documents/GitHub/chempropBayes/log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### args (non-model)\n",
    "\n",
    "# seed for splitting and loading data\n",
    "args.seed = 0\n",
    "\n",
    "# data\n",
    "args.max_data_size = 50000\n",
    "args.features_path = None\n",
    "args.features_generator = None\n",
    "\n",
    "# splitting data\n",
    "args.split_type = 'random'\n",
    "args.split_sizes = (0.8, 0.1, 0.1)\n",
    "\n",
    "# evaluation metric\n",
    "args.metric = 'mae'\n",
    "\n",
    "# epochs and logging\n",
    "args.epochs = 50\n",
    "args.log_frequency = 800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### args (model)\n",
    "\n",
    "# seed for random initial weights\n",
    "args.pytorch_seed = 0\n",
    "\n",
    "# message passing\n",
    "args.atom_messages = False\n",
    "args.undirected = False\n",
    "args.bias = False\n",
    "args.hidden_size = 500\n",
    "args.depth = 5\n",
    "\n",
    "# FFN\n",
    "args.ffn_hidden_size = args.hidden_size\n",
    "args.ffn_num_layers = 3\n",
    "\n",
    "# shared\n",
    "args.activation = 'ReLU'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9639it [00:00, 96383.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-6a41b4ab-3f98-4bbb-82d2-2b0cc542ed11.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'batch_size_gp': 100,\n",
      " 'batch_size_sgld': 50,\n",
      " 'bias': False,\n",
      " 'block': True,\n",
      " 'burnin_epochs': 10,\n",
      " 'c_swag': 0,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0.0,\n",
      " 'dropout_FFNonly': False,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_gp': 100,\n",
      " 'epochs_swag': 0,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'final_lr_gp': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'gp': False,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise_sgld': -2,\n",
      " 'init_lr': 0.0001,\n",
      " 'init_lr_gp': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'log_frequency_gp': 100,\n",
      " 'log_frequency_sgld': 0,\n",
      " 'lr_sgld': 0.0001,\n",
      " 'lr_swag': 0.0001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_lr_gp': 0.001,\n",
      " 'max_num_models': 0,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'mix_epochs': 1,\n",
      " 'momentum_swag': 0.9,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_inducing_points': 2000,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': None,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 1,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': False,\n",
      " 'target_columns': None,\n",
      " 'task_names': None,\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': None,\n",
      " 'undirected': False,\n",
      " 'unfreeze_epoch_gp': 50,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'warmup_epochs_gp': 4,\n",
      " 'wd_swag': 0,\n",
      " 'weight_decay_sgld': 0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44390it [00:00, 71271.90it/s]\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-887484e67c96>\", line 4, in <module>\n",
      "    results_MAP = run_training(args)\n",
      "  File \"/Users/georgelamb/Documents/GitHub/chempropBayes/chemprop/train/run_training.py\", line 60, in run_training\n",
      "    data = get_data(path=args.data_path, args=args, logger=logger)\n",
      "  File \"/Users/georgelamb/Documents/GitHub/chempropBayes/chemprop/data/utils.py\", line 184, in get_data\n",
      "    total=len(all_smiles))\n",
      "  File \"/Users/georgelamb/Documents/GitHub/chempropBayes/chemprop/data/utils.py\", line 183, in <listcomp>\n",
      "    ) for i, (smiles, targets, row) in tqdm(enumerate(zip(all_smiles, all_targets, all_rows)),\n",
      "  File \"/Users/georgelamb/Documents/GitHub/chempropBayes/chemprop/data/data.py\", line 21, in __init__\n",
      "    def __init__(self,\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Applications/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Applications/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Applications/anaconda3/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Applications/anaconda3/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 1\n",
    "\n",
    "results_MAP = run_training(args)\n",
    "#np.savez(args.save_dir+'/results_MAP', results_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.ensemble_size = 10\n",
    "args.samples = 1\n",
    "\n",
    "results_ens = run_training(args)\n",
    "#np.savez(args.save_dir+'/results_ens', results_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC-Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have dropout in the following places:\n",
    "# - after the edge update function (following ReLU)\n",
    "# - after converting to an atomic representation (following ReLU)\n",
    "# - on the global hidden state representation\n",
    "# - after every non-final FFN layer (following ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-832232a1-7bee-48f8-801a-d6a9460319ca.json\n",
      "Args\n",
      "{'RMS': False,\n",
      " 'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'batch_size': 50,\n",
      " 'batch_size_sgld': 50,\n",
      " 'bias': False,\n",
      " 'block': True,\n",
      " 'burnin_epochs': 10,\n",
      " 'c_swag': 0,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0.1,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 250,\n",
      " 'epochs_swag': 0,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise': -2,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'log_frequency_sgld': 0,\n",
      " 'lr_sgld': 0.0001,\n",
      " 'lr_swag': 0.0001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 0,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'mix_epochs': 1,\n",
      " 'momentum_swag': 0.9,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': None,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 100,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': False,\n",
      " 'target_columns': None,\n",
      " 'task_names': None,\n",
      " 'test': False,\n",
      " 'test_dropout': True,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': None,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0,\n",
      " 'weight_decay_sgld': 0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43996it [00:00, 50033.88it/s] \n",
      "100%|██████████| 50000/50000 [00:00<00:00, 131488.74it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 13659.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.1, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 7.5858e-03, PNorm = 49.3504, GNorm = 3.0082, lr_0 = 5.5056e-04\n",
      "Validation mae = 13.446315\n",
      "Epoch 1\n",
      "Loss = 4.0954e-03, PNorm = 51.6300, GNorm = 0.9964, lr_0 = 9.9999e-04\n",
      "Validation mae = 10.057201\n",
      "Epoch 2\n",
      "Loss = 3.0217e-03, PNorm = 54.6649, GNorm = 0.7063, lr_0 = 9.9075e-04\n",
      "Validation mae = 7.887252\n",
      "Epoch 3\n",
      "Loss = 2.2959e-03, PNorm = 57.2296, GNorm = 0.5200, lr_0 = 9.8159e-04\n",
      "Validation mae = 6.646696\n",
      "Epoch 4\n",
      "Loss = 2.0291e-03, PNorm = 59.9616, GNorm = 1.3232, lr_0 = 9.7252e-04\n",
      "Validation mae = 6.043604\n",
      "Epoch 5\n",
      "Loss = 1.8008e-03, PNorm = 62.9302, GNorm = 0.4471, lr_0 = 9.6353e-04\n",
      "Validation mae = 5.899212\n",
      "Epoch 6\n",
      "Loss = 1.7200e-03, PNorm = 66.0375, GNorm = 0.3274, lr_0 = 9.5463e-04\n",
      "Validation mae = 5.536941\n",
      "Epoch 7\n",
      "Loss = 1.6091e-03, PNorm = 69.2617, GNorm = 0.4559, lr_0 = 9.4580e-04\n",
      "Validation mae = 5.271653\n",
      "Epoch 8\n",
      "Loss = 1.5366e-03, PNorm = 72.4906, GNorm = 0.4407, lr_0 = 9.3706e-04\n",
      "Validation mae = 5.532123\n",
      "Epoch 9\n",
      "Loss = 1.4459e-03, PNorm = 75.6749, GNorm = 0.2901, lr_0 = 9.2840e-04\n",
      "Validation mae = 5.114385\n",
      "Epoch 10\n",
      "Loss = 1.3633e-03, PNorm = 78.7618, GNorm = 0.4986, lr_0 = 9.1982e-04\n",
      "Validation mae = 5.302336\n",
      "Epoch 11\n",
      "Loss = 1.3223e-03, PNorm = 81.7845, GNorm = 0.3808, lr_0 = 9.1132e-04\n",
      "Validation mae = 4.580142\n",
      "Epoch 12\n",
      "Loss = 1.2626e-03, PNorm = 84.7600, GNorm = 0.4816, lr_0 = 9.0290e-04\n",
      "Validation mae = 4.507955\n",
      "Epoch 13\n",
      "Loss = 1.2075e-03, PNorm = 87.6968, GNorm = 0.3796, lr_0 = 8.9456e-04\n",
      "Validation mae = 4.632121\n",
      "Epoch 14\n",
      "Loss = 1.1550e-03, PNorm = 90.4773, GNorm = 0.2611, lr_0 = 8.8629e-04\n",
      "Validation mae = 4.535764\n",
      "Epoch 15\n",
      "Loss = 1.1660e-03, PNorm = 93.5421, GNorm = 0.6053, lr_0 = 8.7810e-04\n",
      "Validation mae = 4.562695\n",
      "Epoch 16\n",
      "Loss = 1.1123e-03, PNorm = 96.1665, GNorm = 0.2652, lr_0 = 8.6998e-04\n",
      "Validation mae = 4.715404\n",
      "Epoch 17\n",
      "Loss = 1.0648e-03, PNorm = 98.7278, GNorm = 0.2819, lr_0 = 8.6194e-04\n",
      "Validation mae = 4.509062\n",
      "Epoch 18\n",
      "Loss = 1.0454e-03, PNorm = 101.1745, GNorm = 0.5560, lr_0 = 8.5398e-04\n",
      "Validation mae = 4.304481\n",
      "Epoch 19\n",
      "Loss = 1.0194e-03, PNorm = 103.5852, GNorm = 0.1753, lr_0 = 8.4609e-04\n",
      "Validation mae = 3.721314\n",
      "Epoch 20\n",
      "Loss = 9.7519e-04, PNorm = 105.8696, GNorm = 0.3005, lr_0 = 8.3827e-04\n",
      "Validation mae = 4.291095\n",
      "Epoch 21\n",
      "Loss = 1.0102e-03, PNorm = 108.4091, GNorm = 0.3075, lr_0 = 8.3052e-04\n",
      "Validation mae = 3.935348\n",
      "Epoch 22\n",
      "Loss = 9.7622e-04, PNorm = 110.5720, GNorm = 0.2852, lr_0 = 8.2284e-04\n",
      "Validation mae = 3.981265\n",
      "Epoch 23\n",
      "Loss = 9.3678e-04, PNorm = 112.5432, GNorm = 0.2691, lr_0 = 8.1524e-04\n",
      "Validation mae = 3.902471\n",
      "Epoch 24\n",
      "Loss = 8.9092e-04, PNorm = 114.4689, GNorm = 0.2177, lr_0 = 8.0771e-04\n",
      "Validation mae = 4.039506\n",
      "Epoch 25\n",
      "Loss = 8.9892e-04, PNorm = 116.5498, GNorm = 0.1828, lr_0 = 8.0024e-04\n",
      "Validation mae = 3.546576\n",
      "Epoch 26\n",
      "Loss = 8.8473e-04, PNorm = 118.5865, GNorm = 0.3766, lr_0 = 7.9285e-04\n",
      "Validation mae = 3.639356\n",
      "Epoch 27\n",
      "Loss = 8.5992e-04, PNorm = 120.4006, GNorm = 0.2097, lr_0 = 7.8552e-04\n",
      "Validation mae = 3.567004\n",
      "Epoch 28\n",
      "Loss = 8.4855e-04, PNorm = 122.2396, GNorm = 0.1379, lr_0 = 7.7826e-04\n",
      "Validation mae = 3.581959\n",
      "Epoch 29\n",
      "Loss = 8.4292e-04, PNorm = 124.0732, GNorm = 0.3460, lr_0 = 7.7107e-04\n",
      "Validation mae = 3.941581\n",
      "Epoch 30\n",
      "Loss = 8.4592e-04, PNorm = 125.7766, GNorm = 0.3921, lr_0 = 7.6394e-04\n",
      "Validation mae = 3.437489\n",
      "Epoch 31\n",
      "Loss = 8.2423e-04, PNorm = 127.4785, GNorm = 0.1744, lr_0 = 7.5688e-04\n",
      "Validation mae = 3.430946\n",
      "Epoch 32\n",
      "Loss = 8.0232e-04, PNorm = 128.9970, GNorm = 0.1953, lr_0 = 7.4989e-04\n",
      "Validation mae = 4.130303\n",
      "Epoch 33\n",
      "Loss = 8.1057e-04, PNorm = 130.6830, GNorm = 0.1997, lr_0 = 7.4296e-04\n",
      "Validation mae = 3.419845\n",
      "Epoch 34\n",
      "Loss = 8.0769e-04, PNorm = 132.3121, GNorm = 0.1655, lr_0 = 7.3609e-04\n",
      "Validation mae = 3.969605\n",
      "Epoch 35\n",
      "Loss = 7.9043e-04, PNorm = 133.7578, GNorm = 0.5081, lr_0 = 7.2929e-04\n",
      "Validation mae = 3.276847\n",
      "Epoch 36\n",
      "Loss = 7.5808e-04, PNorm = 135.1084, GNorm = 0.1255, lr_0 = 7.2255e-04\n",
      "Validation mae = 3.496642\n",
      "Epoch 37\n",
      "Loss = 7.5451e-04, PNorm = 136.4909, GNorm = 0.1267, lr_0 = 7.1587e-04\n",
      "Validation mae = 3.376055\n",
      "Epoch 38\n",
      "Loss = 7.3147e-04, PNorm = 137.8131, GNorm = 0.1400, lr_0 = 7.0925e-04\n",
      "Validation mae = 3.420057\n",
      "Epoch 39\n",
      "Loss = 7.3822e-04, PNorm = 139.2253, GNorm = 0.1883, lr_0 = 7.0270e-04\n",
      "Validation mae = 3.478293\n",
      "Epoch 40\n",
      "Loss = 7.3612e-04, PNorm = 140.6155, GNorm = 0.2376, lr_0 = 6.9620e-04\n",
      "Validation mae = 3.276519\n",
      "Epoch 41\n",
      "Loss = 7.2477e-04, PNorm = 141.9407, GNorm = 0.1322, lr_0 = 6.8977e-04\n",
      "Validation mae = 3.226799\n",
      "Epoch 42\n",
      "Loss = 7.0462e-04, PNorm = 143.2012, GNorm = 0.1412, lr_0 = 6.8340e-04\n",
      "Validation mae = 3.187567\n",
      "Epoch 43\n",
      "Loss = 6.9399e-04, PNorm = 144.4349, GNorm = 0.1932, lr_0 = 6.7708e-04\n",
      "Validation mae = 3.161745\n",
      "Epoch 44\n",
      "Loss = 6.9511e-04, PNorm = 145.6499, GNorm = 0.1397, lr_0 = 6.7082e-04\n",
      "Validation mae = 3.097517\n",
      "Epoch 45\n",
      "Loss = 6.8623e-04, PNorm = 146.8402, GNorm = 0.2038, lr_0 = 6.6462e-04\n",
      "Validation mae = 3.098968\n",
      "Epoch 46\n",
      "Loss = 6.7839e-04, PNorm = 147.9477, GNorm = 0.3015, lr_0 = 6.5848e-04\n",
      "Validation mae = 3.133497\n",
      "Epoch 47\n",
      "Loss = 6.7028e-04, PNorm = 149.1140, GNorm = 0.3301, lr_0 = 6.5240e-04\n",
      "Validation mae = 3.521286\n",
      "Epoch 48\n",
      "Loss = 6.7377e-04, PNorm = 150.2483, GNorm = 0.3010, lr_0 = 6.4637e-04\n",
      "Validation mae = 3.122082\n",
      "Epoch 49\n",
      "Loss = 6.5862e-04, PNorm = 151.2711, GNorm = 0.1949, lr_0 = 6.4039e-04\n",
      "Validation mae = 3.216834\n",
      "Epoch 50\n",
      "Loss = 6.6274e-04, PNorm = 152.3534, GNorm = 0.1996, lr_0 = 6.3447e-04\n",
      "Validation mae = 3.176619\n",
      "Epoch 51\n",
      "Loss = 6.4833e-04, PNorm = 153.3349, GNorm = 0.1346, lr_0 = 6.2861e-04\n",
      "Validation mae = 3.105067\n",
      "Epoch 52\n",
      "Loss = 6.1948e-04, PNorm = 154.2650, GNorm = 0.1625, lr_0 = 6.2280e-04\n",
      "Validation mae = 2.988245\n",
      "Epoch 53\n",
      "Loss = 6.1869e-04, PNorm = 155.2393, GNorm = 0.1415, lr_0 = 6.1705e-04\n",
      "Validation mae = 3.184029\n",
      "Epoch 54\n",
      "Loss = 6.2947e-04, PNorm = 156.2471, GNorm = 0.1975, lr_0 = 6.1134e-04\n",
      "Validation mae = 3.090808\n",
      "Epoch 55\n",
      "Loss = 6.1369e-04, PNorm = 157.2124, GNorm = 0.1975, lr_0 = 6.0569e-04\n",
      "Validation mae = 2.986777\n",
      "Epoch 56\n",
      "Loss = 6.2492e-04, PNorm = 158.1429, GNorm = 0.5109, lr_0 = 6.0010e-04\n",
      "Validation mae = 3.463795\n",
      "Epoch 57\n",
      "Loss = 6.0580e-04, PNorm = 159.0365, GNorm = 0.1089, lr_0 = 5.9455e-04\n",
      "Validation mae = 3.028703\n",
      "Epoch 58\n",
      "Loss = 5.9544e-04, PNorm = 159.8839, GNorm = 0.2597, lr_0 = 5.8906e-04\n",
      "Validation mae = 3.159039\n",
      "Epoch 59\n",
      "Loss = 5.9343e-04, PNorm = 160.7376, GNorm = 0.3543, lr_0 = 5.8361e-04\n",
      "Validation mae = 3.027201\n",
      "Epoch 60\n",
      "Loss = 5.9499e-04, PNorm = 161.6157, GNorm = 0.4393, lr_0 = 5.7822e-04\n",
      "Validation mae = 3.122274\n",
      "Epoch 61\n",
      "Loss = 5.7450e-04, PNorm = 162.4256, GNorm = 0.1694, lr_0 = 5.7287e-04\n",
      "Validation mae = 3.138493\n",
      "Epoch 62\n",
      "Loss = 5.9339e-04, PNorm = 163.2699, GNorm = 0.1376, lr_0 = 5.6758e-04\n",
      "Validation mae = 3.046806\n",
      "Epoch 63\n",
      "Loss = 5.7603e-04, PNorm = 164.0863, GNorm = 0.1179, lr_0 = 5.6233e-04\n",
      "Validation mae = 2.948077\n",
      "Epoch 64\n",
      "Loss = 5.7012e-04, PNorm = 164.8831, GNorm = 0.1308, lr_0 = 5.5714e-04\n",
      "Validation mae = 2.925846\n",
      "Epoch 65\n",
      "Loss = 5.7322e-04, PNorm = 165.6728, GNorm = 0.1265, lr_0 = 5.5199e-04\n",
      "Validation mae = 2.872685\n",
      "Epoch 66\n",
      "Loss = 5.4936e-04, PNorm = 166.3397, GNorm = 0.2043, lr_0 = 5.4689e-04\n",
      "Validation mae = 3.076169\n",
      "Epoch 67\n",
      "Loss = 5.6078e-04, PNorm = 167.1293, GNorm = 0.1527, lr_0 = 5.4183e-04\n",
      "Validation mae = 2.822405\n",
      "Epoch 68\n",
      "Loss = 5.5252e-04, PNorm = 167.8227, GNorm = 0.2033, lr_0 = 5.3683e-04\n",
      "Validation mae = 2.903308\n",
      "Epoch 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 5.6047e-04, PNorm = 168.5862, GNorm = 0.3261, lr_0 = 5.3186e-04\n",
      "Validation mae = 3.092782\n",
      "Epoch 70\n",
      "Loss = 5.4370e-04, PNorm = 169.2516, GNorm = 0.1202, lr_0 = 5.2695e-04\n",
      "Validation mae = 3.042189\n",
      "Epoch 71\n",
      "Loss = 5.4269e-04, PNorm = 169.9423, GNorm = 0.1563, lr_0 = 5.2208e-04\n",
      "Validation mae = 3.111689\n",
      "Epoch 72\n",
      "Loss = 5.4389e-04, PNorm = 170.5885, GNorm = 0.1389, lr_0 = 5.1725e-04\n",
      "Validation mae = 2.809120\n",
      "Epoch 73\n",
      "Loss = 5.3294e-04, PNorm = 171.2383, GNorm = 0.1105, lr_0 = 5.1247e-04\n",
      "Validation mae = 2.775399\n",
      "Epoch 74\n",
      "Loss = 5.1902e-04, PNorm = 171.8482, GNorm = 0.1105, lr_0 = 5.0774e-04\n",
      "Validation mae = 2.736141\n",
      "Epoch 75\n",
      "Loss = 5.2317e-04, PNorm = 172.5105, GNorm = 0.1700, lr_0 = 5.0305e-04\n",
      "Validation mae = 2.815205\n",
      "Epoch 76\n",
      "Loss = 5.3230e-04, PNorm = 173.1640, GNorm = 0.3042, lr_0 = 4.9840e-04\n",
      "Validation mae = 2.892644\n",
      "Epoch 77\n",
      "Loss = 5.1875e-04, PNorm = 173.7112, GNorm = 0.1585, lr_0 = 4.9379e-04\n",
      "Validation mae = 2.832198\n",
      "Epoch 78\n",
      "Loss = 5.0980e-04, PNorm = 174.2837, GNorm = 0.2569, lr_0 = 4.8923e-04\n",
      "Validation mae = 2.820694\n",
      "Epoch 79\n",
      "Loss = 5.2464e-04, PNorm = 174.8875, GNorm = 0.1388, lr_0 = 4.8471e-04\n",
      "Validation mae = 3.246835\n",
      "Epoch 80\n",
      "Loss = 5.1456e-04, PNorm = 175.4697, GNorm = 0.1128, lr_0 = 4.8023e-04\n",
      "Validation mae = 2.879739\n",
      "Epoch 81\n",
      "Loss = 5.0488e-04, PNorm = 176.0059, GNorm = 0.1511, lr_0 = 4.7579e-04\n",
      "Validation mae = 2.785743\n",
      "Epoch 82\n",
      "Loss = 4.9075e-04, PNorm = 176.5586, GNorm = 0.1980, lr_0 = 4.7139e-04\n",
      "Validation mae = 2.775575\n",
      "Epoch 83\n",
      "Loss = 5.0121e-04, PNorm = 177.1423, GNorm = 0.1344, lr_0 = 4.6704e-04\n",
      "Validation mae = 2.709833\n",
      "Epoch 84\n",
      "Loss = 4.8995e-04, PNorm = 177.6789, GNorm = 0.1760, lr_0 = 4.6272e-04\n",
      "Validation mae = 2.850588\n",
      "Epoch 85\n",
      "Loss = 5.0256e-04, PNorm = 178.2289, GNorm = 0.1177, lr_0 = 4.5844e-04\n",
      "Validation mae = 2.788092\n",
      "Epoch 86\n",
      "Loss = 4.8583e-04, PNorm = 178.7499, GNorm = 0.1615, lr_0 = 4.5421e-04\n",
      "Validation mae = 2.695935\n",
      "Epoch 87\n",
      "Loss = 4.8608e-04, PNorm = 179.2458, GNorm = 0.1416, lr_0 = 4.5001e-04\n",
      "Validation mae = 2.654897\n",
      "Epoch 88\n",
      "Loss = 4.8719e-04, PNorm = 179.7397, GNorm = 0.2155, lr_0 = 4.4585e-04\n",
      "Validation mae = 2.774200\n",
      "Epoch 89\n",
      "Loss = 4.7870e-04, PNorm = 180.2217, GNorm = 0.1203, lr_0 = 4.4173e-04\n",
      "Validation mae = 2.705593\n",
      "Epoch 90\n",
      "Loss = 4.6966e-04, PNorm = 180.6873, GNorm = 0.1663, lr_0 = 4.3765e-04\n",
      "Validation mae = 2.844524\n",
      "Epoch 91\n",
      "Loss = 4.7829e-04, PNorm = 181.1792, GNorm = 0.1280, lr_0 = 4.3360e-04\n",
      "Validation mae = 2.706719\n",
      "Epoch 92\n",
      "Loss = 4.7997e-04, PNorm = 181.6554, GNorm = 0.1982, lr_0 = 4.2960e-04\n",
      "Validation mae = 2.772624\n",
      "Epoch 93\n",
      "Loss = 4.7495e-04, PNorm = 182.1336, GNorm = 0.3215, lr_0 = 4.2563e-04\n",
      "Validation mae = 2.770334\n",
      "Epoch 94\n",
      "Loss = 4.7897e-04, PNorm = 182.5869, GNorm = 0.1174, lr_0 = 4.2169e-04\n",
      "Validation mae = 2.666207\n",
      "Epoch 95\n",
      "Loss = 4.6258e-04, PNorm = 182.9889, GNorm = 0.1544, lr_0 = 4.1779e-04\n",
      "Validation mae = 2.670496\n",
      "Epoch 96\n",
      "Loss = 4.6445e-04, PNorm = 183.4326, GNorm = 0.1388, lr_0 = 4.1393e-04\n",
      "Validation mae = 2.606083\n",
      "Epoch 97\n",
      "Loss = 4.5783e-04, PNorm = 183.8461, GNorm = 0.1552, lr_0 = 4.1011e-04\n",
      "Validation mae = 2.647211\n",
      "Epoch 98\n",
      "Loss = 4.5527e-04, PNorm = 184.2617, GNorm = 0.4364, lr_0 = 4.0632e-04\n",
      "Validation mae = 2.676169\n",
      "Epoch 99\n",
      "Loss = 4.6435e-04, PNorm = 184.7248, GNorm = 0.1282, lr_0 = 4.0256e-04\n",
      "Validation mae = 2.670294\n",
      "Epoch 100\n",
      "Loss = 4.5137e-04, PNorm = 185.1204, GNorm = 0.1321, lr_0 = 3.9884e-04\n",
      "Validation mae = 2.542393\n",
      "Epoch 101\n",
      "Loss = 4.5120e-04, PNorm = 185.5032, GNorm = 0.4025, lr_0 = 3.9516e-04\n",
      "Validation mae = 2.535114\n",
      "Epoch 102\n",
      "Loss = 4.4733e-04, PNorm = 185.8862, GNorm = 0.0980, lr_0 = 3.9150e-04\n",
      "Validation mae = 2.572455\n",
      "Epoch 103\n",
      "Loss = 4.4369e-04, PNorm = 186.2712, GNorm = 0.1648, lr_0 = 3.8789e-04\n",
      "Validation mae = 2.601695\n",
      "Epoch 104\n",
      "Loss = 4.3810e-04, PNorm = 186.6457, GNorm = 0.1242, lr_0 = 3.8430e-04\n",
      "Validation mae = 2.588195\n",
      "Epoch 105\n",
      "Loss = 4.4150e-04, PNorm = 187.0249, GNorm = 0.2382, lr_0 = 3.8075e-04\n",
      "Validation mae = 2.772209\n",
      "Epoch 106\n",
      "Loss = 4.4627e-04, PNorm = 187.4186, GNorm = 0.1595, lr_0 = 3.7723e-04\n",
      "Validation mae = 2.745355\n",
      "Epoch 107\n",
      "Loss = 4.3606e-04, PNorm = 187.7624, GNorm = 0.1558, lr_0 = 3.7375e-04\n",
      "Validation mae = 2.659359\n",
      "Epoch 108\n",
      "Loss = 4.3102e-04, PNorm = 188.1106, GNorm = 0.1208, lr_0 = 3.7029e-04\n",
      "Validation mae = 2.720973\n",
      "Epoch 109\n",
      "Loss = 4.2720e-04, PNorm = 188.4425, GNorm = 0.1368, lr_0 = 3.6687e-04\n",
      "Validation mae = 2.808208\n",
      "Epoch 110\n",
      "Loss = 4.4135e-04, PNorm = 188.8181, GNorm = 0.1885, lr_0 = 3.6348e-04\n",
      "Validation mae = 2.574164\n",
      "Epoch 111\n",
      "Loss = 4.3719e-04, PNorm = 189.1650, GNorm = 0.1595, lr_0 = 3.6012e-04\n",
      "Validation mae = 2.675887\n",
      "Epoch 112\n",
      "Loss = 4.2849e-04, PNorm = 189.4623, GNorm = 0.1926, lr_0 = 3.5679e-04\n",
      "Validation mae = 2.619151\n",
      "Epoch 113\n",
      "Loss = 4.2574e-04, PNorm = 189.7805, GNorm = 0.4033, lr_0 = 3.5349e-04\n",
      "Validation mae = 2.520947\n",
      "Epoch 114\n",
      "Loss = 4.2525e-04, PNorm = 190.1115, GNorm = 0.0915, lr_0 = 3.5023e-04\n",
      "Validation mae = 2.675252\n",
      "Epoch 115\n",
      "Loss = 4.1881e-04, PNorm = 190.3991, GNorm = 0.1967, lr_0 = 3.4699e-04\n",
      "Validation mae = 2.522113\n",
      "Epoch 116\n",
      "Loss = 4.1669e-04, PNorm = 190.7157, GNorm = 0.1212, lr_0 = 3.4378e-04\n",
      "Validation mae = 2.761565\n",
      "Epoch 117\n",
      "Loss = 4.1646e-04, PNorm = 191.0172, GNorm = 0.2661, lr_0 = 3.4061e-04\n",
      "Validation mae = 2.585622\n",
      "Epoch 118\n",
      "Loss = 4.2055e-04, PNorm = 191.3277, GNorm = 0.1139, lr_0 = 3.3746e-04\n",
      "Validation mae = 2.550833\n",
      "Epoch 119\n",
      "Loss = 4.1696e-04, PNorm = 191.6187, GNorm = 0.1219, lr_0 = 3.3434e-04\n",
      "Validation mae = 2.602211\n",
      "Epoch 120\n",
      "Loss = 4.1176e-04, PNorm = 191.9168, GNorm = 0.4916, lr_0 = 3.3125e-04\n",
      "Validation mae = 2.472779\n",
      "Epoch 121\n",
      "Loss = 4.1299e-04, PNorm = 192.1839, GNorm = 0.1205, lr_0 = 3.2819e-04\n",
      "Validation mae = 2.496586\n",
      "Epoch 122\n",
      "Loss = 4.0818e-04, PNorm = 192.4577, GNorm = 0.3388, lr_0 = 3.2516e-04\n",
      "Validation mae = 2.527601\n",
      "Epoch 123\n",
      "Loss = 4.0973e-04, PNorm = 192.7522, GNorm = 0.1218, lr_0 = 3.2215e-04\n",
      "Validation mae = 2.544217\n",
      "Epoch 124\n",
      "Loss = 4.0399e-04, PNorm = 193.0087, GNorm = 0.0861, lr_0 = 3.1917e-04\n",
      "Validation mae = 2.584626\n",
      "Epoch 125\n",
      "Loss = 4.1147e-04, PNorm = 193.2811, GNorm = 0.1359, lr_0 = 3.1622e-04\n",
      "Validation mae = 2.537367\n",
      "Epoch 126\n",
      "Loss = 4.1229e-04, PNorm = 193.5485, GNorm = 0.1425, lr_0 = 3.1330e-04\n",
      "Validation mae = 2.561583\n",
      "Epoch 127\n",
      "Loss = 4.0123e-04, PNorm = 193.7933, GNorm = 0.1690, lr_0 = 3.1041e-04\n",
      "Validation mae = 2.506098\n",
      "Epoch 128\n",
      "Loss = 4.0129e-04, PNorm = 194.0455, GNorm = 0.1021, lr_0 = 3.0754e-04\n",
      "Validation mae = 2.541742\n",
      "Epoch 129\n",
      "Loss = 3.9718e-04, PNorm = 194.2902, GNorm = 0.2538, lr_0 = 3.0470e-04\n",
      "Validation mae = 2.723401\n",
      "Epoch 130\n",
      "Loss = 3.9459e-04, PNorm = 194.5531, GNorm = 0.1126, lr_0 = 3.0188e-04\n",
      "Validation mae = 2.422391\n",
      "Epoch 131\n",
      "Loss = 4.0758e-04, PNorm = 194.8018, GNorm = 0.1377, lr_0 = 2.9909e-04\n",
      "Validation mae = 2.485539\n",
      "Epoch 132\n",
      "Loss = 3.9518e-04, PNorm = 195.0247, GNorm = 0.1215, lr_0 = 2.9633e-04\n",
      "Validation mae = 2.717449\n",
      "Epoch 133\n",
      "Loss = 3.9130e-04, PNorm = 195.2700, GNorm = 0.0901, lr_0 = 2.9359e-04\n",
      "Validation mae = 2.474418\n",
      "Epoch 134\n",
      "Loss = 3.8614e-04, PNorm = 195.5106, GNorm = 0.0979, lr_0 = 2.9087e-04\n",
      "Validation mae = 2.478267\n",
      "Epoch 135\n",
      "Loss = 3.8398e-04, PNorm = 195.7413, GNorm = 0.1146, lr_0 = 2.8819e-04\n",
      "Validation mae = 2.480837\n",
      "Epoch 136\n",
      "Loss = 3.9310e-04, PNorm = 195.9679, GNorm = 0.1740, lr_0 = 2.8552e-04\n",
      "Validation mae = 2.566556\n",
      "Epoch 137\n",
      "Loss = 3.8615e-04, PNorm = 196.2070, GNorm = 0.1210, lr_0 = 2.8288e-04\n",
      "Validation mae = 2.508050\n",
      "Epoch 138\n",
      "Loss = 3.8388e-04, PNorm = 196.4269, GNorm = 0.1173, lr_0 = 2.8027e-04\n",
      "Validation mae = 2.494270\n",
      "Epoch 139\n",
      "Loss = 3.9189e-04, PNorm = 196.6638, GNorm = 0.1292, lr_0 = 2.7768e-04\n",
      "Validation mae = 2.652298\n",
      "Epoch 140\n",
      "Loss = 3.7944e-04, PNorm = 196.8990, GNorm = 0.2382, lr_0 = 2.7511e-04\n",
      "Validation mae = 2.455959\n",
      "Epoch 141\n",
      "Loss = 3.8567e-04, PNorm = 197.1159, GNorm = 0.1135, lr_0 = 2.7257e-04\n",
      "Validation mae = 2.470289\n",
      "Epoch 142\n",
      "Loss = 3.8312e-04, PNorm = 197.3259, GNorm = 0.0850, lr_0 = 2.7005e-04\n",
      "Validation mae = 2.426505\n",
      "Epoch 143\n",
      "Loss = 3.7983e-04, PNorm = 197.5391, GNorm = 0.1098, lr_0 = 2.6756e-04\n",
      "Validation mae = 2.431541\n",
      "Epoch 144\n",
      "Loss = 3.7581e-04, PNorm = 197.7529, GNorm = 0.1052, lr_0 = 2.6508e-04\n",
      "Validation mae = 2.398444\n",
      "Epoch 145\n",
      "Loss = 3.7801e-04, PNorm = 197.9488, GNorm = 0.1145, lr_0 = 2.6263e-04\n",
      "Validation mae = 2.443838\n",
      "Epoch 146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 3.7833e-04, PNorm = 198.1248, GNorm = 0.1082, lr_0 = 2.6021e-04\n",
      "Validation mae = 2.366049\n",
      "Epoch 147\n",
      "Loss = 3.7411e-04, PNorm = 198.3311, GNorm = 0.0918, lr_0 = 2.5780e-04\n",
      "Validation mae = 2.383507\n",
      "Epoch 148\n",
      "Loss = 3.7508e-04, PNorm = 198.5238, GNorm = 0.0848, lr_0 = 2.5542e-04\n",
      "Validation mae = 2.423979\n",
      "Epoch 149\n",
      "Loss = 3.7203e-04, PNorm = 198.7144, GNorm = 0.1037, lr_0 = 2.5306e-04\n",
      "Validation mae = 2.433752\n",
      "Epoch 150\n",
      "Loss = 3.6902e-04, PNorm = 198.8876, GNorm = 0.1092, lr_0 = 2.5072e-04\n",
      "Validation mae = 2.420938\n",
      "Epoch 151\n",
      "Loss = 3.7394e-04, PNorm = 199.0645, GNorm = 0.1066, lr_0 = 2.4840e-04\n",
      "Validation mae = 2.342943\n",
      "Epoch 152\n",
      "Loss = 3.6581e-04, PNorm = 199.2369, GNorm = 0.1109, lr_0 = 2.4611e-04\n",
      "Validation mae = 2.331077\n",
      "Epoch 153\n",
      "Loss = 3.7052e-04, PNorm = 199.4257, GNorm = 0.1024, lr_0 = 2.4383e-04\n",
      "Validation mae = 2.436813\n",
      "Epoch 154\n",
      "Loss = 3.7025e-04, PNorm = 199.6050, GNorm = 0.1223, lr_0 = 2.4158e-04\n",
      "Validation mae = 2.423140\n",
      "Epoch 155\n",
      "Loss = 3.6365e-04, PNorm = 199.7720, GNorm = 0.1148, lr_0 = 2.3935e-04\n",
      "Validation mae = 2.520785\n",
      "Epoch 156\n",
      "Loss = 3.6504e-04, PNorm = 199.9379, GNorm = 0.1110, lr_0 = 2.3713e-04\n",
      "Validation mae = 2.341693\n",
      "Epoch 157\n",
      "Loss = 3.6221e-04, PNorm = 200.1066, GNorm = 0.0936, lr_0 = 2.3494e-04\n",
      "Validation mae = 2.386637\n",
      "Epoch 158\n",
      "Loss = 3.6430e-04, PNorm = 200.2785, GNorm = 0.5503, lr_0 = 2.3277e-04\n",
      "Validation mae = 2.463548\n",
      "Epoch 159\n",
      "Loss = 3.6263e-04, PNorm = 200.4360, GNorm = 0.1177, lr_0 = 2.3062e-04\n",
      "Validation mae = 2.331399\n",
      "Epoch 160\n",
      "Loss = 3.6049e-04, PNorm = 200.5792, GNorm = 0.1102, lr_0 = 2.2849e-04\n",
      "Validation mae = 2.410099\n",
      "Epoch 161\n",
      "Loss = 3.5994e-04, PNorm = 200.7383, GNorm = 0.1067, lr_0 = 2.2638e-04\n",
      "Validation mae = 2.342330\n",
      "Epoch 162\n",
      "Loss = 3.5831e-04, PNorm = 200.8954, GNorm = 0.1517, lr_0 = 2.2429e-04\n",
      "Validation mae = 2.462162\n",
      "Epoch 163\n",
      "Loss = 3.5593e-04, PNorm = 201.0606, GNorm = 0.0879, lr_0 = 2.2221e-04\n",
      "Validation mae = 2.401998\n",
      "Epoch 164\n",
      "Loss = 3.5768e-04, PNorm = 201.2251, GNorm = 0.1151, lr_0 = 2.2016e-04\n",
      "Validation mae = 2.407330\n",
      "Epoch 165\n",
      "Loss = 3.5438e-04, PNorm = 201.3692, GNorm = 0.1132, lr_0 = 2.1812e-04\n",
      "Validation mae = 2.314580\n",
      "Epoch 166\n",
      "Loss = 3.5659e-04, PNorm = 201.5320, GNorm = 0.0977, lr_0 = 2.1611e-04\n",
      "Validation mae = 2.336610\n",
      "Epoch 167\n",
      "Loss = 3.5675e-04, PNorm = 201.6739, GNorm = 0.1335, lr_0 = 2.1411e-04\n",
      "Validation mae = 2.396053\n",
      "Epoch 168\n",
      "Loss = 3.5591e-04, PNorm = 201.8171, GNorm = 0.1647, lr_0 = 2.1213e-04\n",
      "Validation mae = 2.400192\n",
      "Epoch 169\n",
      "Loss = 3.5057e-04, PNorm = 201.9546, GNorm = 0.1268, lr_0 = 2.1017e-04\n",
      "Validation mae = 2.320127\n",
      "Epoch 170\n",
      "Loss = 3.5180e-04, PNorm = 202.1037, GNorm = 0.1040, lr_0 = 2.0823e-04\n",
      "Validation mae = 2.320614\n",
      "Epoch 171\n",
      "Loss = 3.5063e-04, PNorm = 202.2265, GNorm = 0.0982, lr_0 = 2.0631e-04\n",
      "Validation mae = 2.296285\n",
      "Epoch 172\n",
      "Loss = 3.4920e-04, PNorm = 202.3473, GNorm = 0.1288, lr_0 = 2.0440e-04\n",
      "Validation mae = 2.421002\n",
      "Epoch 173\n",
      "Loss = 3.4907e-04, PNorm = 202.4751, GNorm = 0.0956, lr_0 = 2.0251e-04\n",
      "Validation mae = 2.358726\n",
      "Epoch 174\n",
      "Loss = 3.4655e-04, PNorm = 202.6059, GNorm = 0.1056, lr_0 = 2.0064e-04\n",
      "Validation mae = 2.356002\n",
      "Epoch 175\n",
      "Loss = 3.4498e-04, PNorm = 202.7318, GNorm = 0.1007, lr_0 = 1.9878e-04\n",
      "Validation mae = 2.364075\n",
      "Epoch 176\n",
      "Loss = 3.4421e-04, PNorm = 202.8468, GNorm = 0.0960, lr_0 = 1.9695e-04\n",
      "Validation mae = 2.334824\n",
      "Epoch 177\n",
      "Loss = 3.4748e-04, PNorm = 202.9769, GNorm = 0.1039, lr_0 = 1.9513e-04\n",
      "Validation mae = 2.377571\n",
      "Epoch 178\n",
      "Loss = 3.4422e-04, PNorm = 203.1038, GNorm = 0.0932, lr_0 = 1.9332e-04\n",
      "Validation mae = 2.280419\n",
      "Epoch 179\n",
      "Loss = 3.4371e-04, PNorm = 203.2320, GNorm = 0.1061, lr_0 = 1.9154e-04\n",
      "Validation mae = 2.362208\n",
      "Epoch 180\n",
      "Loss = 3.4152e-04, PNorm = 203.3519, GNorm = 0.0845, lr_0 = 1.8977e-04\n",
      "Validation mae = 2.317433\n",
      "Epoch 181\n",
      "Loss = 3.4258e-04, PNorm = 203.4662, GNorm = 0.0845, lr_0 = 1.8801e-04\n",
      "Validation mae = 2.397749\n",
      "Epoch 182\n",
      "Loss = 3.4172e-04, PNorm = 203.5986, GNorm = 0.1190, lr_0 = 1.8628e-04\n",
      "Validation mae = 2.310460\n",
      "Epoch 183\n",
      "Loss = 3.3649e-04, PNorm = 203.7058, GNorm = 0.1056, lr_0 = 1.8455e-04\n",
      "Validation mae = 2.289564\n",
      "Epoch 184\n",
      "Loss = 3.4119e-04, PNorm = 203.8263, GNorm = 0.1046, lr_0 = 1.8285e-04\n",
      "Validation mae = 2.301099\n",
      "Epoch 185\n",
      "Loss = 3.3881e-04, PNorm = 203.9375, GNorm = 0.1483, lr_0 = 1.8116e-04\n",
      "Validation mae = 2.248890\n",
      "Epoch 186\n",
      "Loss = 3.3854e-04, PNorm = 204.0516, GNorm = 0.1243, lr_0 = 1.7948e-04\n",
      "Validation mae = 2.280641\n",
      "Epoch 187\n",
      "Loss = 3.3402e-04, PNorm = 204.1753, GNorm = 0.0959, lr_0 = 1.7783e-04\n",
      "Validation mae = 2.349003\n",
      "Epoch 188\n",
      "Loss = 3.3583e-04, PNorm = 204.2916, GNorm = 0.1078, lr_0 = 1.7618e-04\n",
      "Validation mae = 2.330930\n",
      "Epoch 189\n",
      "Loss = 3.3855e-04, PNorm = 204.3888, GNorm = 0.1286, lr_0 = 1.7455e-04\n",
      "Validation mae = 2.435755\n",
      "Epoch 190\n",
      "Loss = 3.3601e-04, PNorm = 204.4918, GNorm = 0.0915, lr_0 = 1.7294e-04\n",
      "Validation mae = 2.324906\n",
      "Epoch 191\n",
      "Loss = 3.3457e-04, PNorm = 204.6046, GNorm = 0.1270, lr_0 = 1.7134e-04\n",
      "Validation mae = 2.343907\n",
      "Epoch 192\n",
      "Loss = 3.3660e-04, PNorm = 204.7091, GNorm = 0.1161, lr_0 = 1.6976e-04\n",
      "Validation mae = 2.300179\n",
      "Epoch 193\n",
      "Loss = 3.3535e-04, PNorm = 204.8013, GNorm = 0.1016, lr_0 = 1.6819e-04\n",
      "Validation mae = 2.359104\n",
      "Epoch 194\n",
      "Loss = 3.3245e-04, PNorm = 204.8989, GNorm = 0.1388, lr_0 = 1.6664e-04\n",
      "Validation mae = 2.282763\n",
      "Epoch 195\n",
      "Loss = 3.3591e-04, PNorm = 204.9936, GNorm = 0.1296, lr_0 = 1.6510e-04\n",
      "Validation mae = 2.278972\n",
      "Epoch 196\n",
      "Loss = 3.3262e-04, PNorm = 205.1007, GNorm = 0.0952, lr_0 = 1.6357e-04\n",
      "Validation mae = 2.278802\n",
      "Epoch 197\n",
      "Loss = 3.3250e-04, PNorm = 205.1969, GNorm = 0.1015, lr_0 = 1.6206e-04\n",
      "Validation mae = 2.295983\n",
      "Epoch 198\n",
      "Loss = 3.3454e-04, PNorm = 205.2923, GNorm = 0.0982, lr_0 = 1.6056e-04\n",
      "Validation mae = 2.312254\n",
      "Epoch 199\n",
      "Loss = 3.2934e-04, PNorm = 205.3844, GNorm = 0.1030, lr_0 = 1.5908e-04\n",
      "Validation mae = 2.317655\n",
      "Epoch 200\n",
      "Loss = 3.2878e-04, PNorm = 205.4776, GNorm = 0.0909, lr_0 = 1.5761e-04\n",
      "Validation mae = 2.253617\n",
      "Epoch 201\n",
      "Loss = 3.3027e-04, PNorm = 205.5540, GNorm = 0.1123, lr_0 = 1.5615e-04\n",
      "Validation mae = 2.295404\n",
      "Epoch 202\n",
      "Loss = 3.3062e-04, PNorm = 205.6444, GNorm = 0.0850, lr_0 = 1.5471e-04\n",
      "Validation mae = 2.225732\n",
      "Epoch 203\n",
      "Loss = 3.3160e-04, PNorm = 205.7331, GNorm = 0.1273, lr_0 = 1.5328e-04\n",
      "Validation mae = 2.278377\n",
      "Epoch 204\n",
      "Loss = 3.3046e-04, PNorm = 205.8101, GNorm = 0.0953, lr_0 = 1.5186e-04\n",
      "Validation mae = 2.264451\n",
      "Epoch 205\n",
      "Loss = 3.2378e-04, PNorm = 205.8809, GNorm = 0.1130, lr_0 = 1.5046e-04\n",
      "Validation mae = 2.213371\n",
      "Epoch 206\n",
      "Loss = 3.2664e-04, PNorm = 205.9705, GNorm = 0.1041, lr_0 = 1.4907e-04\n",
      "Validation mae = 2.271712\n",
      "Epoch 207\n",
      "Loss = 3.2711e-04, PNorm = 206.0436, GNorm = 0.1082, lr_0 = 1.4769e-04\n",
      "Validation mae = 2.251748\n",
      "Epoch 208\n",
      "Loss = 3.2444e-04, PNorm = 206.1297, GNorm = 0.1021, lr_0 = 1.4632e-04\n",
      "Validation mae = 2.323510\n",
      "Epoch 209\n",
      "Loss = 3.2381e-04, PNorm = 206.2187, GNorm = 0.1004, lr_0 = 1.4497e-04\n",
      "Validation mae = 2.234304\n",
      "Epoch 210\n",
      "Loss = 3.2415e-04, PNorm = 206.2938, GNorm = 0.0868, lr_0 = 1.4363e-04\n",
      "Validation mae = 2.233257\n",
      "Epoch 211\n",
      "Loss = 3.2124e-04, PNorm = 206.3695, GNorm = 0.0827, lr_0 = 1.4231e-04\n",
      "Validation mae = 2.310469\n",
      "Epoch 212\n",
      "Loss = 3.2344e-04, PNorm = 206.4504, GNorm = 0.0987, lr_0 = 1.4099e-04\n",
      "Validation mae = 2.238590\n",
      "Epoch 213\n",
      "Loss = 3.2301e-04, PNorm = 206.5252, GNorm = 0.0878, lr_0 = 1.3969e-04\n",
      "Validation mae = 2.249057\n",
      "Epoch 214\n",
      "Loss = 3.2114e-04, PNorm = 206.6032, GNorm = 0.2547, lr_0 = 1.3840e-04\n",
      "Validation mae = 2.240592\n",
      "Epoch 215\n",
      "Loss = 3.1946e-04, PNorm = 206.6810, GNorm = 0.0932, lr_0 = 1.3712e-04\n",
      "Validation mae = 2.227252\n",
      "Epoch 216\n",
      "Loss = 3.2235e-04, PNorm = 206.7537, GNorm = 0.2563, lr_0 = 1.3585e-04\n",
      "Validation mae = 2.385730\n",
      "Epoch 217\n",
      "Loss = 3.2135e-04, PNorm = 206.8305, GNorm = 0.1022, lr_0 = 1.3459e-04\n",
      "Validation mae = 2.281564\n",
      "Epoch 218\n",
      "Loss = 3.2204e-04, PNorm = 206.9090, GNorm = 0.1166, lr_0 = 1.3335e-04\n",
      "Validation mae = 2.217625\n",
      "Epoch 219\n",
      "Loss = 3.2251e-04, PNorm = 206.9814, GNorm = 0.0921, lr_0 = 1.3212e-04\n",
      "Validation mae = 2.239714\n",
      "Epoch 220\n",
      "Loss = 3.2117e-04, PNorm = 207.0546, GNorm = 0.1200, lr_0 = 1.3090e-04\n",
      "Validation mae = 2.257343\n",
      "Epoch 221\n",
      "Loss = 3.2019e-04, PNorm = 207.1175, GNorm = 0.0767, lr_0 = 1.2969e-04\n",
      "Validation mae = 2.264591\n",
      "Epoch 222\n",
      "Loss = 3.1740e-04, PNorm = 207.1861, GNorm = 0.1095, lr_0 = 1.2849e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 2.223402\n",
      "Epoch 223\n",
      "Loss = 3.1767e-04, PNorm = 207.2461, GNorm = 0.0975, lr_0 = 1.2730e-04\n",
      "Validation mae = 2.261421\n",
      "Epoch 224\n",
      "Loss = 3.1785e-04, PNorm = 207.3153, GNorm = 0.0954, lr_0 = 1.2613e-04\n",
      "Validation mae = 2.213537\n",
      "Epoch 225\n",
      "Loss = 3.1655e-04, PNorm = 207.3830, GNorm = 0.0841, lr_0 = 1.2496e-04\n",
      "Validation mae = 2.230956\n",
      "Epoch 226\n",
      "Loss = 3.1867e-04, PNorm = 207.4441, GNorm = 0.3352, lr_0 = 1.2380e-04\n",
      "Validation mae = 2.227172\n",
      "Epoch 227\n",
      "Loss = 3.1591e-04, PNorm = 207.5038, GNorm = 0.1043, lr_0 = 1.2266e-04\n",
      "Validation mae = 2.310653\n",
      "Epoch 228\n",
      "Loss = 3.1493e-04, PNorm = 207.5676, GNorm = 0.1142, lr_0 = 1.2153e-04\n",
      "Validation mae = 2.235040\n",
      "Epoch 229\n",
      "Loss = 3.1603e-04, PNorm = 207.6281, GNorm = 0.1073, lr_0 = 1.2040e-04\n",
      "Validation mae = 2.220497\n",
      "Epoch 230\n",
      "Loss = 3.1550e-04, PNorm = 207.6886, GNorm = 0.0987, lr_0 = 1.1929e-04\n",
      "Validation mae = 2.250058\n",
      "Epoch 231\n",
      "Loss = 3.1698e-04, PNorm = 207.7503, GNorm = 0.0798, lr_0 = 1.1819e-04\n",
      "Validation mae = 2.261920\n",
      "Epoch 232\n",
      "Loss = 3.1561e-04, PNorm = 207.8224, GNorm = 0.1025, lr_0 = 1.1710e-04\n",
      "Validation mae = 2.265703\n",
      "Epoch 233\n",
      "Loss = 3.1444e-04, PNorm = 207.8839, GNorm = 0.2007, lr_0 = 1.1601e-04\n",
      "Validation mae = 2.232802\n",
      "Epoch 234\n",
      "Loss = 3.1497e-04, PNorm = 207.9410, GNorm = 0.1348, lr_0 = 1.1494e-04\n",
      "Validation mae = 2.260640\n",
      "Epoch 235\n",
      "Loss = 3.1194e-04, PNorm = 207.9967, GNorm = 0.1195, lr_0 = 1.1388e-04\n",
      "Validation mae = 2.321894\n",
      "Epoch 236\n",
      "Loss = 3.1231e-04, PNorm = 208.0524, GNorm = 0.1303, lr_0 = 1.1283e-04\n",
      "Validation mae = 2.216310\n",
      "Epoch 237\n",
      "Loss = 3.1256e-04, PNorm = 208.1084, GNorm = 0.0989, lr_0 = 1.1178e-04\n",
      "Validation mae = 2.233767\n",
      "Epoch 238\n",
      "Loss = 3.1301e-04, PNorm = 208.1621, GNorm = 0.3513, lr_0 = 1.1075e-04\n",
      "Validation mae = 2.289536\n",
      "Epoch 239\n",
      "Loss = 3.1160e-04, PNorm = 208.2247, GNorm = 0.1007, lr_0 = 1.0973e-04\n",
      "Validation mae = 2.204781\n",
      "Epoch 240\n",
      "Loss = 3.1043e-04, PNorm = 208.2772, GNorm = 0.0977, lr_0 = 1.0871e-04\n",
      "Validation mae = 2.232893\n",
      "Epoch 241\n",
      "Loss = 3.1047e-04, PNorm = 208.3373, GNorm = 0.1197, lr_0 = 1.0771e-04\n",
      "Validation mae = 2.194851\n",
      "Epoch 242\n",
      "Loss = 3.1247e-04, PNorm = 208.3888, GNorm = 0.0856, lr_0 = 1.0671e-04\n",
      "Validation mae = 2.177808\n",
      "Epoch 243\n",
      "Loss = 3.1002e-04, PNorm = 208.4462, GNorm = 0.1064, lr_0 = 1.0573e-04\n",
      "Validation mae = 2.233553\n",
      "Epoch 244\n",
      "Loss = 3.0776e-04, PNorm = 208.5011, GNorm = 0.1579, lr_0 = 1.0475e-04\n",
      "Validation mae = 2.226082\n",
      "Epoch 245\n",
      "Loss = 3.0696e-04, PNorm = 208.5522, GNorm = 0.1238, lr_0 = 1.0378e-04\n",
      "Validation mae = 2.248335\n",
      "Epoch 246\n",
      "Loss = 3.0950e-04, PNorm = 208.6069, GNorm = 0.1027, lr_0 = 1.0282e-04\n",
      "Validation mae = 2.214710\n",
      "Epoch 247\n",
      "Loss = 3.0918e-04, PNorm = 208.6534, GNorm = 0.1011, lr_0 = 1.0187e-04\n",
      "Validation mae = 2.205020\n",
      "Epoch 248\n",
      "Loss = 3.1160e-04, PNorm = 208.7047, GNorm = 0.0816, lr_0 = 1.0093e-04\n",
      "Validation mae = 2.204733\n",
      "Epoch 249\n",
      "Loss = 3.1088e-04, PNorm = 208.7577, GNorm = 0.1057, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.146908\n",
      "Model 0 best validation mae = 2.146908 on epoch 249\n",
      "Model 0, sample 0 test mae = 3.538270\n",
      "Model 0, sample 1 test mae = 3.586245\n",
      "Model 0, sample 2 test mae = 3.563208\n",
      "Model 0, sample 3 test mae = 3.546604\n",
      "Model 0, sample 4 test mae = 3.525812\n",
      "Model 0, sample 5 test mae = 3.531480\n",
      "Model 0, sample 6 test mae = 3.588937\n",
      "Model 0, sample 7 test mae = 3.540112\n",
      "Model 0, sample 8 test mae = 3.523608\n",
      "Model 0, sample 9 test mae = 3.620461\n",
      "Model 0, sample 10 test mae = 3.623479\n",
      "Model 0, sample 11 test mae = 3.582538\n",
      "Model 0, sample 12 test mae = 3.608741\n",
      "Model 0, sample 13 test mae = 3.609205\n",
      "Model 0, sample 14 test mae = 3.559520\n",
      "Model 0, sample 15 test mae = 3.615160\n",
      "Model 0, sample 16 test mae = 3.622142\n",
      "Model 0, sample 17 test mae = 3.577991\n",
      "Model 0, sample 18 test mae = 3.543082\n",
      "Model 0, sample 19 test mae = 3.621255\n",
      "Model 0, sample 20 test mae = 3.605350\n",
      "Model 0, sample 21 test mae = 3.613217\n",
      "Model 0, sample 22 test mae = 3.574066\n",
      "Model 0, sample 23 test mae = 3.556197\n",
      "Model 0, sample 24 test mae = 3.530293\n",
      "Model 0, sample 25 test mae = 3.539579\n",
      "Model 0, sample 26 test mae = 3.559968\n",
      "Model 0, sample 27 test mae = 3.573479\n",
      "Model 0, sample 28 test mae = 3.573572\n",
      "Model 0, sample 29 test mae = 3.589870\n",
      "Model 0, sample 30 test mae = 3.570280\n",
      "Model 0, sample 31 test mae = 3.610890\n",
      "Model 0, sample 32 test mae = 3.578162\n",
      "Model 0, sample 33 test mae = 3.587769\n",
      "Model 0, sample 34 test mae = 3.561012\n",
      "Model 0, sample 35 test mae = 3.542940\n",
      "Model 0, sample 36 test mae = 3.587479\n",
      "Model 0, sample 37 test mae = 3.625602\n",
      "Model 0, sample 38 test mae = 3.542390\n",
      "Model 0, sample 39 test mae = 3.608178\n",
      "Model 0, sample 40 test mae = 3.591314\n",
      "Model 0, sample 41 test mae = 3.601868\n",
      "Model 0, sample 42 test mae = 3.536311\n",
      "Model 0, sample 43 test mae = 3.570679\n",
      "Model 0, sample 44 test mae = 3.550456\n",
      "Model 0, sample 45 test mae = 3.603537\n",
      "Model 0, sample 46 test mae = 3.599062\n",
      "Model 0, sample 47 test mae = 3.573580\n",
      "Model 0, sample 48 test mae = 3.550671\n",
      "Model 0, sample 49 test mae = 3.583988\n",
      "Model 0, sample 50 test mae = 3.540730\n",
      "Model 0, sample 51 test mae = 3.570140\n",
      "Model 0, sample 52 test mae = 3.555194\n",
      "Model 0, sample 53 test mae = 3.594180\n",
      "Model 0, sample 54 test mae = 3.566791\n",
      "Model 0, sample 55 test mae = 3.574956\n",
      "Model 0, sample 56 test mae = 3.584162\n",
      "Model 0, sample 57 test mae = 3.583487\n",
      "Model 0, sample 58 test mae = 3.582976\n",
      "Model 0, sample 59 test mae = 3.568851\n",
      "Model 0, sample 60 test mae = 3.548546\n",
      "Model 0, sample 61 test mae = 3.558725\n",
      "Model 0, sample 62 test mae = 3.502697\n",
      "Model 0, sample 63 test mae = 3.544151\n",
      "Model 0, sample 64 test mae = 3.587066\n",
      "Model 0, sample 65 test mae = 3.586101\n",
      "Model 0, sample 66 test mae = 3.577205\n",
      "Model 0, sample 67 test mae = 3.571135\n",
      "Model 0, sample 68 test mae = 3.603588\n",
      "Model 0, sample 69 test mae = 3.587632\n",
      "Model 0, sample 70 test mae = 3.556134\n",
      "Model 0, sample 71 test mae = 3.609586\n",
      "Model 0, sample 72 test mae = 3.550972\n",
      "Model 0, sample 73 test mae = 3.599328\n",
      "Model 0, sample 74 test mae = 3.587293\n",
      "Model 0, sample 75 test mae = 3.575105\n",
      "Model 0, sample 76 test mae = 3.640286\n",
      "Model 0, sample 77 test mae = 3.611541\n",
      "Model 0, sample 78 test mae = 3.571500\n",
      "Model 0, sample 79 test mae = 3.610680\n",
      "Model 0, sample 80 test mae = 3.552628\n",
      "Model 0, sample 81 test mae = 3.563804\n",
      "Model 0, sample 82 test mae = 3.535324\n",
      "Model 0, sample 83 test mae = 3.554289\n",
      "Model 0, sample 84 test mae = 3.564378\n",
      "Model 0, sample 85 test mae = 3.608448\n",
      "Model 0, sample 86 test mae = 3.535597\n",
      "Model 0, sample 87 test mae = 3.579117\n",
      "Model 0, sample 88 test mae = 3.572734\n",
      "Model 0, sample 89 test mae = 3.555642\n",
      "Model 0, sample 90 test mae = 3.609133\n",
      "Model 0, sample 91 test mae = 3.535727\n",
      "Model 0, sample 92 test mae = 3.594165\n",
      "Model 0, sample 93 test mae = 3.616609\n",
      "Model 0, sample 94 test mae = 3.579692\n",
      "Model 0, sample 95 test mae = 3.576923\n",
      "Model 0, sample 96 test mae = 3.588453\n",
      "Model 0, sample 97 test mae = 3.599216\n",
      "Model 0, sample 98 test mae = 3.537146\n",
      "Model 0, sample 99 test mae = 3.589550\n",
      "BMA test mae = 2.246839\n"
     ]
    }
   ],
   "source": [
    "args.epochs = 250\n",
    "args.init_lr = 1e-4\n",
    "args.max_lr = 1e-3\n",
    "args.final_lr = 1e-4\n",
    "args.batch_size = 50\n",
    "args.log_frequency = 800\n",
    "\n",
    "args.ensemble_size = 1\n",
    "args.samples = 100\n",
    "\n",
    "args.dropout = 0.1\n",
    "args.test_dropout = True\n",
    "\n",
    "results_MCdrop = run_training(args)\n",
    "#np.savez(args.save_dir+'/results_MCdrop', results_MCdrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG-Diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-eec2b1f7-f2f6-4a68-beac-8aee973255b6.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'bias': False,\n",
      " 'block': False,\n",
      " 'c_swag': 200,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_swag': 25,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise': 0,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'lr_sgld': 0.001,\n",
      " 'lr_swag': 0.001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 30,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'momentum_swag': 0.5,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'norm_sigma_sgld': 100,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': None,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 30,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': True,\n",
      " 'target_columns': None,\n",
      " 'task_names': None,\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': None,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0.001}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47074it [00:00, 79154.59it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 259260.70it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 13420.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Model 0 best validation mae = inf on epoch 0\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.2671e-05, PNorm = 87.4527, GNorm = 0.0434, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141925\n",
      "SWAG spoch 1\n",
      "Loss = 5.1794e-05, PNorm = 87.3134, GNorm = 0.0335, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138139\n",
      "SWAG spoch 2\n",
      "Loss = 5.1467e-05, PNorm = 87.1742, GNorm = 0.0327, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135514\n",
      "SWAG spoch 3\n",
      "Loss = 5.1343e-05, PNorm = 87.0355, GNorm = 0.0264, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132543\n",
      "SWAG spoch 4\n",
      "Loss = 5.1268e-05, PNorm = 86.8971, GNorm = 0.0294, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131398\n",
      "SWAG spoch 5\n",
      "Loss = 5.1320e-05, PNorm = 86.7590, GNorm = 0.0304, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130107\n",
      "SWAG spoch 6\n",
      "Loss = 5.1412e-05, PNorm = 86.6211, GNorm = 0.0297, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129003\n",
      "SWAG spoch 7\n",
      "Loss = 5.1565e-05, PNorm = 86.4835, GNorm = 0.0389, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127970\n",
      "SWAG spoch 8\n",
      "Loss = 5.1738e-05, PNorm = 86.3459, GNorm = 0.0407, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127302\n",
      "SWAG spoch 9\n",
      "Loss = 5.1942e-05, PNorm = 86.2089, GNorm = 0.0418, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127138\n",
      "SWAG spoch 10\n",
      "Loss = 5.2183e-05, PNorm = 86.0718, GNorm = 0.0336, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.126791\n",
      "SWAG spoch 11\n",
      "Loss = 5.2442e-05, PNorm = 85.9350, GNorm = 0.0334, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127004\n",
      "SWAG spoch 12\n",
      "Loss = 5.2726e-05, PNorm = 85.7986, GNorm = 0.0406, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127079\n",
      "SWAG spoch 13\n",
      "Loss = 5.3030e-05, PNorm = 85.6623, GNorm = 0.0331, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127515\n",
      "SWAG spoch 14\n",
      "Loss = 5.3359e-05, PNorm = 85.5266, GNorm = 0.0346, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127970\n",
      "SWAG spoch 15\n",
      "Loss = 5.3692e-05, PNorm = 85.3908, GNorm = 0.0499, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.128943\n",
      "SWAG spoch 16\n",
      "Loss = 5.4070e-05, PNorm = 85.2553, GNorm = 0.0376, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129344\n",
      "SWAG spoch 17\n",
      "Loss = 5.4428e-05, PNorm = 85.1202, GNorm = 0.0355, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130124\n",
      "SWAG spoch 18\n",
      "Loss = 5.4811e-05, PNorm = 84.9849, GNorm = 0.0464, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131290\n",
      "SWAG spoch 19\n",
      "Loss = 5.5201e-05, PNorm = 84.8503, GNorm = 0.0346, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132690\n",
      "SWAG spoch 20\n",
      "Loss = 5.5624e-05, PNorm = 84.7155, GNorm = 0.0302, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.133645\n",
      "SWAG spoch 21\n",
      "Loss = 5.6057e-05, PNorm = 84.5810, GNorm = 0.0584, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.134905\n",
      "SWAG spoch 22\n",
      "Loss = 5.6494e-05, PNorm = 84.4470, GNorm = 0.0435, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135948\n",
      "SWAG spoch 23\n",
      "Loss = 5.6962e-05, PNorm = 84.3132, GNorm = 0.0350, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.136968\n",
      "SWAG spoch 24\n",
      "Loss = 5.7404e-05, PNorm = 84.1795, GNorm = 0.0363, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138221\n",
      "Model 0, sample 0 test mae = 2.215999\n",
      "Model 0, sample 1 test mae = 2.211334\n",
      "Model 0, sample 2 test mae = 2.220151\n",
      "Model 0, sample 3 test mae = 2.219173\n",
      "Model 0, sample 4 test mae = 2.205197\n",
      "Model 0, sample 5 test mae = 2.213687\n",
      "Model 0, sample 6 test mae = 2.235530\n",
      "Model 0, sample 7 test mae = 2.213811\n",
      "Model 0, sample 8 test mae = 2.216789\n",
      "Model 0, sample 9 test mae = 2.219743\n",
      "Model 0, sample 10 test mae = 2.223194\n",
      "Model 0, sample 11 test mae = 2.205754\n",
      "Model 0, sample 12 test mae = 2.214806\n",
      "Model 0, sample 13 test mae = 2.220438\n",
      "Model 0, sample 14 test mae = 2.214931\n",
      "Model 0, sample 15 test mae = 2.221605\n",
      "Model 0, sample 16 test mae = 2.212378\n",
      "Model 0, sample 17 test mae = 2.216015\n",
      "Model 0, sample 18 test mae = 2.218594\n",
      "Model 0, sample 19 test mae = 2.229232\n",
      "Model 0, sample 20 test mae = 2.217281\n",
      "Model 0, sample 21 test mae = 2.211613\n",
      "Model 0, sample 22 test mae = 2.220104\n",
      "Model 0, sample 23 test mae = 2.217689\n",
      "Model 0, sample 24 test mae = 2.210539\n",
      "Model 0, sample 25 test mae = 2.213517\n",
      "Model 0, sample 26 test mae = 2.214669\n",
      "Model 0, sample 27 test mae = 2.216382\n",
      "Model 0, sample 28 test mae = 2.210967\n",
      "Model 0, sample 29 test mae = 2.212941\n",
      "BMA test mae = 2.205125\n"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 30\n",
    "args.dropout = 0\n",
    "args.test_dropout = False\n",
    "\n",
    "args.swag = True # SWAG switch\n",
    "args.cov_mat = False # whether to compute deviations and then covariance\n",
    "args.block = False # whether to compute covariances layer by layer\n",
    "args.max_num_models = 30 # max number of columns of deviations matrix\n",
    "\n",
    "args.epochs_swag = 25 # number of epochs\n",
    "args.c_swag = 200 # how frequently to collect a model (in batches)\n",
    "\n",
    "args.lr_swag = 1e-3\n",
    "args.wd_swag = 0.001\n",
    "args.momentum_swag = 0.5\n",
    "\n",
    "results_swagD = run_training(args)\n",
    "#np.savez(args.save_dir+'/results_swagD', results_swagD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-832232a1-7bee-48f8-801a-d6a9460319ca.json\n",
      "Args\n",
      "{'RMS': False,\n",
      " 'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'batch_size_sgld': 50,\n",
      " 'bias': False,\n",
      " 'block': False,\n",
      " 'burnin_epochs': 10,\n",
      " 'c_swag': 200,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': True,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_swag': 25,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise': -2,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'log_frequency_sgld': 0,\n",
      " 'lr_sgld': 0.0001,\n",
      " 'lr_swag': 0.001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 30,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'mix_epochs': 1,\n",
      " 'momentum_swag': 0.5,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': 12,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 30,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': True,\n",
      " 'target_columns': None,\n",
      " 'task_names': ['mu',\n",
      "                'alpha',\n",
      "                'homo',\n",
      "                'lumo',\n",
      "                'gap',\n",
      "                'r2',\n",
      "                'zpve',\n",
      "                'cv',\n",
      "                'u0',\n",
      "                'u298',\n",
      "                'h298',\n",
      "                'g298'],\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': 40000,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0.001,\n",
      " 'weight_decay_sgld': 0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41446it [00:00, 69921.23it/s]\n",
      "100%|██████████| 50000/50000 [00:01<00:00, 28798.80it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 13861.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Model 0 best validation mae = inf on epoch 0\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.2671e-05, PNorm = 87.4527, GNorm = 0.0434, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141925\n",
      "SWAG spoch 1\n",
      "Loss = 5.1794e-05, PNorm = 87.3134, GNorm = 0.0335, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138139\n",
      "SWAG spoch 2\n",
      "Loss = 5.1467e-05, PNorm = 87.1742, GNorm = 0.0327, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135514\n",
      "SWAG spoch 3\n",
      "Loss = 5.1343e-05, PNorm = 87.0355, GNorm = 0.0264, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132543\n",
      "SWAG spoch 4\n",
      "Loss = 5.1268e-05, PNorm = 86.8971, GNorm = 0.0294, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131398\n",
      "SWAG spoch 5\n",
      "Loss = 5.1320e-05, PNorm = 86.7590, GNorm = 0.0304, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130107\n",
      "SWAG spoch 6\n",
      "Loss = 5.1412e-05, PNorm = 86.6211, GNorm = 0.0297, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129003\n",
      "SWAG spoch 7\n",
      "Loss = 5.1565e-05, PNorm = 86.4835, GNorm = 0.0389, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127970\n",
      "SWAG spoch 8\n",
      "Loss = 5.1738e-05, PNorm = 86.3459, GNorm = 0.0407, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127302\n",
      "SWAG spoch 9\n",
      "Loss = 5.1942e-05, PNorm = 86.2089, GNorm = 0.0418, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127138\n",
      "SWAG spoch 10\n",
      "Loss = 5.2183e-05, PNorm = 86.0718, GNorm = 0.0336, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.126791\n",
      "SWAG spoch 11\n",
      "Loss = 5.2442e-05, PNorm = 85.9350, GNorm = 0.0334, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127004\n",
      "SWAG spoch 12\n",
      "Loss = 5.2726e-05, PNorm = 85.7986, GNorm = 0.0406, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127079\n",
      "SWAG spoch 13\n",
      "Loss = 5.3030e-05, PNorm = 85.6623, GNorm = 0.0331, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127515\n",
      "SWAG spoch 14\n",
      "Loss = 5.3359e-05, PNorm = 85.5266, GNorm = 0.0346, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127970\n",
      "SWAG spoch 15\n",
      "Loss = 5.3692e-05, PNorm = 85.3908, GNorm = 0.0499, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.128943\n",
      "SWAG spoch 16\n",
      "Loss = 5.4070e-05, PNorm = 85.2553, GNorm = 0.0376, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129344\n",
      "SWAG spoch 17\n",
      "Loss = 5.4428e-05, PNorm = 85.1202, GNorm = 0.0355, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130124\n",
      "SWAG spoch 18\n",
      "Loss = 5.4811e-05, PNorm = 84.9849, GNorm = 0.0464, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131290\n",
      "SWAG spoch 19\n",
      "Loss = 5.5201e-05, PNorm = 84.8503, GNorm = 0.0346, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132690\n",
      "SWAG spoch 20\n",
      "Loss = 5.5624e-05, PNorm = 84.7155, GNorm = 0.0302, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.133645\n",
      "SWAG spoch 21\n",
      "Loss = 5.6057e-05, PNorm = 84.5810, GNorm = 0.0584, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.134905\n",
      "SWAG spoch 22\n",
      "Loss = 5.6494e-05, PNorm = 84.4470, GNorm = 0.0435, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135948\n",
      "SWAG spoch 23\n",
      "Loss = 5.6962e-05, PNorm = 84.3132, GNorm = 0.0350, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.136968\n",
      "SWAG spoch 24\n",
      "Loss = 5.7404e-05, PNorm = 84.1795, GNorm = 0.0363, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138221\n",
      "Model 0, sample 0 test mae = 2.214307\n",
      "Model 0, sample 1 test mae = 2.215639\n",
      "Model 0, sample 2 test mae = 2.220493\n",
      "Model 0, sample 3 test mae = 2.219553\n",
      "Model 0, sample 4 test mae = 2.228939\n",
      "Model 0, sample 5 test mae = 2.228063\n",
      "Model 0, sample 6 test mae = 2.282192\n",
      "Model 0, sample 7 test mae = 2.215986\n",
      "Model 0, sample 8 test mae = 2.216008\n",
      "Model 0, sample 9 test mae = 2.220143\n",
      "Model 0, sample 10 test mae = 2.226687\n",
      "Model 0, sample 11 test mae = 2.218533\n",
      "Model 0, sample 12 test mae = 2.216789\n",
      "Model 0, sample 13 test mae = 2.227806\n",
      "Model 0, sample 14 test mae = 2.238156\n",
      "Model 0, sample 15 test mae = 2.213463\n",
      "Model 0, sample 16 test mae = 2.211369\n",
      "Model 0, sample 17 test mae = 2.215626\n",
      "Model 0, sample 18 test mae = 2.216234\n",
      "Model 0, sample 19 test mae = 2.208882\n",
      "Model 0, sample 20 test mae = 2.224230\n",
      "Model 0, sample 21 test mae = 2.219651\n",
      "Model 0, sample 22 test mae = 2.219185\n",
      "Model 0, sample 23 test mae = 2.214392\n",
      "Model 0, sample 24 test mae = 2.211128\n",
      "Model 0, sample 25 test mae = 2.210485\n",
      "Model 0, sample 26 test mae = 2.244593\n",
      "Model 0, sample 27 test mae = 2.269718\n",
      "Model 0, sample 28 test mae = 2.213902\n",
      "Model 0, sample 29 test mae = 2.218234\n",
      "BMA test mae = 2.204640\n"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 30\n",
    "args.dropout = 0\n",
    "args.test_dropout = False\n",
    "\n",
    "args.swag = True # SWAG switch\n",
    "args.cov_mat = True # whether to compute deviations and then covariance\n",
    "args.block = False # whether to compute covariances layer by layer\n",
    "args.max_num_models = 30 # max number of columns of deviations matrix\n",
    "\n",
    "args.epochs_swag = 25 # number of epochs\n",
    "args.c_swag = 200 # how frequently to collect a model (in batches)\n",
    "\n",
    "args.lr_swag = 1e-3\n",
    "args.wd_swag = 0.001\n",
    "args.momentum_swag = 0.5\n",
    "\n",
    "results_swag = run_training(args)\n",
    "#np.savez(args.save_dir+'/results_swag', results_swag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiSWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-832232a1-7bee-48f8-801a-d6a9460319ca.json\n",
      "Args\n",
      "{'RMS': False,\n",
      " 'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'batch_size_sgld': 50,\n",
      " 'bias': False,\n",
      " 'block': False,\n",
      " 'burnin_epochs': 10,\n",
      " 'c_swag': 200,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': True,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0,\n",
      " 'ensemble_size': 10,\n",
      " 'epochs': 50,\n",
      " 'epochs_swag': 25,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise': -2,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'log_frequency_sgld': 0,\n",
      " 'lr_sgld': 0.0001,\n",
      " 'lr_swag': 0.001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 30,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'mix_epochs': 1,\n",
      " 'momentum_swag': 0.5,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': None,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 30,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': True,\n",
      " 'target_columns': None,\n",
      " 'task_names': None,\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': None,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0.001,\n",
      " 'weight_decay_sgld': 0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43280it [00:00, 68612.48it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 264493.03it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 13561.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.6179e-03, PNorm = 49.3502, GNorm = 3.9430, lr_0 = 5.5056e-04\n",
      "Validation mae = 12.860934\n",
      "Epoch 1\n",
      "Loss = 3.3376e-03, PNorm = 51.5382, GNorm = 1.6459, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.169757\n",
      "Epoch 2\n",
      "Loss = 2.2128e-03, PNorm = 54.0087, GNorm = 0.6652, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.177531\n",
      "Epoch 3\n",
      "Loss = 1.6222e-03, PNorm = 55.9070, GNorm = 0.8129, lr_0 = 9.0846e-04\n",
      "Validation mae = 7.748067\n",
      "Epoch 4\n",
      "Loss = 1.4080e-03, PNorm = 57.8874, GNorm = 0.7858, lr_0 = 8.6591e-04\n",
      "Validation mae = 7.995050\n",
      "Epoch 5\n",
      "Loss = 1.1951e-03, PNorm = 59.7011, GNorm = 0.9493, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.806514\n",
      "Epoch 6\n",
      "Loss = 1.0894e-03, PNorm = 61.5091, GNorm = 0.3030, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.281439\n",
      "Epoch 7\n",
      "Loss = 1.0005e-03, PNorm = 63.3134, GNorm = 0.5665, lr_0 = 7.4985e-04\n",
      "Validation mae = 5.099877\n",
      "Epoch 8\n",
      "Loss = 8.9264e-04, PNorm = 64.9155, GNorm = 0.3422, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.292120\n",
      "Epoch 9\n",
      "Loss = 7.8944e-04, PNorm = 66.3597, GNorm = 0.2399, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.346213\n",
      "Epoch 10\n",
      "Loss = 7.6237e-04, PNorm = 67.9968, GNorm = 0.5543, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.487941\n",
      "Epoch 11\n",
      "Loss = 6.6571e-04, PNorm = 69.3753, GNorm = 0.2699, lr_0 = 6.1893e-04\n",
      "Validation mae = 4.052892\n",
      "Epoch 12\n",
      "Loss = 5.9497e-04, PNorm = 70.6016, GNorm = 0.3558, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.280318\n",
      "Epoch 13\n",
      "Loss = 5.4275e-04, PNorm = 71.8562, GNorm = 0.1506, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.238623\n",
      "Epoch 14\n",
      "Loss = 5.5829e-04, PNorm = 73.2488, GNorm = 0.1540, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.346532\n",
      "Epoch 15\n",
      "Loss = 5.2317e-04, PNorm = 74.4343, GNorm = 0.4223, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.235020\n",
      "Epoch 16\n",
      "Loss = 4.5317e-04, PNorm = 75.4005, GNorm = 0.1824, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.485054\n",
      "Epoch 17\n",
      "Loss = 3.9069e-04, PNorm = 76.2208, GNorm = 0.3152, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.391913\n",
      "Epoch 18\n",
      "Loss = 4.0170e-04, PNorm = 77.2611, GNorm = 0.3298, lr_0 = 4.4239e-04\n",
      "Validation mae = 3.120937\n",
      "Epoch 19\n",
      "Loss = 3.5528e-04, PNorm = 78.0467, GNorm = 0.1210, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.811184\n",
      "Epoch 20\n",
      "Loss = 3.1973e-04, PNorm = 78.7542, GNorm = 0.2209, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.733041\n",
      "Epoch 21\n",
      "Loss = 3.0105e-04, PNorm = 79.4718, GNorm = 0.1581, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.750101\n",
      "Epoch 22\n",
      "Loss = 2.7726e-04, PNorm = 80.1164, GNorm = 0.1703, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.736319\n",
      "Epoch 23\n",
      "Loss = 2.5734e-04, PNorm = 80.7267, GNorm = 0.1092, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.627317\n",
      "Epoch 24\n",
      "Loss = 2.4745e-04, PNorm = 81.3700, GNorm = 0.1553, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.801777\n",
      "Epoch 25\n",
      "Loss = 2.4464e-04, PNorm = 81.9136, GNorm = 0.1295, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.550626\n",
      "Epoch 26\n",
      "Loss = 2.0595e-04, PNorm = 82.3519, GNorm = 0.1275, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.526890\n",
      "Epoch 27\n",
      "Loss = 1.8733e-04, PNorm = 82.7512, GNorm = 0.1006, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.436306\n",
      "Epoch 28\n",
      "Loss = 1.7636e-04, PNorm = 83.1732, GNorm = 0.0876, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.464180\n",
      "Epoch 29\n",
      "Loss = 1.7206e-04, PNorm = 83.6087, GNorm = 0.1431, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.511821\n",
      "Epoch 30\n",
      "Loss = 1.5775e-04, PNorm = 83.9577, GNorm = 0.1254, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.391637\n",
      "Epoch 31\n",
      "Loss = 1.6260e-04, PNorm = 84.3189, GNorm = 0.1218, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.390429\n",
      "Epoch 32\n",
      "Loss = 1.3568e-04, PNorm = 84.6040, GNorm = 0.0955, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.354117\n",
      "Epoch 33\n",
      "Loss = 1.2337e-04, PNorm = 84.8708, GNorm = 0.0874, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.355782\n",
      "Epoch 34\n",
      "Loss = 1.1948e-04, PNorm = 85.1333, GNorm = 0.0964, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.357777\n",
      "Epoch 35\n",
      "Loss = 1.0906e-04, PNorm = 85.3728, GNorm = 0.0875, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.277918\n",
      "Epoch 36\n",
      "Loss = 1.0578e-04, PNorm = 85.6323, GNorm = 0.0614, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.396424\n",
      "Epoch 37\n",
      "Loss = 1.0383e-04, PNorm = 85.8613, GNorm = 0.0748, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.255885\n",
      "Epoch 38\n",
      "Loss = 9.2029e-05, PNorm = 86.0428, GNorm = 0.0672, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.219827\n",
      "Epoch 39\n",
      "Loss = 8.9530e-05, PNorm = 86.2335, GNorm = 0.0504, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.224348\n",
      "Epoch 40\n",
      "Loss = 8.6073e-05, PNorm = 86.4148, GNorm = 0.0689, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.216106\n",
      "Epoch 41\n",
      "Loss = 8.3960e-05, PNorm = 86.5933, GNorm = 0.0671, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.222690\n",
      "Epoch 42\n",
      "Loss = 7.7010e-05, PNorm = 86.7432, GNorm = 0.0491, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.205915\n",
      "Epoch 43\n",
      "Loss = 7.4229e-05, PNorm = 86.8854, GNorm = 0.0636, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.176927\n",
      "Epoch 44\n",
      "Loss = 7.0844e-05, PNorm = 87.0141, GNorm = 0.0635, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.319141\n",
      "Epoch 45\n",
      "Loss = 6.8878e-05, PNorm = 87.1449, GNorm = 0.0439, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.164977\n",
      "Epoch 46\n",
      "Loss = 6.5918e-05, PNorm = 87.2662, GNorm = 0.0709, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.168121\n",
      "Epoch 47\n",
      "Loss = 6.4436e-05, PNorm = 87.3904, GNorm = 0.0771, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.202392\n",
      "Epoch 48\n",
      "Loss = 6.1387e-05, PNorm = 87.4890, GNorm = 0.2994, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.198103\n",
      "Epoch 49\n",
      "Loss = 5.9817e-05, PNorm = 87.5923, GNorm = 0.0597, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.152893\n",
      "Model 0 best validation mae = 2.152893 on epoch 49\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.2687e-05, PNorm = 87.4527, GNorm = 0.0489, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141429\n",
      "SWAG spoch 1\n",
      "Loss = 5.1798e-05, PNorm = 87.3133, GNorm = 0.0364, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.137889\n",
      "SWAG spoch 2\n",
      "Loss = 5.1468e-05, PNorm = 87.1742, GNorm = 0.0307, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135218\n",
      "SWAG spoch 3\n",
      "Loss = 5.1323e-05, PNorm = 87.0355, GNorm = 0.0387, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.133070\n",
      "SWAG spoch 4\n",
      "Loss = 5.1282e-05, PNorm = 86.8971, GNorm = 0.0363, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131190\n",
      "SWAG spoch 5\n",
      "Loss = 5.1319e-05, PNorm = 86.7590, GNorm = 0.0349, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130036\n",
      "SWAG spoch 6\n",
      "Loss = 5.1424e-05, PNorm = 86.6211, GNorm = 0.0278, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.128812\n",
      "SWAG spoch 7\n",
      "Loss = 5.1546e-05, PNorm = 86.4835, GNorm = 0.0273, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.128259\n",
      "SWAG spoch 8\n",
      "Loss = 5.1741e-05, PNorm = 86.3459, GNorm = 0.0389, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127519\n",
      "SWAG spoch 9\n",
      "Loss = 5.1955e-05, PNorm = 86.2089, GNorm = 0.0506, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.126917\n",
      "SWAG spoch 10\n",
      "Loss = 5.2190e-05, PNorm = 86.0719, GNorm = 0.0291, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.126569\n",
      "SWAG spoch 11\n",
      "Loss = 5.2423e-05, PNorm = 85.9350, GNorm = 0.0349, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127069\n",
      "SWAG spoch 12\n",
      "Loss = 5.2724e-05, PNorm = 85.7986, GNorm = 0.0299, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127393\n",
      "SWAG spoch 13\n",
      "Loss = 5.3036e-05, PNorm = 85.6623, GNorm = 0.0340, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127456\n",
      "SWAG spoch 14\n",
      "Loss = 5.3358e-05, PNorm = 85.5266, GNorm = 0.0302, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.128056\n",
      "SWAG spoch 15\n",
      "Loss = 5.3694e-05, PNorm = 85.3908, GNorm = 0.0465, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.128954\n",
      "SWAG spoch 16\n",
      "Loss = 5.4045e-05, PNorm = 85.2553, GNorm = 0.0466, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129947\n",
      "SWAG spoch 17\n",
      "Loss = 5.4445e-05, PNorm = 85.1202, GNorm = 0.0350, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130346\n",
      "SWAG spoch 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 5.4815e-05, PNorm = 84.9849, GNorm = 0.0466, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131425\n",
      "SWAG spoch 19\n",
      "Loss = 5.5210e-05, PNorm = 84.8503, GNorm = 0.0506, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132689\n",
      "SWAG spoch 20\n",
      "Loss = 5.5627e-05, PNorm = 84.7155, GNorm = 0.0453, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.133446\n",
      "SWAG spoch 21\n",
      "Loss = 5.6054e-05, PNorm = 84.5810, GNorm = 0.0645, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.134710\n",
      "SWAG spoch 22\n",
      "Loss = 5.6513e-05, PNorm = 84.4471, GNorm = 0.1075, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135571\n",
      "SWAG spoch 23\n",
      "Loss = 5.6928e-05, PNorm = 84.3132, GNorm = 0.0404, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.137227\n",
      "SWAG spoch 24\n",
      "Loss = 5.7423e-05, PNorm = 84.1795, GNorm = 0.0336, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138277\n",
      "Model 0, sample 0 test mae = 2.232634\n",
      "Model 0, sample 1 test mae = 2.210806\n",
      "Model 0, sample 2 test mae = 2.221074\n",
      "Model 0, sample 3 test mae = 2.217181\n",
      "Model 0, sample 4 test mae = 2.220303\n",
      "Model 0, sample 5 test mae = 2.214229\n",
      "Model 0, sample 6 test mae = 2.251860\n",
      "Model 0, sample 7 test mae = 2.275994\n",
      "Model 0, sample 8 test mae = 2.223335\n",
      "Model 0, sample 9 test mae = 2.238912\n",
      "Model 0, sample 10 test mae = 2.210204\n",
      "Model 0, sample 11 test mae = 2.212348\n",
      "Model 0, sample 12 test mae = 2.210935\n",
      "Model 0, sample 13 test mae = 2.229834\n",
      "Model 0, sample 14 test mae = 2.239501\n",
      "Model 0, sample 15 test mae = 2.213546\n",
      "Model 0, sample 16 test mae = 2.222098\n",
      "Model 0, sample 17 test mae = 2.219629\n",
      "Model 0, sample 18 test mae = 2.211314\n",
      "Model 0, sample 19 test mae = 2.224306\n",
      "Model 0, sample 20 test mae = 2.219666\n",
      "Model 0, sample 21 test mae = 2.215022\n",
      "Model 0, sample 22 test mae = 2.212329\n",
      "Model 0, sample 23 test mae = 2.207430\n",
      "Model 0, sample 24 test mae = 2.214263\n",
      "Model 0, sample 25 test mae = 2.227315\n",
      "Model 0, sample 26 test mae = 2.222320\n",
      "Model 0, sample 27 test mae = 2.216049\n",
      "Model 0, sample 28 test mae = 2.224656\n",
      "Model 0, sample 29 test mae = 2.229002\n",
      "Building model 1\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7383e-03, PNorm = 49.3599, GNorm = 1.8495, lr_0 = 5.5056e-04\n",
      "Validation mae = 11.941261\n",
      "Epoch 1\n",
      "Loss = 3.3173e-03, PNorm = 51.7161, GNorm = 1.1433, lr_0 = 9.9994e-04\n",
      "Validation mae = 8.397221\n",
      "Epoch 2\n",
      "Loss = 2.0454e-03, PNorm = 54.0497, GNorm = 0.6737, lr_0 = 9.5310e-04\n",
      "Validation mae = 6.157245\n",
      "Epoch 3\n",
      "Loss = 1.5340e-03, PNorm = 55.9190, GNorm = 0.6344, lr_0 = 9.0846e-04\n",
      "Validation mae = 6.892603\n",
      "Epoch 4\n",
      "Loss = 1.3103e-03, PNorm = 57.8679, GNorm = 0.8085, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.294102\n",
      "Epoch 5\n",
      "Loss = 1.1518e-03, PNorm = 59.6204, GNorm = 1.0577, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.158729\n",
      "Epoch 6\n",
      "Loss = 1.0361e-03, PNorm = 61.4264, GNorm = 1.2250, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.842790\n",
      "Epoch 7\n",
      "Loss = 9.3172e-04, PNorm = 63.0781, GNorm = 0.2155, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.555255\n",
      "Epoch 8\n",
      "Loss = 8.9684e-04, PNorm = 64.9011, GNorm = 0.4415, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.469888\n",
      "Epoch 9\n",
      "Loss = 7.7040e-04, PNorm = 66.4529, GNorm = 0.3311, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.106529\n",
      "Epoch 10\n",
      "Loss = 7.2825e-04, PNorm = 68.0935, GNorm = 0.1889, lr_0 = 6.4934e-04\n",
      "Validation mae = 3.675211\n",
      "Epoch 11\n",
      "Loss = 6.2387e-04, PNorm = 69.3546, GNorm = 0.1894, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.911246\n",
      "Epoch 12\n",
      "Loss = 6.2644e-04, PNorm = 70.9037, GNorm = 0.2149, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.277805\n",
      "Epoch 13\n",
      "Loss = 5.4063e-04, PNorm = 72.0980, GNorm = 0.4524, lr_0 = 5.6231e-04\n",
      "Validation mae = 4.371532\n",
      "Epoch 14\n",
      "Loss = 5.4539e-04, PNorm = 73.4259, GNorm = 0.2048, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.124394\n",
      "Epoch 15\n",
      "Loss = 5.0592e-04, PNorm = 74.6584, GNorm = 0.1985, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.286191\n",
      "Epoch 16\n",
      "Loss = 4.6773e-04, PNorm = 75.6816, GNorm = 0.2577, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.482424\n",
      "Epoch 17\n",
      "Loss = 4.0961e-04, PNorm = 76.6509, GNorm = 0.1342, lr_0 = 4.6413e-04\n",
      "Validation mae = 2.996103\n",
      "Epoch 18\n",
      "Loss = 3.5534e-04, PNorm = 77.4714, GNorm = 0.1480, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.891245\n",
      "Epoch 19\n",
      "Loss = 3.4598e-04, PNorm = 78.3325, GNorm = 0.7190, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.505857\n",
      "Epoch 20\n",
      "Loss = 3.1483e-04, PNorm = 79.1130, GNorm = 0.1497, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.612895\n",
      "Epoch 21\n",
      "Loss = 2.8473e-04, PNorm = 79.8144, GNorm = 0.3759, lr_0 = 3.8310e-04\n",
      "Validation mae = 3.207497\n",
      "Epoch 22\n",
      "Loss = 2.8107e-04, PNorm = 80.4976, GNorm = 0.1239, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.593103\n",
      "Epoch 23\n",
      "Loss = 2.3384e-04, PNorm = 81.0567, GNorm = 0.1167, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.634497\n",
      "Epoch 24\n",
      "Loss = 2.2755e-04, PNorm = 81.6312, GNorm = 0.0953, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.569232\n",
      "Epoch 25\n",
      "Loss = 2.1656e-04, PNorm = 82.1804, GNorm = 0.2492, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.558703\n",
      "Epoch 26\n",
      "Loss = 2.0638e-04, PNorm = 82.7165, GNorm = 0.1291, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.362899\n",
      "Epoch 27\n",
      "Loss = 1.7456e-04, PNorm = 83.1010, GNorm = 0.0870, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.356928\n",
      "Epoch 28\n",
      "Loss = 1.6057e-04, PNorm = 83.5012, GNorm = 0.0837, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.313425\n",
      "Epoch 29\n",
      "Loss = 1.5021e-04, PNorm = 83.8741, GNorm = 0.0919, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.373106\n",
      "Epoch 30\n",
      "Loss = 1.4789e-04, PNorm = 84.2783, GNorm = 0.1048, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.453469\n",
      "Epoch 31\n",
      "Loss = 1.4120e-04, PNorm = 84.6323, GNorm = 0.1129, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.318835\n",
      "Epoch 32\n",
      "Loss = 1.3201e-04, PNorm = 84.9443, GNorm = 0.0747, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.375766\n",
      "Epoch 33\n",
      "Loss = 1.1878e-04, PNorm = 85.2227, GNorm = 0.0591, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.245702\n",
      "Epoch 34\n",
      "Loss = 1.1012e-04, PNorm = 85.4886, GNorm = 0.0952, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.312397\n",
      "Epoch 35\n",
      "Loss = 1.0824e-04, PNorm = 85.7587, GNorm = 0.2505, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.257109\n",
      "Epoch 36\n",
      "Loss = 1.0215e-04, PNorm = 85.9825, GNorm = 0.1007, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.261422\n",
      "Epoch 37\n",
      "Loss = 9.4687e-05, PNorm = 86.1945, GNorm = 0.1182, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.189639\n",
      "Epoch 38\n",
      "Loss = 8.9291e-05, PNorm = 86.3923, GNorm = 0.2153, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.283005\n",
      "Epoch 39\n",
      "Loss = 8.4692e-05, PNorm = 86.5766, GNorm = 0.0869, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.245798\n",
      "Epoch 40\n",
      "Loss = 8.1003e-05, PNorm = 86.7585, GNorm = 0.0735, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.209893\n",
      "Epoch 41\n",
      "Loss = 7.7963e-05, PNorm = 86.9172, GNorm = 0.0447, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.167942\n",
      "Epoch 42\n",
      "Loss = 7.4114e-05, PNorm = 87.0704, GNorm = 0.0593, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.187468\n",
      "Epoch 43\n",
      "Loss = 7.2535e-05, PNorm = 87.2152, GNorm = 0.0574, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.176537\n",
      "Epoch 44\n",
      "Loss = 6.9567e-05, PNorm = 87.3480, GNorm = 0.0444, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.163302\n",
      "Epoch 45\n",
      "Loss = 6.6755e-05, PNorm = 87.4820, GNorm = 0.0483, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.153296\n",
      "Epoch 46\n",
      "Loss = 6.4088e-05, PNorm = 87.5976, GNorm = 0.0491, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.150960\n",
      "Epoch 47\n",
      "Loss = 6.1831e-05, PNorm = 87.7099, GNorm = 0.0501, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.131991\n",
      "Epoch 48\n",
      "Loss = 6.0213e-05, PNorm = 87.8175, GNorm = 0.0465, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.124699\n",
      "Epoch 49\n",
      "Loss = 5.8258e-05, PNorm = 87.9161, GNorm = 0.0424, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.123940\n",
      "Model 1 best validation mae = 2.123940 on epoch 49\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.0740e-05, PNorm = 87.7760, GNorm = 0.0375, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.099687\n",
      "SWAG spoch 1\n",
      "Loss = 5.0067e-05, PNorm = 87.6363, GNorm = 0.0465, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.091054\n",
      "SWAG spoch 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 4.9853e-05, PNorm = 87.4968, GNorm = 0.0342, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.086651\n",
      "SWAG spoch 3\n",
      "Loss = 4.9776e-05, PNorm = 87.3578, GNorm = 0.0386, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.084148\n",
      "SWAG spoch 4\n",
      "Loss = 4.9809e-05, PNorm = 87.2189, GNorm = 0.0363, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.081669\n",
      "SWAG spoch 5\n",
      "Loss = 4.9905e-05, PNorm = 87.0801, GNorm = 0.0370, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.078882\n",
      "SWAG spoch 6\n",
      "Loss = 5.0021e-05, PNorm = 86.9415, GNorm = 0.0307, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.077686\n",
      "SWAG spoch 7\n",
      "Loss = 5.0178e-05, PNorm = 86.8032, GNorm = 0.0373, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.076961\n",
      "SWAG spoch 8\n",
      "Loss = 5.0382e-05, PNorm = 86.6655, GNorm = 0.0333, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.076798\n",
      "SWAG spoch 9\n",
      "Loss = 5.0609e-05, PNorm = 86.5281, GNorm = 0.0303, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.076380\n",
      "SWAG spoch 10\n",
      "Loss = 5.0860e-05, PNorm = 86.3904, GNorm = 0.0377, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.076620\n",
      "SWAG spoch 11\n",
      "Loss = 5.1149e-05, PNorm = 86.2530, GNorm = 0.0462, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.076154\n",
      "SWAG spoch 12\n",
      "Loss = 5.1443e-05, PNorm = 86.1160, GNorm = 0.0682, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.076422\n",
      "SWAG spoch 13\n",
      "Loss = 5.1766e-05, PNorm = 85.9794, GNorm = 0.0348, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.076588\n",
      "SWAG spoch 14\n",
      "Loss = 5.2084e-05, PNorm = 85.8429, GNorm = 0.0460, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.077203\n",
      "SWAG spoch 15\n",
      "Loss = 5.2457e-05, PNorm = 85.7065, GNorm = 0.0328, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.077669\n",
      "SWAG spoch 16\n",
      "Loss = 5.2812e-05, PNorm = 85.5707, GNorm = 0.0636, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.078091\n",
      "SWAG spoch 17\n",
      "Loss = 5.3198e-05, PNorm = 85.4350, GNorm = 0.0316, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.078405\n",
      "SWAG spoch 18\n",
      "Loss = 5.3574e-05, PNorm = 85.2993, GNorm = 0.0304, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.079432\n",
      "SWAG spoch 19\n",
      "Loss = 5.4000e-05, PNorm = 85.1642, GNorm = 0.0414, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.080361\n",
      "SWAG spoch 20\n",
      "Loss = 5.4396e-05, PNorm = 85.0291, GNorm = 0.0362, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.081489\n",
      "SWAG spoch 21\n",
      "Loss = 5.4866e-05, PNorm = 84.8944, GNorm = 0.0332, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.082186\n",
      "SWAG spoch 22\n",
      "Loss = 5.5291e-05, PNorm = 84.7597, GNorm = 0.0369, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.083331\n",
      "SWAG spoch 23\n",
      "Loss = 5.5753e-05, PNorm = 84.6253, GNorm = 0.0305, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.084383\n",
      "SWAG spoch 24\n",
      "Loss = 5.6219e-05, PNorm = 84.4911, GNorm = 0.0502, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.085493\n",
      "Model 1, sample 0 test mae = 2.208419\n",
      "Model 1, sample 1 test mae = 2.274234\n",
      "Model 1, sample 2 test mae = 2.202477\n",
      "Model 1, sample 3 test mae = 2.201701\n",
      "Model 1, sample 4 test mae = 2.206478\n",
      "Model 1, sample 5 test mae = 2.197796\n",
      "Model 1, sample 6 test mae = 2.195755\n",
      "Model 1, sample 7 test mae = 2.219529\n",
      "Model 1, sample 8 test mae = 2.201964\n",
      "Model 1, sample 9 test mae = 2.219667\n",
      "Model 1, sample 10 test mae = 2.200099\n",
      "Model 1, sample 11 test mae = 2.206600\n",
      "Model 1, sample 12 test mae = 2.193184\n",
      "Model 1, sample 13 test mae = 2.214017\n",
      "Model 1, sample 14 test mae = 2.200913\n",
      "Model 1, sample 15 test mae = 2.194805\n",
      "Model 1, sample 16 test mae = 2.225234\n",
      "Model 1, sample 17 test mae = 2.223355\n",
      "Model 1, sample 18 test mae = 2.196986\n",
      "Model 1, sample 19 test mae = 2.204002\n",
      "Model 1, sample 20 test mae = 2.208339\n",
      "Model 1, sample 21 test mae = 2.215235\n",
      "Model 1, sample 22 test mae = 2.213080\n",
      "Model 1, sample 23 test mae = 2.199829\n",
      "Model 1, sample 24 test mae = 2.209605\n",
      "Model 1, sample 25 test mae = 2.201433\n",
      "Model 1, sample 26 test mae = 2.226817\n",
      "Model 1, sample 27 test mae = 2.203640\n",
      "Model 1, sample 28 test mae = 2.223028\n",
      "Model 1, sample 29 test mae = 2.201193\n",
      "Building model 2\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.6985e-03, PNorm = 49.2306, GNorm = 1.6060, lr_0 = 5.5056e-04\n",
      "Validation mae = 14.044228\n",
      "Epoch 1\n",
      "Loss = 3.5219e-03, PNorm = 51.5561, GNorm = 1.3565, lr_0 = 9.9994e-04\n",
      "Validation mae = 11.160860\n",
      "Epoch 2\n",
      "Loss = 2.2201e-03, PNorm = 53.8495, GNorm = 0.7821, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.098013\n",
      "Epoch 3\n",
      "Loss = 1.6099e-03, PNorm = 55.6785, GNorm = 0.6277, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.572813\n",
      "Epoch 4\n",
      "Loss = 1.3487e-03, PNorm = 57.5028, GNorm = 0.4348, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.648544\n",
      "Epoch 5\n",
      "Loss = 1.1699e-03, PNorm = 59.2163, GNorm = 0.3451, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.504050\n",
      "Epoch 6\n",
      "Loss = 1.0594e-03, PNorm = 60.9506, GNorm = 0.4991, lr_0 = 7.8670e-04\n",
      "Validation mae = 5.595658\n",
      "Epoch 7\n",
      "Loss = 9.3899e-04, PNorm = 62.5170, GNorm = 0.3302, lr_0 = 7.4985e-04\n",
      "Validation mae = 5.245759\n",
      "Epoch 8\n",
      "Loss = 8.6232e-04, PNorm = 64.1511, GNorm = 0.6828, lr_0 = 7.1473e-04\n",
      "Validation mae = 5.286988\n",
      "Epoch 9\n",
      "Loss = 8.1155e-04, PNorm = 65.6994, GNorm = 0.4261, lr_0 = 6.8125e-04\n",
      "Validation mae = 3.854090\n",
      "Epoch 10\n",
      "Loss = 7.0038e-04, PNorm = 67.0146, GNorm = 0.2303, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.235266\n",
      "Epoch 11\n",
      "Loss = 6.7093e-04, PNorm = 68.5161, GNorm = 0.2471, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.790548\n",
      "Epoch 12\n",
      "Loss = 6.0214e-04, PNorm = 69.7944, GNorm = 0.3840, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.762946\n",
      "Epoch 13\n",
      "Loss = 5.4974e-04, PNorm = 71.0670, GNorm = 0.4460, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.363080\n",
      "Epoch 14\n",
      "Loss = 5.1817e-04, PNorm = 72.2931, GNorm = 0.2566, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.453197\n",
      "Epoch 15\n",
      "Loss = 4.8307e-04, PNorm = 73.4261, GNorm = 0.8523, lr_0 = 5.1087e-04\n",
      "Validation mae = 5.758439\n",
      "Epoch 16\n",
      "Loss = 4.3489e-04, PNorm = 74.4736, GNorm = 0.1958, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.114452\n",
      "Epoch 17\n",
      "Loss = 4.0660e-04, PNorm = 75.4343, GNorm = 0.1805, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.243445\n",
      "Epoch 18\n",
      "Loss = 3.7897e-04, PNorm = 76.3766, GNorm = 0.2908, lr_0 = 4.4239e-04\n",
      "Validation mae = 3.560135\n",
      "Epoch 19\n",
      "Loss = 3.7129e-04, PNorm = 77.3230, GNorm = 0.1054, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.801960\n",
      "Epoch 20\n",
      "Loss = 3.3460e-04, PNorm = 78.1116, GNorm = 0.1503, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.659035\n",
      "Epoch 21\n",
      "Loss = 2.8410e-04, PNorm = 78.6993, GNorm = 0.1701, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.673525\n",
      "Epoch 22\n",
      "Loss = 2.7398e-04, PNorm = 79.3703, GNorm = 0.1091, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.596099\n",
      "Epoch 23\n",
      "Loss = 2.5865e-04, PNorm = 80.0178, GNorm = 0.2380, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.765483\n",
      "Epoch 24\n",
      "Loss = 2.4816e-04, PNorm = 80.6333, GNorm = 0.1327, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.632232\n",
      "Epoch 25\n",
      "Loss = 2.0960e-04, PNorm = 81.1110, GNorm = 0.1240, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.622108\n",
      "Epoch 26\n",
      "Loss = 2.0747e-04, PNorm = 81.6323, GNorm = 0.1352, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.500442\n",
      "Epoch 27\n",
      "Loss = 1.8726e-04, PNorm = 82.1080, GNorm = 0.1269, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.670207\n",
      "Epoch 28\n",
      "Loss = 1.8331e-04, PNorm = 82.5701, GNorm = 0.1037, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.437385\n",
      "Epoch 29\n",
      "Loss = 1.6331e-04, PNorm = 82.9497, GNorm = 0.1151, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.492820\n",
      "Epoch 30\n",
      "Loss = 1.5236e-04, PNorm = 83.2990, GNorm = 0.0801, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.439628\n",
      "Epoch 31\n",
      "Loss = 1.3834e-04, PNorm = 83.6332, GNorm = 0.0831, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.487606\n",
      "Epoch 32\n",
      "Loss = 1.3342e-04, PNorm = 83.9542, GNorm = 0.1044, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.387539\n",
      "Epoch 33\n",
      "Loss = 1.2083e-04, PNorm = 84.2322, GNorm = 0.0761, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.333237\n",
      "Epoch 34\n",
      "Loss = 1.1354e-04, PNorm = 84.4973, GNorm = 0.0868, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.314299\n",
      "Epoch 35\n",
      "Loss = 1.0651e-04, PNorm = 84.7504, GNorm = 0.0552, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.321629\n",
      "Epoch 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.0237e-04, PNorm = 84.9852, GNorm = 0.0966, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.265635\n",
      "Epoch 37\n",
      "Loss = 9.6911e-05, PNorm = 85.2040, GNorm = 0.0682, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.385478\n",
      "Epoch 38\n",
      "Loss = 9.1579e-05, PNorm = 85.4082, GNorm = 0.0668, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.344072\n",
      "Epoch 39\n",
      "Loss = 8.8306e-05, PNorm = 85.6136, GNorm = 0.0544, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.269639\n",
      "Epoch 40\n",
      "Loss = 8.3100e-05, PNorm = 85.7801, GNorm = 0.0675, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.273337\n",
      "Epoch 41\n",
      "Loss = 7.8568e-05, PNorm = 85.9605, GNorm = 0.0957, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.283553\n",
      "Epoch 42\n",
      "Loss = 7.6853e-05, PNorm = 86.1140, GNorm = 0.0603, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.237637\n",
      "Epoch 43\n",
      "Loss = 7.3190e-05, PNorm = 86.2646, GNorm = 0.0750, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.325169\n",
      "Epoch 44\n",
      "Loss = 7.1064e-05, PNorm = 86.4036, GNorm = 0.0509, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.213826\n",
      "Epoch 45\n",
      "Loss = 6.7626e-05, PNorm = 86.5314, GNorm = 0.0498, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.243526\n",
      "Epoch 46\n",
      "Loss = 6.5354e-05, PNorm = 86.6550, GNorm = 0.0559, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.197936\n",
      "Epoch 47\n",
      "Loss = 6.4728e-05, PNorm = 86.7661, GNorm = 0.0395, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.214225\n",
      "Epoch 48\n",
      "Loss = 6.1642e-05, PNorm = 86.8705, GNorm = 0.0485, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.194549\n",
      "Epoch 49\n",
      "Loss = 5.9167e-05, PNorm = 86.9736, GNorm = 0.0507, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.197536\n",
      "Model 2 best validation mae = 2.194549 on epoch 48\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.2823e-05, PNorm = 86.7322, GNorm = 0.0287, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.173235\n",
      "SWAG spoch 1\n",
      "Loss = 5.2064e-05, PNorm = 86.5941, GNorm = 0.0642, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.167866\n",
      "SWAG spoch 2\n",
      "Loss = 5.1772e-05, PNorm = 86.4564, GNorm = 0.0386, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.164400\n",
      "SWAG spoch 3\n",
      "Loss = 5.1646e-05, PNorm = 86.3187, GNorm = 0.0401, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.161705\n",
      "SWAG spoch 4\n",
      "Loss = 5.1615e-05, PNorm = 86.1814, GNorm = 0.0340, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.160473\n",
      "SWAG spoch 5\n",
      "Loss = 5.1699e-05, PNorm = 86.0444, GNorm = 0.0342, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158249\n",
      "SWAG spoch 6\n",
      "Loss = 5.1790e-05, PNorm = 85.9079, GNorm = 0.0667, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.157715\n",
      "SWAG spoch 7\n",
      "Loss = 5.1972e-05, PNorm = 85.7713, GNorm = 0.0332, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.156410\n",
      "SWAG spoch 8\n",
      "Loss = 5.2154e-05, PNorm = 85.6348, GNorm = 0.0353, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.156115\n",
      "SWAG spoch 9\n",
      "Loss = 5.2409e-05, PNorm = 85.4987, GNorm = 0.0467, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.155427\n",
      "SWAG spoch 10\n",
      "Loss = 5.2651e-05, PNorm = 85.3630, GNorm = 0.0321, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.155137\n",
      "SWAG spoch 11\n",
      "Loss = 5.2941e-05, PNorm = 85.2274, GNorm = 0.0511, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.155009\n",
      "SWAG spoch 12\n",
      "Loss = 5.3249e-05, PNorm = 85.0918, GNorm = 0.0302, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.155561\n",
      "SWAG spoch 13\n",
      "Loss = 5.3587e-05, PNorm = 84.9568, GNorm = 0.0379, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.156038\n",
      "SWAG spoch 14\n",
      "Loss = 5.3937e-05, PNorm = 84.8222, GNorm = 0.1874, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.155569\n",
      "SWAG spoch 15\n",
      "Loss = 5.4307e-05, PNorm = 84.6872, GNorm = 0.0270, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.155762\n",
      "SWAG spoch 16\n",
      "Loss = 5.4674e-05, PNorm = 84.5530, GNorm = 0.0332, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.156639\n",
      "SWAG spoch 17\n",
      "Loss = 5.5091e-05, PNorm = 84.4188, GNorm = 0.0370, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.156537\n",
      "SWAG spoch 18\n",
      "Loss = 5.5517e-05, PNorm = 84.2851, GNorm = 0.0369, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.157082\n",
      "SWAG spoch 19\n",
      "Loss = 5.5931e-05, PNorm = 84.1513, GNorm = 0.0283, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.157697\n",
      "SWAG spoch 20\n",
      "Loss = 5.6366e-05, PNorm = 84.0180, GNorm = 0.0339, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158134\n",
      "SWAG spoch 21\n",
      "Loss = 5.6825e-05, PNorm = 83.8847, GNorm = 0.0313, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.159104\n",
      "SWAG spoch 22\n",
      "Loss = 5.7295e-05, PNorm = 83.7517, GNorm = 0.0666, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.159705\n",
      "SWAG spoch 23\n",
      "Loss = 5.7752e-05, PNorm = 83.6189, GNorm = 0.0348, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.161438\n",
      "SWAG spoch 24\n",
      "Loss = 5.8237e-05, PNorm = 83.4863, GNorm = 0.0378, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.162354\n",
      "Model 2, sample 0 test mae = 2.225742\n",
      "Model 2, sample 1 test mae = 2.230118\n",
      "Model 2, sample 2 test mae = 2.218731\n",
      "Model 2, sample 3 test mae = 2.231678\n",
      "Model 2, sample 4 test mae = 2.229570\n",
      "Model 2, sample 5 test mae = 2.225052\n",
      "Model 2, sample 6 test mae = 2.229461\n",
      "Model 2, sample 7 test mae = 2.250554\n",
      "Model 2, sample 8 test mae = 2.254903\n",
      "Model 2, sample 9 test mae = 2.226384\n",
      "Model 2, sample 10 test mae = 2.250276\n",
      "Model 2, sample 11 test mae = 2.224129\n",
      "Model 2, sample 12 test mae = 2.237069\n",
      "Model 2, sample 13 test mae = 2.230264\n",
      "Model 2, sample 14 test mae = 2.225819\n",
      "Model 2, sample 15 test mae = 2.228765\n",
      "Model 2, sample 16 test mae = 2.224415\n",
      "Model 2, sample 17 test mae = 2.231771\n",
      "Model 2, sample 18 test mae = 2.234063\n",
      "Model 2, sample 19 test mae = 2.226614\n",
      "Model 2, sample 20 test mae = 2.228891\n",
      "Model 2, sample 21 test mae = 2.225243\n",
      "Model 2, sample 22 test mae = 2.232690\n",
      "Model 2, sample 23 test mae = 2.233211\n",
      "Model 2, sample 24 test mae = 2.228769\n",
      "Model 2, sample 25 test mae = 2.234775\n",
      "Model 2, sample 26 test mae = 2.228569\n",
      "Model 2, sample 27 test mae = 2.220528\n",
      "Model 2, sample 28 test mae = 2.232295\n",
      "Model 2, sample 29 test mae = 2.240625\n",
      "Building model 3\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7576e-03, PNorm = 49.3485, GNorm = 1.5390, lr_0 = 5.5056e-04\n",
      "Validation mae = 12.919380\n",
      "Epoch 1\n",
      "Loss = 3.4265e-03, PNorm = 51.6531, GNorm = 1.1568, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.465199\n",
      "Epoch 2\n",
      "Loss = 2.1013e-03, PNorm = 54.0490, GNorm = 0.8204, lr_0 = 9.5310e-04\n",
      "Validation mae = 6.859334\n",
      "Epoch 3\n",
      "Loss = 1.5772e-03, PNorm = 55.8988, GNorm = 0.7789, lr_0 = 9.0846e-04\n",
      "Validation mae = 6.763869\n",
      "Epoch 4\n",
      "Loss = 1.3576e-03, PNorm = 57.7549, GNorm = 0.6512, lr_0 = 8.6591e-04\n",
      "Validation mae = 6.062253\n",
      "Epoch 5\n",
      "Loss = 1.1835e-03, PNorm = 59.3985, GNorm = 0.6757, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.293964\n",
      "Epoch 6\n",
      "Loss = 1.0500e-03, PNorm = 61.1270, GNorm = 0.4871, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.333022\n",
      "Epoch 7\n",
      "Loss = 9.2979e-04, PNorm = 62.8567, GNorm = 0.3850, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.895644\n",
      "Epoch 8\n",
      "Loss = 8.9112e-04, PNorm = 64.6064, GNorm = 0.2486, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.018809\n",
      "Epoch 9\n",
      "Loss = 7.9460e-04, PNorm = 66.2099, GNorm = 0.3509, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.328329\n",
      "Epoch 10\n",
      "Loss = 7.2390e-04, PNorm = 67.7091, GNorm = 0.2137, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.001607\n",
      "Epoch 11\n",
      "Loss = 6.4512e-04, PNorm = 69.1590, GNorm = 0.2325, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.855794\n",
      "Epoch 12\n",
      "Loss = 5.9901e-04, PNorm = 70.4653, GNorm = 0.3402, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.771127\n",
      "Epoch 13\n",
      "Loss = 5.4134e-04, PNorm = 71.6130, GNorm = 1.5710, lr_0 = 5.6231e-04\n",
      "Validation mae = 4.025747\n",
      "Epoch 14\n",
      "Loss = 5.3119e-04, PNorm = 72.9458, GNorm = 0.4376, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.360526\n",
      "Epoch 15\n",
      "Loss = 4.9496e-04, PNorm = 74.1778, GNorm = 0.2866, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.556882\n",
      "Epoch 16\n",
      "Loss = 4.4847e-04, PNorm = 75.2034, GNorm = 0.5666, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.108474\n",
      "Epoch 17\n",
      "Loss = 4.0390e-04, PNorm = 76.1011, GNorm = 0.1331, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.285515\n",
      "Epoch 18\n",
      "Loss = 3.9212e-04, PNorm = 77.0453, GNorm = 0.1846, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.909753\n",
      "Epoch 19\n",
      "Loss = 3.5302e-04, PNorm = 77.9260, GNorm = 0.3493, lr_0 = 4.2167e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 3.265083\n",
      "Epoch 20\n",
      "Loss = 3.1352e-04, PNorm = 78.6282, GNorm = 0.1416, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.679342\n",
      "Epoch 21\n",
      "Loss = 2.7890e-04, PNorm = 79.3054, GNorm = 0.1592, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.617798\n",
      "Epoch 22\n",
      "Loss = 2.6362e-04, PNorm = 79.9578, GNorm = 0.2558, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.562533\n",
      "Epoch 23\n",
      "Loss = 2.5085e-04, PNorm = 80.5989, GNorm = 0.1291, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.895226\n",
      "Epoch 24\n",
      "Loss = 2.5371e-04, PNorm = 81.3100, GNorm = 0.3356, lr_0 = 3.3175e-04\n",
      "Validation mae = 3.066902\n",
      "Epoch 25\n",
      "Loss = 2.1825e-04, PNorm = 81.8059, GNorm = 0.1543, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.562028\n",
      "Epoch 26\n",
      "Loss = 1.9812e-04, PNorm = 82.2636, GNorm = 0.0983, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.412629\n",
      "Epoch 27\n",
      "Loss = 1.7559e-04, PNorm = 82.6920, GNorm = 0.0798, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.477852\n",
      "Epoch 28\n",
      "Loss = 1.6420e-04, PNorm = 83.1166, GNorm = 0.0854, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.430160\n",
      "Epoch 29\n",
      "Loss = 1.5912e-04, PNorm = 83.5074, GNorm = 0.1153, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.467516\n",
      "Epoch 30\n",
      "Loss = 1.4448e-04, PNorm = 83.8570, GNorm = 0.0687, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.436183\n",
      "Epoch 31\n",
      "Loss = 1.4148e-04, PNorm = 84.2438, GNorm = 0.1060, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.416459\n",
      "Epoch 32\n",
      "Loss = 1.3072e-04, PNorm = 84.5245, GNorm = 0.0882, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.343747\n",
      "Epoch 33\n",
      "Loss = 1.2222e-04, PNorm = 84.8201, GNorm = 0.0947, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.328721\n",
      "Epoch 34\n",
      "Loss = 1.1257e-04, PNorm = 85.0798, GNorm = 0.0801, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.429635\n",
      "Epoch 35\n",
      "Loss = 1.0974e-04, PNorm = 85.3424, GNorm = 0.0803, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.296197\n",
      "Epoch 36\n",
      "Loss = 1.0040e-04, PNorm = 85.5593, GNorm = 0.0885, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.320274\n",
      "Epoch 37\n",
      "Loss = 9.6716e-05, PNorm = 85.7796, GNorm = 0.0714, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.255300\n",
      "Epoch 38\n",
      "Loss = 9.1019e-05, PNorm = 85.9777, GNorm = 0.0545, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.244435\n",
      "Epoch 39\n",
      "Loss = 8.7365e-05, PNorm = 86.1627, GNorm = 0.0695, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.227994\n",
      "Epoch 40\n",
      "Loss = 8.1687e-05, PNorm = 86.3394, GNorm = 0.0997, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.249270\n",
      "Epoch 41\n",
      "Loss = 7.7811e-05, PNorm = 86.5010, GNorm = 0.0561, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.215478\n",
      "Epoch 42\n",
      "Loss = 7.5444e-05, PNorm = 86.6549, GNorm = 0.0480, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.168490\n",
      "Epoch 43\n",
      "Loss = 7.2440e-05, PNorm = 86.7928, GNorm = 0.0443, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.227147\n",
      "Epoch 44\n",
      "Loss = 6.8809e-05, PNorm = 86.9380, GNorm = 0.0648, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.250836\n",
      "Epoch 45\n",
      "Loss = 6.6629e-05, PNorm = 87.0578, GNorm = 0.0497, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.199094\n",
      "Epoch 46\n",
      "Loss = 6.4580e-05, PNorm = 87.1815, GNorm = 0.0532, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.232791\n",
      "Epoch 47\n",
      "Loss = 6.2442e-05, PNorm = 87.2904, GNorm = 0.0390, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.146020\n",
      "Epoch 48\n",
      "Loss = 5.9966e-05, PNorm = 87.3923, GNorm = 0.0489, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.168299\n",
      "Epoch 49\n",
      "Loss = 5.8256e-05, PNorm = 87.4912, GNorm = 0.0357, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.148110\n",
      "Model 3 best validation mae = 2.146020 on epoch 47\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.3776e-05, PNorm = 87.1513, GNorm = 0.0341, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.140486\n",
      "SWAG spoch 1\n",
      "Loss = 5.3113e-05, PNorm = 87.0123, GNorm = 0.0335, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.137494\n",
      "SWAG spoch 2\n",
      "Loss = 5.2871e-05, PNorm = 86.8736, GNorm = 0.0469, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135058\n",
      "SWAG spoch 3\n",
      "Loss = 5.2754e-05, PNorm = 86.7355, GNorm = 0.0372, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.133804\n",
      "SWAG spoch 4\n",
      "Loss = 5.2745e-05, PNorm = 86.5975, GNorm = 0.0304, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132138\n",
      "SWAG spoch 5\n",
      "Loss = 5.2774e-05, PNorm = 86.4601, GNorm = 0.0314, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131437\n",
      "SWAG spoch 6\n",
      "Loss = 5.2892e-05, PNorm = 86.3227, GNorm = 0.0325, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130470\n",
      "SWAG spoch 7\n",
      "Loss = 5.3025e-05, PNorm = 86.1855, GNorm = 0.0618, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129613\n",
      "SWAG spoch 8\n",
      "Loss = 5.3171e-05, PNorm = 86.0485, GNorm = 0.0461, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130086\n",
      "SWAG spoch 9\n",
      "Loss = 5.3384e-05, PNorm = 85.9117, GNorm = 0.0395, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129883\n",
      "SWAG spoch 10\n",
      "Loss = 5.3617e-05, PNorm = 85.7753, GNorm = 0.0378, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130227\n",
      "SWAG spoch 11\n",
      "Loss = 5.3861e-05, PNorm = 85.6391, GNorm = 0.0332, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129896\n",
      "SWAG spoch 12\n",
      "Loss = 5.4114e-05, PNorm = 85.5031, GNorm = 0.0437, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130475\n",
      "SWAG spoch 13\n",
      "Loss = 5.4425e-05, PNorm = 85.3675, GNorm = 0.0501, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130335\n",
      "SWAG spoch 14\n",
      "Loss = 5.4721e-05, PNorm = 85.2318, GNorm = 0.0334, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131010\n",
      "SWAG spoch 15\n",
      "Loss = 5.5064e-05, PNorm = 85.0964, GNorm = 0.0330, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131746\n",
      "SWAG spoch 16\n",
      "Loss = 5.5391e-05, PNorm = 84.9614, GNorm = 0.2928, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132418\n",
      "SWAG spoch 17\n",
      "Loss = 5.5770e-05, PNorm = 84.8264, GNorm = 0.0407, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.133075\n",
      "SWAG spoch 18\n",
      "Loss = 5.6120e-05, PNorm = 84.6919, GNorm = 0.0764, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.134090\n",
      "SWAG spoch 19\n",
      "Loss = 5.6529e-05, PNorm = 84.5576, GNorm = 0.0628, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135101\n",
      "SWAG spoch 20\n",
      "Loss = 5.6929e-05, PNorm = 84.4239, GNorm = 0.0432, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135784\n",
      "SWAG spoch 21\n",
      "Loss = 5.7329e-05, PNorm = 84.2900, GNorm = 0.0463, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.137901\n",
      "SWAG spoch 22\n",
      "Loss = 5.7786e-05, PNorm = 84.1564, GNorm = 0.0351, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138515\n",
      "SWAG spoch 23\n",
      "Loss = 5.8218e-05, PNorm = 84.0230, GNorm = 0.0389, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.140039\n",
      "SWAG spoch 24\n",
      "Loss = 5.8621e-05, PNorm = 83.8897, GNorm = 0.1448, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141778\n",
      "Model 3, sample 0 test mae = 2.270610\n",
      "Model 3, sample 1 test mae = 2.267141\n",
      "Model 3, sample 2 test mae = 2.265088\n",
      "Model 3, sample 3 test mae = 2.259223\n",
      "Model 3, sample 4 test mae = 2.269346\n",
      "Model 3, sample 5 test mae = 2.295091\n",
      "Model 3, sample 6 test mae = 2.272165\n",
      "Model 3, sample 7 test mae = 2.268318\n",
      "Model 3, sample 8 test mae = 2.281943\n",
      "Model 3, sample 9 test mae = 2.262489\n",
      "Model 3, sample 10 test mae = 2.329455\n",
      "Model 3, sample 11 test mae = 2.254573\n",
      "Model 3, sample 12 test mae = 2.264637\n",
      "Model 3, sample 13 test mae = 2.259683\n",
      "Model 3, sample 14 test mae = 2.264649\n",
      "Model 3, sample 15 test mae = 2.261662\n",
      "Model 3, sample 16 test mae = 2.293636\n",
      "Model 3, sample 17 test mae = 2.264834\n",
      "Model 3, sample 18 test mae = 2.259867\n",
      "Model 3, sample 19 test mae = 2.257071\n",
      "Model 3, sample 20 test mae = 2.259001\n",
      "Model 3, sample 21 test mae = 2.277248\n",
      "Model 3, sample 22 test mae = 2.255652\n",
      "Model 3, sample 23 test mae = 2.264437\n",
      "Model 3, sample 24 test mae = 2.260369\n",
      "Model 3, sample 25 test mae = 2.262988\n",
      "Model 3, sample 26 test mae = 2.269622\n",
      "Model 3, sample 27 test mae = 2.253998\n",
      "Model 3, sample 28 test mae = 2.259159\n",
      "Model 3, sample 29 test mae = 2.265367\n",
      "Building model 4\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7159e-03, PNorm = 49.3155, GNorm = 2.0839, lr_0 = 5.5056e-04\n",
      "Validation mae = 11.727915\n",
      "Epoch 1\n",
      "Loss = 3.4599e-03, PNorm = 51.6548, GNorm = 1.8837, lr_0 = 9.9994e-04\n",
      "Validation mae = 10.218139\n",
      "Epoch 2\n",
      "Loss = 2.1138e-03, PNorm = 54.0481, GNorm = 0.7161, lr_0 = 9.5310e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 8.298347\n",
      "Epoch 3\n",
      "Loss = 1.6387e-03, PNorm = 55.9716, GNorm = 0.8944, lr_0 = 9.0846e-04\n",
      "Validation mae = 6.624432\n",
      "Epoch 4\n",
      "Loss = 1.3116e-03, PNorm = 57.5724, GNorm = 0.4368, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.477179\n",
      "Epoch 5\n",
      "Loss = 1.1780e-03, PNorm = 59.4190, GNorm = 0.3029, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.154437\n",
      "Epoch 6\n",
      "Loss = 1.0581e-03, PNorm = 61.2869, GNorm = 0.7567, lr_0 = 7.8670e-04\n",
      "Validation mae = 5.454299\n",
      "Epoch 7\n",
      "Loss = 9.7366e-04, PNorm = 63.0687, GNorm = 0.4572, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.702783\n",
      "Epoch 8\n",
      "Loss = 8.6694e-04, PNorm = 64.7170, GNorm = 0.4127, lr_0 = 7.1473e-04\n",
      "Validation mae = 5.222343\n",
      "Epoch 9\n",
      "Loss = 7.7720e-04, PNorm = 66.2566, GNorm = 0.2193, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.124288\n",
      "Epoch 10\n",
      "Loss = 7.4297e-04, PNorm = 67.8533, GNorm = 0.2003, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.002275\n",
      "Epoch 11\n",
      "Loss = 6.7003e-04, PNorm = 69.2253, GNorm = 0.2530, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.538144\n",
      "Epoch 12\n",
      "Loss = 6.1203e-04, PNorm = 70.6149, GNorm = 0.3767, lr_0 = 5.8994e-04\n",
      "Validation mae = 4.153420\n",
      "Epoch 13\n",
      "Loss = 5.6651e-04, PNorm = 71.8257, GNorm = 0.1753, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.373087\n",
      "Epoch 14\n",
      "Loss = 5.2943e-04, PNorm = 73.0957, GNorm = 0.2248, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.562291\n",
      "Epoch 15\n",
      "Loss = 4.8390e-04, PNorm = 74.2339, GNorm = 0.2022, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.036268\n",
      "Epoch 16\n",
      "Loss = 4.3123e-04, PNorm = 75.1930, GNorm = 0.1978, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.462144\n",
      "Epoch 17\n",
      "Loss = 4.2376e-04, PNorm = 76.2188, GNorm = 0.2805, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.136952\n",
      "Epoch 18\n",
      "Loss = 3.9298e-04, PNorm = 77.1398, GNorm = 0.1167, lr_0 = 4.4239e-04\n",
      "Validation mae = 3.281673\n",
      "Epoch 19\n",
      "Loss = 3.5659e-04, PNorm = 77.9868, GNorm = 0.1108, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.880464\n",
      "Epoch 20\n",
      "Loss = 3.3258e-04, PNorm = 78.7237, GNorm = 0.1834, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.880090\n",
      "Epoch 21\n",
      "Loss = 2.8707e-04, PNorm = 79.3607, GNorm = 0.0931, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.582914\n",
      "Epoch 22\n",
      "Loss = 2.6195e-04, PNorm = 79.9597, GNorm = 0.1359, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.640796\n",
      "Epoch 23\n",
      "Loss = 2.5713e-04, PNorm = 80.6179, GNorm = 0.1134, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.725228\n",
      "Epoch 24\n",
      "Loss = 2.3035e-04, PNorm = 81.1708, GNorm = 0.1386, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.694921\n",
      "Epoch 25\n",
      "Loss = 2.1148e-04, PNorm = 81.7009, GNorm = 0.1236, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.475991\n",
      "Epoch 26\n",
      "Loss = 2.0453e-04, PNorm = 82.2151, GNorm = 0.1157, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.566769\n",
      "Epoch 27\n",
      "Loss = 1.8563e-04, PNorm = 82.6660, GNorm = 0.1530, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.548427\n",
      "Epoch 28\n",
      "Loss = 1.9527e-04, PNorm = 83.1794, GNorm = 0.1137, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.447173\n",
      "Epoch 29\n",
      "Loss = 1.5719e-04, PNorm = 83.5169, GNorm = 0.0961, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.377386\n",
      "Epoch 30\n",
      "Loss = 1.5010e-04, PNorm = 83.8802, GNorm = 0.1139, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.488261\n",
      "Epoch 31\n",
      "Loss = 1.3960e-04, PNorm = 84.1858, GNorm = 0.0660, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.317524\n",
      "Epoch 32\n",
      "Loss = 1.2754e-04, PNorm = 84.4849, GNorm = 0.0587, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.399680\n",
      "Epoch 33\n",
      "Loss = 1.2055e-04, PNorm = 84.7773, GNorm = 0.0728, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.309373\n",
      "Epoch 34\n",
      "Loss = 1.1817e-04, PNorm = 85.0637, GNorm = 0.1142, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.416023\n",
      "Epoch 35\n",
      "Loss = 1.1224e-04, PNorm = 85.3171, GNorm = 0.0665, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.294408\n",
      "Epoch 36\n",
      "Loss = 1.0029e-04, PNorm = 85.5235, GNorm = 0.3117, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.278675\n",
      "Epoch 37\n",
      "Loss = 9.6749e-05, PNorm = 85.7468, GNorm = 0.0549, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.287527\n",
      "Epoch 38\n",
      "Loss = 9.6737e-05, PNorm = 85.9608, GNorm = 0.0701, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.245878\n",
      "Epoch 39\n",
      "Loss = 8.5876e-05, PNorm = 86.1369, GNorm = 0.0614, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.270774\n",
      "Epoch 40\n",
      "Loss = 8.2186e-05, PNorm = 86.3014, GNorm = 0.1192, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.253602\n",
      "Epoch 41\n",
      "Loss = 7.9126e-05, PNorm = 86.4675, GNorm = 0.0704, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.242687\n",
      "Epoch 42\n",
      "Loss = 7.6455e-05, PNorm = 86.6225, GNorm = 0.0534, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.218960\n",
      "Epoch 43\n",
      "Loss = 7.3413e-05, PNorm = 86.7640, GNorm = 0.0512, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.215485\n",
      "Epoch 44\n",
      "Loss = 6.9473e-05, PNorm = 86.9043, GNorm = 0.0681, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.190530\n",
      "Epoch 45\n",
      "Loss = 6.7967e-05, PNorm = 87.0310, GNorm = 0.0528, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.179551\n",
      "Epoch 46\n",
      "Loss = 6.5201e-05, PNorm = 87.1480, GNorm = 0.0513, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.205795\n",
      "Epoch 47\n",
      "Loss = 6.2388e-05, PNorm = 87.2620, GNorm = 0.0922, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.215210\n",
      "Epoch 48\n",
      "Loss = 6.0733e-05, PNorm = 87.3591, GNorm = 0.0549, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.172427\n",
      "Epoch 49\n",
      "Loss = 5.8739e-05, PNorm = 87.4598, GNorm = 0.0383, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.183821\n",
      "Model 4 best validation mae = 2.172427 on epoch 48\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.2297e-05, PNorm = 87.2198, GNorm = 0.0306, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.154820\n",
      "SWAG spoch 1\n",
      "Loss = 5.1386e-05, PNorm = 87.0806, GNorm = 0.2603, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.151157\n",
      "SWAG spoch 2\n",
      "Loss = 5.1189e-05, PNorm = 86.9421, GNorm = 0.0359, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.146926\n",
      "SWAG spoch 3\n",
      "Loss = 5.1107e-05, PNorm = 86.8038, GNorm = 0.0570, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.145169\n",
      "SWAG spoch 4\n",
      "Loss = 5.1155e-05, PNorm = 86.6658, GNorm = 0.0291, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.143678\n",
      "SWAG spoch 5\n",
      "Loss = 5.1235e-05, PNorm = 86.5279, GNorm = 0.0327, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.142844\n",
      "SWAG spoch 6\n",
      "Loss = 5.1398e-05, PNorm = 86.3903, GNorm = 0.0304, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141852\n",
      "SWAG spoch 7\n",
      "Loss = 5.1581e-05, PNorm = 86.2530, GNorm = 0.1416, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141117\n",
      "SWAG spoch 8\n",
      "Loss = 5.1788e-05, PNorm = 86.1160, GNorm = 0.0418, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.140805\n",
      "SWAG spoch 9\n",
      "Loss = 5.2019e-05, PNorm = 85.9792, GNorm = 0.0349, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141588\n",
      "SWAG spoch 10\n",
      "Loss = 5.2291e-05, PNorm = 85.8424, GNorm = 0.0363, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141785\n",
      "SWAG spoch 11\n",
      "Loss = 5.2595e-05, PNorm = 85.7060, GNorm = 0.0423, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.140984\n",
      "SWAG spoch 12\n",
      "Loss = 5.2874e-05, PNorm = 85.5698, GNorm = 0.0629, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.142047\n",
      "SWAG spoch 13\n",
      "Loss = 5.3242e-05, PNorm = 85.4341, GNorm = 0.0314, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.142046\n",
      "SWAG spoch 14\n",
      "Loss = 5.3567e-05, PNorm = 85.2986, GNorm = 0.0360, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.142648\n",
      "SWAG spoch 15\n",
      "Loss = 5.3944e-05, PNorm = 85.1630, GNorm = 0.0334, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.143336\n",
      "SWAG spoch 16\n",
      "Loss = 5.4314e-05, PNorm = 85.0278, GNorm = 0.0381, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.144036\n",
      "SWAG spoch 17\n",
      "Loss = 5.4713e-05, PNorm = 84.8930, GNorm = 0.0335, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.144764\n",
      "SWAG spoch 18\n",
      "Loss = 5.5089e-05, PNorm = 84.7582, GNorm = 0.0386, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.145996\n",
      "SWAG spoch 19\n",
      "Loss = 5.5538e-05, PNorm = 84.6239, GNorm = 0.0538, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.146628\n",
      "SWAG spoch 20\n",
      "Loss = 5.5970e-05, PNorm = 84.4897, GNorm = 0.0335, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.147555\n",
      "SWAG spoch 21\n",
      "Loss = 5.6408e-05, PNorm = 84.3558, GNorm = 0.0406, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.148544\n",
      "SWAG spoch 22\n",
      "Loss = 5.6858e-05, PNorm = 84.2221, GNorm = 0.0328, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.149930\n",
      "SWAG spoch 23\n",
      "Loss = 5.7332e-05, PNorm = 84.0883, GNorm = 0.0383, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.151344\n",
      "SWAG spoch 24\n",
      "Loss = 5.7832e-05, PNorm = 83.9553, GNorm = 0.0373, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.152478\n",
      "Model 4, sample 0 test mae = 2.245333\n",
      "Model 4, sample 1 test mae = 2.230627\n",
      "Model 4, sample 2 test mae = 2.229731\n",
      "Model 4, sample 3 test mae = 2.242014\n",
      "Model 4, sample 4 test mae = 2.252870\n",
      "Model 4, sample 5 test mae = 2.226154\n",
      "Model 4, sample 6 test mae = 2.232734\n",
      "Model 4, sample 7 test mae = 2.256310\n",
      "Model 4, sample 8 test mae = 2.226736\n",
      "Model 4, sample 9 test mae = 2.223841\n",
      "Model 4, sample 10 test mae = 2.322481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4, sample 11 test mae = 2.225902\n",
      "Model 4, sample 12 test mae = 2.239711\n",
      "Model 4, sample 13 test mae = 2.232309\n",
      "Model 4, sample 14 test mae = 2.222795\n",
      "Model 4, sample 15 test mae = 2.228702\n",
      "Model 4, sample 16 test mae = 2.254737\n",
      "Model 4, sample 17 test mae = 2.230978\n",
      "Model 4, sample 18 test mae = 2.272125\n",
      "Model 4, sample 19 test mae = 2.226917\n",
      "Model 4, sample 20 test mae = 2.223904\n",
      "Model 4, sample 21 test mae = 2.233976\n",
      "Model 4, sample 22 test mae = 2.231906\n",
      "Model 4, sample 23 test mae = 2.235409\n",
      "Model 4, sample 24 test mae = 2.233160\n",
      "Model 4, sample 25 test mae = 2.221562\n",
      "Model 4, sample 26 test mae = 2.225460\n",
      "Model 4, sample 27 test mae = 2.254232\n",
      "Model 4, sample 28 test mae = 2.243817\n",
      "Model 4, sample 29 test mae = 2.226292\n",
      "Building model 5\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7693e-03, PNorm = 49.3384, GNorm = 1.5965, lr_0 = 5.5056e-04\n",
      "Validation mae = 13.740121\n",
      "Epoch 1\n",
      "Loss = 3.5186e-03, PNorm = 51.6079, GNorm = 2.0550, lr_0 = 9.9994e-04\n",
      "Validation mae = 12.406653\n",
      "Epoch 2\n",
      "Loss = 2.2251e-03, PNorm = 53.9931, GNorm = 0.9008, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.116071\n",
      "Epoch 3\n",
      "Loss = 1.5797e-03, PNorm = 55.8078, GNorm = 0.6138, lr_0 = 9.0846e-04\n",
      "Validation mae = 6.583184\n",
      "Epoch 4\n",
      "Loss = 1.3574e-03, PNorm = 57.5440, GNorm = 0.5190, lr_0 = 8.6591e-04\n",
      "Validation mae = 6.417748\n",
      "Epoch 5\n",
      "Loss = 1.2051e-03, PNorm = 59.2693, GNorm = 0.6016, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.815966\n",
      "Epoch 6\n",
      "Loss = 1.0584e-03, PNorm = 60.8910, GNorm = 0.4792, lr_0 = 7.8670e-04\n",
      "Validation mae = 5.708163\n",
      "Epoch 7\n",
      "Loss = 9.7463e-04, PNorm = 62.6177, GNorm = 0.7492, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.909701\n",
      "Epoch 8\n",
      "Loss = 8.8130e-04, PNorm = 64.2306, GNorm = 0.3903, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.330409\n",
      "Epoch 9\n",
      "Loss = 8.0336e-04, PNorm = 65.8138, GNorm = 0.1817, lr_0 = 6.8125e-04\n",
      "Validation mae = 3.744648\n",
      "Epoch 10\n",
      "Loss = 7.5709e-04, PNorm = 67.3863, GNorm = 0.2437, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.146481\n",
      "Epoch 11\n",
      "Loss = 6.6733e-04, PNorm = 68.7503, GNorm = 0.4574, lr_0 = 6.1893e-04\n",
      "Validation mae = 4.434734\n",
      "Epoch 12\n",
      "Loss = 6.0811e-04, PNorm = 69.9962, GNorm = 0.2338, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.535122\n",
      "Epoch 13\n",
      "Loss = 6.2073e-04, PNorm = 71.4794, GNorm = 0.2653, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.657186\n",
      "Epoch 14\n",
      "Loss = 5.0614e-04, PNorm = 72.5013, GNorm = 0.2515, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.585626\n",
      "Epoch 15\n",
      "Loss = 4.9297e-04, PNorm = 73.6344, GNorm = 0.1405, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.161815\n",
      "Epoch 16\n",
      "Loss = 4.5332e-04, PNorm = 74.6829, GNorm = 0.1793, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.167388\n",
      "Epoch 17\n",
      "Loss = 4.0017e-04, PNorm = 75.5709, GNorm = 0.3046, lr_0 = 4.6413e-04\n",
      "Validation mae = 2.988890\n",
      "Epoch 18\n",
      "Loss = 3.6772e-04, PNorm = 76.3952, GNorm = 0.2303, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.835086\n",
      "Epoch 19\n",
      "Loss = 3.6121e-04, PNorm = 77.3258, GNorm = 0.1352, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.324795\n",
      "Epoch 20\n",
      "Loss = 3.2992e-04, PNorm = 78.1116, GNorm = 0.1464, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.802672\n",
      "Epoch 21\n",
      "Loss = 2.9316e-04, PNorm = 78.8141, GNorm = 0.1368, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.814442\n",
      "Epoch 22\n",
      "Loss = 2.7573e-04, PNorm = 79.4560, GNorm = 0.1560, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.802820\n",
      "Epoch 23\n",
      "Loss = 2.6033e-04, PNorm = 80.1150, GNorm = 0.1376, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.717969\n",
      "Epoch 24\n",
      "Loss = 2.3659e-04, PNorm = 80.6689, GNorm = 0.1856, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.611604\n",
      "Epoch 25\n",
      "Loss = 2.2431e-04, PNorm = 81.2354, GNorm = 0.1292, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.721507\n",
      "Epoch 26\n",
      "Loss = 2.1323e-04, PNorm = 81.7470, GNorm = 0.1157, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.530136\n",
      "Epoch 27\n",
      "Loss = 1.8850e-04, PNorm = 82.1809, GNorm = 0.1181, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.453508\n",
      "Epoch 28\n",
      "Loss = 1.7526e-04, PNorm = 82.6000, GNorm = 0.1319, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.516944\n",
      "Epoch 29\n",
      "Loss = 1.7372e-04, PNorm = 83.0208, GNorm = 0.0987, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.443673\n",
      "Epoch 30\n",
      "Loss = 1.5812e-04, PNorm = 83.3778, GNorm = 0.0871, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.459594\n",
      "Epoch 31\n",
      "Loss = 1.4828e-04, PNorm = 83.7180, GNorm = 0.0630, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.446241\n",
      "Epoch 32\n",
      "Loss = 1.3523e-04, PNorm = 84.0248, GNorm = 0.0920, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.348489\n",
      "Epoch 33\n",
      "Loss = 1.3076e-04, PNorm = 84.3279, GNorm = 0.0915, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.337800\n",
      "Epoch 34\n",
      "Loss = 1.2010e-04, PNorm = 84.5976, GNorm = 0.0781, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.359263\n",
      "Epoch 35\n",
      "Loss = 1.1470e-04, PNorm = 84.8690, GNorm = 0.0732, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.330607\n",
      "Epoch 36\n",
      "Loss = 1.1215e-04, PNorm = 85.1229, GNorm = 0.0655, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.338794\n",
      "Epoch 37\n",
      "Loss = 1.0450e-04, PNorm = 85.3319, GNorm = 0.5642, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.287719\n",
      "Epoch 38\n",
      "Loss = 9.3810e-05, PNorm = 85.5243, GNorm = 0.0714, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.433405\n",
      "Epoch 39\n",
      "Loss = 8.9697e-05, PNorm = 85.7119, GNorm = 0.0879, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.264672\n",
      "Epoch 40\n",
      "Loss = 9.0667e-05, PNorm = 85.9161, GNorm = 0.0643, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.263093\n",
      "Epoch 41\n",
      "Loss = 8.2584e-05, PNorm = 86.0772, GNorm = 0.0747, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.247984\n",
      "Epoch 42\n",
      "Loss = 7.6682e-05, PNorm = 86.2399, GNorm = 0.0664, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.286120\n",
      "Epoch 43\n",
      "Loss = 7.4261e-05, PNorm = 86.3820, GNorm = 0.0558, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.256892\n",
      "Epoch 44\n",
      "Loss = 7.1360e-05, PNorm = 86.5220, GNorm = 0.0799, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.322776\n",
      "Epoch 45\n",
      "Loss = 6.8759e-05, PNorm = 86.6525, GNorm = 0.0603, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.197814\n",
      "Epoch 46\n",
      "Loss = 6.7401e-05, PNorm = 86.7796, GNorm = 0.0474, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.208814\n",
      "Epoch 47\n",
      "Loss = 6.6204e-05, PNorm = 86.8950, GNorm = 0.0316, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.211054\n",
      "Epoch 48\n",
      "Loss = 6.4444e-05, PNorm = 87.0139, GNorm = 0.0864, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.222917\n",
      "Epoch 49\n",
      "Loss = 6.0504e-05, PNorm = 87.1138, GNorm = 0.0407, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.183048\n",
      "Model 5 best validation mae = 2.183048 on epoch 49\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.3097e-05, PNorm = 86.9749, GNorm = 0.0463, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.168142\n",
      "SWAG spoch 1\n",
      "Loss = 5.2300e-05, PNorm = 86.8365, GNorm = 0.0362, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.164390\n",
      "SWAG spoch 2\n",
      "Loss = 5.2039e-05, PNorm = 86.6981, GNorm = 0.0314, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.161477\n",
      "SWAG spoch 3\n",
      "Loss = 5.1925e-05, PNorm = 86.5602, GNorm = 0.0262, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.160070\n",
      "SWAG spoch 4\n",
      "Loss = 5.1897e-05, PNorm = 86.4226, GNorm = 0.0303, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.159706\n",
      "SWAG spoch 5\n",
      "Loss = 5.1958e-05, PNorm = 86.2852, GNorm = 0.2801, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.159143\n",
      "SWAG spoch 6\n",
      "Loss = 5.2081e-05, PNorm = 86.1479, GNorm = 0.0562, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158700\n",
      "SWAG spoch 7\n",
      "Loss = 5.2212e-05, PNorm = 86.0109, GNorm = 0.0379, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158578\n",
      "SWAG spoch 8\n",
      "Loss = 5.2394e-05, PNorm = 85.8743, GNorm = 0.0437, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158422\n",
      "SWAG spoch 9\n",
      "Loss = 5.2631e-05, PNorm = 85.7378, GNorm = 0.0873, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158550\n",
      "SWAG spoch 10\n",
      "Loss = 5.2850e-05, PNorm = 85.6018, GNorm = 0.0396, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158962\n",
      "SWAG spoch 11\n",
      "Loss = 5.3144e-05, PNorm = 85.4655, GNorm = 0.0394, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.159215\n",
      "SWAG spoch 12\n",
      "Loss = 5.3417e-05, PNorm = 85.3300, GNorm = 0.0389, lr_0 = 1.0000e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 2.160096\n",
      "SWAG spoch 13\n",
      "Loss = 5.3760e-05, PNorm = 85.1946, GNorm = 0.0336, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.160510\n",
      "SWAG spoch 14\n",
      "Loss = 5.4075e-05, PNorm = 85.0594, GNorm = 0.0866, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.161310\n",
      "SWAG spoch 15\n",
      "Loss = 5.4431e-05, PNorm = 84.9244, GNorm = 0.0318, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.162271\n",
      "SWAG spoch 16\n",
      "Loss = 5.4808e-05, PNorm = 84.7895, GNorm = 0.0344, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.162906\n",
      "SWAG spoch 17\n",
      "Loss = 5.5195e-05, PNorm = 84.6549, GNorm = 0.0380, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.164039\n",
      "SWAG spoch 18\n",
      "Loss = 5.5594e-05, PNorm = 84.5207, GNorm = 0.0339, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.164914\n",
      "SWAG spoch 19\n",
      "Loss = 5.6021e-05, PNorm = 84.3866, GNorm = 0.0497, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.166355\n",
      "SWAG spoch 20\n",
      "Loss = 5.6432e-05, PNorm = 84.2532, GNorm = 0.0443, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.167605\n",
      "SWAG spoch 21\n",
      "Loss = 5.6885e-05, PNorm = 84.1195, GNorm = 0.0861, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.168805\n",
      "SWAG spoch 22\n",
      "Loss = 5.7355e-05, PNorm = 83.9862, GNorm = 0.0463, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169834\n",
      "SWAG spoch 23\n",
      "Loss = 5.7805e-05, PNorm = 83.8531, GNorm = 0.0338, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.171340\n",
      "SWAG spoch 24\n",
      "Loss = 5.8298e-05, PNorm = 83.7203, GNorm = 0.0366, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.172593\n",
      "Model 5, sample 0 test mae = 2.268532\n",
      "Model 5, sample 1 test mae = 2.273898\n",
      "Model 5, sample 2 test mae = 2.237172\n",
      "Model 5, sample 3 test mae = 2.235136\n",
      "Model 5, sample 4 test mae = 2.245115\n",
      "Model 5, sample 5 test mae = 2.291042\n",
      "Model 5, sample 6 test mae = 2.238019\n",
      "Model 5, sample 7 test mae = 2.241117\n",
      "Model 5, sample 8 test mae = 2.242008\n",
      "Model 5, sample 9 test mae = 2.267473\n",
      "Model 5, sample 10 test mae = 2.236380\n",
      "Model 5, sample 11 test mae = 2.240868\n",
      "Model 5, sample 12 test mae = 2.231156\n",
      "Model 5, sample 13 test mae = 2.238726\n",
      "Model 5, sample 14 test mae = 2.236484\n",
      "Model 5, sample 15 test mae = 2.250007\n",
      "Model 5, sample 16 test mae = 2.238282\n",
      "Model 5, sample 17 test mae = 2.261122\n",
      "Model 5, sample 18 test mae = 2.244889\n",
      "Model 5, sample 19 test mae = 2.236536\n",
      "Model 5, sample 20 test mae = 2.243436\n",
      "Model 5, sample 21 test mae = 2.243960\n",
      "Model 5, sample 22 test mae = 2.232194\n",
      "Model 5, sample 23 test mae = 2.239880\n",
      "Model 5, sample 24 test mae = 2.246428\n",
      "Model 5, sample 25 test mae = 2.249432\n",
      "Model 5, sample 26 test mae = 2.239561\n",
      "Model 5, sample 27 test mae = 2.245144\n",
      "Model 5, sample 28 test mae = 2.238678\n",
      "Model 5, sample 29 test mae = 2.256160\n",
      "Building model 6\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.8418e-03, PNorm = 49.2515, GNorm = 4.0758, lr_0 = 5.5056e-04\n",
      "Validation mae = 12.083511\n",
      "Epoch 1\n",
      "Loss = 3.4807e-03, PNorm = 51.6627, GNorm = 2.9620, lr_0 = 9.9994e-04\n",
      "Validation mae = 8.663669\n",
      "Epoch 2\n",
      "Loss = 2.1937e-03, PNorm = 54.1715, GNorm = 0.6016, lr_0 = 9.5310e-04\n",
      "Validation mae = 6.578258\n",
      "Epoch 3\n",
      "Loss = 1.5427e-03, PNorm = 55.8590, GNorm = 0.5919, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.771139\n",
      "Epoch 4\n",
      "Loss = 1.4057e-03, PNorm = 57.7481, GNorm = 0.3676, lr_0 = 8.6591e-04\n",
      "Validation mae = 4.921439\n",
      "Epoch 5\n",
      "Loss = 1.1844e-03, PNorm = 59.5265, GNorm = 0.4072, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.056915\n",
      "Epoch 6\n",
      "Loss = 1.1107e-03, PNorm = 61.2731, GNorm = 0.4331, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.991777\n",
      "Epoch 7\n",
      "Loss = 9.3713e-04, PNorm = 62.8013, GNorm = 0.2991, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.554344\n",
      "Epoch 8\n",
      "Loss = 9.0755e-04, PNorm = 64.6211, GNorm = 0.4714, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.002330\n",
      "Epoch 9\n",
      "Loss = 7.9591e-04, PNorm = 66.1025, GNorm = 0.2772, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.265339\n",
      "Epoch 10\n",
      "Loss = 7.3208e-04, PNorm = 67.5373, GNorm = 0.2141, lr_0 = 6.4934e-04\n",
      "Validation mae = 3.460696\n",
      "Epoch 11\n",
      "Loss = 6.4544e-04, PNorm = 68.8565, GNorm = 0.3276, lr_0 = 6.1893e-04\n",
      "Validation mae = 4.024590\n",
      "Epoch 12\n",
      "Loss = 6.2831e-04, PNorm = 70.3347, GNorm = 0.2114, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.547027\n",
      "Epoch 13\n",
      "Loss = 5.4548e-04, PNorm = 71.4866, GNorm = 0.1804, lr_0 = 5.6231e-04\n",
      "Validation mae = 4.036877\n",
      "Epoch 14\n",
      "Loss = 5.2526e-04, PNorm = 72.7202, GNorm = 0.1985, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.611155\n",
      "Epoch 15\n",
      "Loss = 4.9740e-04, PNorm = 73.8679, GNorm = 0.2605, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.545165\n",
      "Epoch 16\n",
      "Loss = 4.5442e-04, PNorm = 74.9223, GNorm = 0.2598, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.166683\n",
      "Epoch 17\n",
      "Loss = 4.5106e-04, PNorm = 76.0033, GNorm = 0.4511, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.276831\n",
      "Epoch 18\n",
      "Loss = 3.9731e-04, PNorm = 76.8375, GNorm = 0.1782, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.901735\n",
      "Epoch 19\n",
      "Loss = 3.5197e-04, PNorm = 77.6261, GNorm = 0.1334, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.839230\n",
      "Epoch 20\n",
      "Loss = 3.5169e-04, PNorm = 78.4384, GNorm = 0.1803, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.885652\n",
      "Epoch 21\n",
      "Loss = 3.1000e-04, PNorm = 79.1034, GNorm = 0.1971, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.958374\n",
      "Epoch 22\n",
      "Loss = 2.9033e-04, PNorm = 79.7189, GNorm = 0.1622, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.712765\n",
      "Epoch 23\n",
      "Loss = 2.5520e-04, PNorm = 80.2988, GNorm = 0.1398, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.694970\n",
      "Epoch 24\n",
      "Loss = 2.4603e-04, PNorm = 80.8984, GNorm = 0.1431, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.823811\n",
      "Epoch 25\n",
      "Loss = 2.2432e-04, PNorm = 81.4205, GNorm = 0.1005, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.551563\n",
      "Epoch 26\n",
      "Loss = 2.1009e-04, PNorm = 81.8976, GNorm = 0.1582, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.718866\n",
      "Epoch 27\n",
      "Loss = 1.8877e-04, PNorm = 82.3645, GNorm = 0.1175, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.738688\n",
      "Epoch 28\n",
      "Loss = 1.9494e-04, PNorm = 82.8417, GNorm = 0.1393, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.563168\n",
      "Epoch 29\n",
      "Loss = 1.7209e-04, PNorm = 83.2168, GNorm = 0.0860, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.501145\n",
      "Epoch 30\n",
      "Loss = 1.5025e-04, PNorm = 83.5287, GNorm = 0.1002, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.525265\n",
      "Epoch 31\n",
      "Loss = 1.4072e-04, PNorm = 83.8710, GNorm = 0.1207, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.544096\n",
      "Epoch 32\n",
      "Loss = 1.4016e-04, PNorm = 84.1827, GNorm = 0.0685, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.405705\n",
      "Epoch 33\n",
      "Loss = 1.2675e-04, PNorm = 84.4573, GNorm = 0.0862, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.353140\n",
      "Epoch 34\n",
      "Loss = 1.1697e-04, PNorm = 84.7248, GNorm = 0.0745, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.381244\n",
      "Epoch 35\n",
      "Loss = 1.1144e-04, PNorm = 84.9677, GNorm = 0.0940, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.406607\n",
      "Epoch 36\n",
      "Loss = 1.0769e-04, PNorm = 85.2033, GNorm = 0.0644, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.291856\n",
      "Epoch 37\n",
      "Loss = 9.8441e-05, PNorm = 85.4080, GNorm = 0.0523, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.291527\n",
      "Epoch 38\n",
      "Loss = 9.4461e-05, PNorm = 85.6031, GNorm = 0.0822, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.341321\n",
      "Epoch 39\n",
      "Loss = 8.9125e-05, PNorm = 85.7800, GNorm = 0.1703, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.450690\n",
      "Epoch 40\n",
      "Loss = 8.4070e-05, PNorm = 85.9483, GNorm = 0.0750, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.257724\n",
      "Epoch 41\n",
      "Loss = 8.2934e-05, PNorm = 86.1287, GNorm = 0.0547, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.240773\n",
      "Epoch 42\n",
      "Loss = 7.8629e-05, PNorm = 86.2730, GNorm = 0.0929, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.292041\n",
      "Epoch 43\n",
      "Loss = 7.3756e-05, PNorm = 86.4137, GNorm = 0.0431, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.282260\n",
      "Epoch 44\n",
      "Loss = 7.2151e-05, PNorm = 86.5420, GNorm = 0.0565, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.253175\n",
      "Epoch 45\n",
      "Loss = 6.9659e-05, PNorm = 86.6685, GNorm = 0.0615, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.230313\n",
      "Epoch 46\n",
      "Loss = 6.5879e-05, PNorm = 86.7834, GNorm = 0.0479, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.220227\n",
      "Epoch 47\n",
      "Loss = 6.4545e-05, PNorm = 86.8925, GNorm = 0.0486, lr_0 = 1.1006e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 2.201949\n",
      "Epoch 48\n",
      "Loss = 6.1872e-05, PNorm = 87.0003, GNorm = 0.0716, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.210195\n",
      "Epoch 49\n",
      "Loss = 5.9750e-05, PNorm = 87.0970, GNorm = 0.0466, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.259626\n",
      "Model 6 best validation mae = 2.201949 on epoch 47\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.5395e-05, PNorm = 86.7541, GNorm = 0.0446, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.186991\n",
      "SWAG spoch 1\n",
      "Loss = 5.4465e-05, PNorm = 86.6162, GNorm = 0.0351, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.182276\n",
      "SWAG spoch 2\n",
      "Loss = 5.4196e-05, PNorm = 86.4785, GNorm = 0.0644, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.178206\n",
      "SWAG spoch 3\n",
      "Loss = 5.4078e-05, PNorm = 86.3412, GNorm = 0.0431, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.175538\n",
      "SWAG spoch 4\n",
      "Loss = 5.4084e-05, PNorm = 86.2038, GNorm = 0.0282, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.173502\n",
      "SWAG spoch 5\n",
      "Loss = 5.4137e-05, PNorm = 86.0666, GNorm = 0.0592, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.170803\n",
      "SWAG spoch 6\n",
      "Loss = 5.4258e-05, PNorm = 85.9299, GNorm = 0.0339, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169444\n",
      "SWAG spoch 7\n",
      "Loss = 5.4408e-05, PNorm = 85.7935, GNorm = 0.0396, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.168600\n",
      "SWAG spoch 8\n",
      "Loss = 5.4585e-05, PNorm = 85.6570, GNorm = 0.0326, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.167409\n",
      "SWAG spoch 9\n",
      "Loss = 5.4791e-05, PNorm = 85.5209, GNorm = 0.0413, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.166968\n",
      "SWAG spoch 10\n",
      "Loss = 5.5042e-05, PNorm = 85.3852, GNorm = 0.0453, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.166561\n",
      "SWAG spoch 11\n",
      "Loss = 5.5304e-05, PNorm = 85.2500, GNorm = 0.0419, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.165615\n",
      "SWAG spoch 12\n",
      "Loss = 5.5575e-05, PNorm = 85.1144, GNorm = 0.0461, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.165793\n",
      "SWAG spoch 13\n",
      "Loss = 5.5886e-05, PNorm = 84.9792, GNorm = 0.0428, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.165864\n",
      "SWAG spoch 14\n",
      "Loss = 5.6201e-05, PNorm = 84.8444, GNorm = 0.0623, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.165792\n",
      "SWAG spoch 15\n",
      "Loss = 5.6555e-05, PNorm = 84.7098, GNorm = 0.0439, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.166597\n",
      "SWAG spoch 16\n",
      "Loss = 5.6926e-05, PNorm = 84.5754, GNorm = 0.0393, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.166940\n",
      "SWAG spoch 17\n",
      "Loss = 5.7289e-05, PNorm = 84.4411, GNorm = 0.0512, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.167968\n",
      "SWAG spoch 18\n",
      "Loss = 5.7682e-05, PNorm = 84.3071, GNorm = 0.0400, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.167776\n",
      "SWAG spoch 19\n",
      "Loss = 5.8107e-05, PNorm = 84.1735, GNorm = 0.0435, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.168868\n",
      "SWAG spoch 20\n",
      "Loss = 5.8495e-05, PNorm = 84.0404, GNorm = 0.0545, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169780\n",
      "SWAG spoch 21\n",
      "Loss = 5.8982e-05, PNorm = 83.9072, GNorm = 0.0606, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169818\n",
      "SWAG spoch 22\n",
      "Loss = 5.9400e-05, PNorm = 83.7741, GNorm = 0.0377, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.170879\n",
      "SWAG spoch 23\n",
      "Loss = 5.9854e-05, PNorm = 83.6413, GNorm = 0.0354, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.171851\n",
      "SWAG spoch 24\n",
      "Loss = 6.0323e-05, PNorm = 83.5087, GNorm = 0.0424, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.172691\n",
      "Model 6, sample 0 test mae = 2.313810\n",
      "Model 6, sample 1 test mae = 2.281866\n",
      "Model 6, sample 2 test mae = 2.277633\n",
      "Model 6, sample 3 test mae = 2.284683\n",
      "Model 6, sample 4 test mae = 2.322010\n",
      "Model 6, sample 5 test mae = 2.279698\n",
      "Model 6, sample 6 test mae = 2.268231\n",
      "Model 6, sample 7 test mae = 2.284352\n",
      "Model 6, sample 8 test mae = 2.298062\n",
      "Model 6, sample 9 test mae = 2.273475\n",
      "Model 6, sample 10 test mae = 2.264283\n",
      "Model 6, sample 11 test mae = 2.285736\n",
      "Model 6, sample 12 test mae = 2.265390\n",
      "Model 6, sample 13 test mae = 2.278690\n",
      "Model 6, sample 14 test mae = 2.272513\n",
      "Model 6, sample 15 test mae = 2.269376\n",
      "Model 6, sample 16 test mae = 2.273268\n",
      "Model 6, sample 17 test mae = 2.294033\n",
      "Model 6, sample 18 test mae = 2.276816\n",
      "Model 6, sample 19 test mae = 2.316837\n",
      "Model 6, sample 20 test mae = 2.270570\n",
      "Model 6, sample 21 test mae = 2.269645\n",
      "Model 6, sample 22 test mae = 2.270331\n",
      "Model 6, sample 23 test mae = 2.286263\n",
      "Model 6, sample 24 test mae = 2.272023\n",
      "Model 6, sample 25 test mae = 2.296255\n",
      "Model 6, sample 26 test mae = 2.284469\n",
      "Model 6, sample 27 test mae = 2.271032\n",
      "Model 6, sample 28 test mae = 2.272438\n",
      "Model 6, sample 29 test mae = 2.276792\n",
      "Building model 7\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7765e-03, PNorm = 49.3917, GNorm = 1.4554, lr_0 = 5.5056e-04\n",
      "Validation mae = 14.046230\n",
      "Epoch 1\n",
      "Loss = 3.2892e-03, PNorm = 51.7592, GNorm = 1.1795, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.037516\n",
      "Epoch 2\n",
      "Loss = 2.1122e-03, PNorm = 54.2409, GNorm = 1.6012, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.212564\n",
      "Epoch 3\n",
      "Loss = 1.5430e-03, PNorm = 56.0534, GNorm = 0.8508, lr_0 = 9.0846e-04\n",
      "Validation mae = 6.222723\n",
      "Epoch 4\n",
      "Loss = 1.3157e-03, PNorm = 57.7901, GNorm = 0.4349, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.169858\n",
      "Epoch 5\n",
      "Loss = 1.2057e-03, PNorm = 59.6112, GNorm = 0.4707, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.363017\n",
      "Epoch 6\n",
      "Loss = 1.0529e-03, PNorm = 61.3177, GNorm = 0.2709, lr_0 = 7.8670e-04\n",
      "Validation mae = 5.233212\n",
      "Epoch 7\n",
      "Loss = 9.3388e-04, PNorm = 62.9138, GNorm = 0.4018, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.446819\n",
      "Epoch 8\n",
      "Loss = 8.4340e-04, PNorm = 64.5335, GNorm = 0.2545, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.313264\n",
      "Epoch 9\n",
      "Loss = 8.3858e-04, PNorm = 66.2166, GNorm = 0.2160, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.080120\n",
      "Epoch 10\n",
      "Loss = 7.1040e-04, PNorm = 67.6152, GNorm = 0.1372, lr_0 = 6.4934e-04\n",
      "Validation mae = 3.456999\n",
      "Epoch 11\n",
      "Loss = 6.4819e-04, PNorm = 68.9778, GNorm = 0.1897, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.642716\n",
      "Epoch 12\n",
      "Loss = 6.0529e-04, PNorm = 70.2953, GNorm = 0.7207, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.786128\n",
      "Epoch 13\n",
      "Loss = 5.8330e-04, PNorm = 71.6660, GNorm = 0.3222, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.238352\n",
      "Epoch 14\n",
      "Loss = 4.8336e-04, PNorm = 72.6487, GNorm = 0.2218, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.175864\n",
      "Epoch 15\n",
      "Loss = 4.8563e-04, PNorm = 73.9239, GNorm = 0.2750, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.944227\n",
      "Epoch 16\n",
      "Loss = 4.6494e-04, PNorm = 74.9782, GNorm = 0.2414, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.157084\n",
      "Epoch 17\n",
      "Loss = 3.9963e-04, PNorm = 75.9254, GNorm = 0.1181, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.150139\n",
      "Epoch 18\n",
      "Loss = 3.7972e-04, PNorm = 76.7756, GNorm = 0.2958, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.957087\n",
      "Epoch 19\n",
      "Loss = 3.6225e-04, PNorm = 77.6188, GNorm = 0.1374, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.809641\n",
      "Epoch 20\n",
      "Loss = 3.0729e-04, PNorm = 78.2678, GNorm = 0.1446, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.853699\n",
      "Epoch 21\n",
      "Loss = 2.8928e-04, PNorm = 78.9597, GNorm = 0.1129, lr_0 = 3.8310e-04\n",
      "Validation mae = 3.244453\n",
      "Epoch 22\n",
      "Loss = 2.7481e-04, PNorm = 79.6344, GNorm = 0.1365, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.583037\n",
      "Epoch 23\n",
      "Loss = 2.4976e-04, PNorm = 80.2368, GNorm = 0.1118, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.553029\n",
      "Epoch 24\n",
      "Loss = 2.2821e-04, PNorm = 80.8058, GNorm = 0.2130, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.517242\n",
      "Epoch 25\n",
      "Loss = 2.2016e-04, PNorm = 81.3525, GNorm = 0.1150, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.498679\n",
      "Epoch 26\n",
      "Loss = 2.0436e-04, PNorm = 81.8498, GNorm = 0.1183, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.620347\n",
      "Epoch 27\n",
      "Loss = 1.8679e-04, PNorm = 82.2874, GNorm = 0.0956, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.469805\n",
      "Epoch 28\n",
      "Loss = 1.7602e-04, PNorm = 82.6964, GNorm = 0.1030, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.556167\n",
      "Epoch 29\n",
      "Loss = 1.6898e-04, PNorm = 83.1021, GNorm = 0.0922, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.365411\n",
      "Epoch 30\n",
      "Loss = 1.4901e-04, PNorm = 83.4424, GNorm = 0.0957, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.419902\n",
      "Epoch 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1.4436e-04, PNorm = 83.7784, GNorm = 0.1164, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.365630\n",
      "Epoch 32\n",
      "Loss = 1.4007e-04, PNorm = 84.1282, GNorm = 0.0814, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.379583\n",
      "Epoch 33\n",
      "Loss = 1.2511e-04, PNorm = 84.3855, GNorm = 0.0671, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.438041\n",
      "Epoch 34\n",
      "Loss = 1.1294e-04, PNorm = 84.6184, GNorm = 0.0771, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.263824\n",
      "Epoch 35\n",
      "Loss = 1.0660e-04, PNorm = 84.8467, GNorm = 0.0962, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.303209\n",
      "Epoch 36\n",
      "Loss = 1.0357e-04, PNorm = 85.0828, GNorm = 0.0913, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.287324\n",
      "Epoch 37\n",
      "Loss = 9.9401e-05, PNorm = 85.2989, GNorm = 0.0711, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.323317\n",
      "Epoch 38\n",
      "Loss = 9.3249e-05, PNorm = 85.4841, GNorm = 0.1064, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.287220\n",
      "Epoch 39\n",
      "Loss = 8.6566e-05, PNorm = 85.6530, GNorm = 0.0567, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.292515\n",
      "Epoch 40\n",
      "Loss = 8.8755e-05, PNorm = 85.8408, GNorm = 0.0518, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.239694\n",
      "Epoch 41\n",
      "Loss = 8.2649e-05, PNorm = 86.0009, GNorm = 0.0780, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.314644\n",
      "Epoch 42\n",
      "Loss = 7.8298e-05, PNorm = 86.1445, GNorm = 0.0586, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.218625\n",
      "Epoch 43\n",
      "Loss = 7.2380e-05, PNorm = 86.2760, GNorm = 0.0437, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.213144\n",
      "Epoch 44\n",
      "Loss = 7.0623e-05, PNorm = 86.3996, GNorm = 0.0434, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.200294\n",
      "Epoch 45\n",
      "Loss = 6.7347e-05, PNorm = 86.5154, GNorm = 0.0578, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.242277\n",
      "Epoch 46\n",
      "Loss = 6.5365e-05, PNorm = 86.6338, GNorm = 0.0411, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.194739\n",
      "Epoch 47\n",
      "Loss = 6.3408e-05, PNorm = 86.7423, GNorm = 0.0477, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.172897\n",
      "Epoch 48\n",
      "Loss = 6.0870e-05, PNorm = 86.8409, GNorm = 0.0510, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.208255\n",
      "Epoch 49\n",
      "Loss = 5.8961e-05, PNorm = 86.9310, GNorm = 0.0775, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.226351\n",
      "Model 7 best validation mae = 2.172897 on epoch 47\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.4104e-05, PNorm = 86.6039, GNorm = 0.0399, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.161316\n",
      "SWAG spoch 1\n",
      "Loss = 5.3238e-05, PNorm = 86.4659, GNorm = 0.0383, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.159109\n",
      "SWAG spoch 2\n",
      "Loss = 5.2995e-05, PNorm = 86.3284, GNorm = 0.0325, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.157803\n",
      "SWAG spoch 3\n",
      "Loss = 5.2931e-05, PNorm = 86.1910, GNorm = 0.0363, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.156022\n",
      "SWAG spoch 4\n",
      "Loss = 5.2906e-05, PNorm = 86.0539, GNorm = 0.0363, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.155280\n",
      "SWAG spoch 5\n",
      "Loss = 5.2973e-05, PNorm = 85.9169, GNorm = 0.0293, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.153991\n",
      "SWAG spoch 6\n",
      "Loss = 5.3056e-05, PNorm = 85.7802, GNorm = 0.0369, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.154037\n",
      "SWAG spoch 7\n",
      "Loss = 5.3187e-05, PNorm = 85.6438, GNorm = 0.0446, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.154305\n",
      "SWAG spoch 8\n",
      "Loss = 5.3378e-05, PNorm = 85.5079, GNorm = 0.0325, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.154423\n",
      "SWAG spoch 9\n",
      "Loss = 5.3584e-05, PNorm = 85.3721, GNorm = 0.0298, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.154616\n",
      "SWAG spoch 10\n",
      "Loss = 5.3841e-05, PNorm = 85.2362, GNorm = 0.0265, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.154351\n",
      "SWAG spoch 11\n",
      "Loss = 5.4090e-05, PNorm = 85.1009, GNorm = 0.0334, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.155115\n",
      "SWAG spoch 12\n",
      "Loss = 5.4383e-05, PNorm = 84.9656, GNorm = 0.0310, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.155654\n",
      "SWAG spoch 13\n",
      "Loss = 5.4703e-05, PNorm = 84.8309, GNorm = 0.0669, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.156079\n",
      "SWAG spoch 14\n",
      "Loss = 5.5021e-05, PNorm = 84.6964, GNorm = 0.0324, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.156633\n",
      "SWAG spoch 15\n",
      "Loss = 5.5361e-05, PNorm = 84.5617, GNorm = 0.0404, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158086\n",
      "SWAG spoch 16\n",
      "Loss = 5.5721e-05, PNorm = 84.4276, GNorm = 0.0362, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158930\n",
      "SWAG spoch 17\n",
      "Loss = 5.6124e-05, PNorm = 84.2935, GNorm = 0.0402, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.159694\n",
      "SWAG spoch 18\n",
      "Loss = 5.6513e-05, PNorm = 84.1600, GNorm = 0.0370, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.160877\n",
      "SWAG spoch 19\n",
      "Loss = 5.6932e-05, PNorm = 84.0262, GNorm = 0.0407, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.161666\n",
      "SWAG spoch 20\n",
      "Loss = 5.7349e-05, PNorm = 83.8932, GNorm = 0.0428, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.163028\n",
      "SWAG spoch 21\n",
      "Loss = 5.7802e-05, PNorm = 83.7601, GNorm = 0.0378, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.163892\n",
      "SWAG spoch 22\n",
      "Loss = 5.8259e-05, PNorm = 83.6272, GNorm = 0.0366, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.164912\n",
      "SWAG spoch 23\n",
      "Loss = 5.8703e-05, PNorm = 83.4950, GNorm = 0.0318, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.166073\n",
      "SWAG spoch 24\n",
      "Loss = 5.9186e-05, PNorm = 83.3625, GNorm = 0.0490, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.167248\n",
      "Model 7, sample 0 test mae = 2.285899\n",
      "Model 7, sample 1 test mae = 2.278915\n",
      "Model 7, sample 2 test mae = 2.266002\n",
      "Model 7, sample 3 test mae = 2.279287\n",
      "Model 7, sample 4 test mae = 2.281055\n",
      "Model 7, sample 5 test mae = 2.262108\n",
      "Model 7, sample 6 test mae = 2.260857\n",
      "Model 7, sample 7 test mae = 2.249777\n",
      "Model 7, sample 8 test mae = 2.253007\n",
      "Model 7, sample 9 test mae = 2.246385\n",
      "Model 7, sample 10 test mae = 2.257734\n",
      "Model 7, sample 11 test mae = 2.259782\n",
      "Model 7, sample 12 test mae = 2.275201\n",
      "Model 7, sample 13 test mae = 2.266715\n",
      "Model 7, sample 14 test mae = 2.254597\n",
      "Model 7, sample 15 test mae = 2.253722\n",
      "Model 7, sample 16 test mae = 2.263647\n",
      "Model 7, sample 17 test mae = 2.291808\n",
      "Model 7, sample 18 test mae = 2.255309\n",
      "Model 7, sample 19 test mae = 2.253926\n",
      "Model 7, sample 20 test mae = 2.258074\n",
      "Model 7, sample 21 test mae = 2.259371\n",
      "Model 7, sample 22 test mae = 2.258944\n",
      "Model 7, sample 23 test mae = 2.253176\n",
      "Model 7, sample 24 test mae = 2.264742\n",
      "Model 7, sample 25 test mae = 2.269015\n",
      "Model 7, sample 26 test mae = 2.303593\n",
      "Model 7, sample 27 test mae = 2.256935\n",
      "Model 7, sample 28 test mae = 2.249843\n",
      "Model 7, sample 29 test mae = 2.254450\n",
      "Building model 8\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7708e-03, PNorm = 49.2908, GNorm = 3.0992, lr_0 = 5.5056e-04\n",
      "Validation mae = 13.949302\n",
      "Epoch 1\n",
      "Loss = 3.5018e-03, PNorm = 51.5470, GNorm = 1.4389, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.991374\n",
      "Epoch 2\n",
      "Loss = 2.2424e-03, PNorm = 53.9342, GNorm = 0.6042, lr_0 = 9.5310e-04\n",
      "Validation mae = 8.423733\n",
      "Epoch 3\n",
      "Loss = 1.5925e-03, PNorm = 55.7515, GNorm = 1.0117, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.985270\n",
      "Epoch 4\n",
      "Loss = 1.3357e-03, PNorm = 57.5079, GNorm = 0.5978, lr_0 = 8.6591e-04\n",
      "Validation mae = 6.221075\n",
      "Epoch 5\n",
      "Loss = 1.1963e-03, PNorm = 59.3119, GNorm = 0.4314, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.390076\n",
      "Epoch 6\n",
      "Loss = 1.1136e-03, PNorm = 61.2049, GNorm = 0.4823, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.556253\n",
      "Epoch 7\n",
      "Loss = 9.6238e-04, PNorm = 62.7156, GNorm = 0.6023, lr_0 = 7.4985e-04\n",
      "Validation mae = 5.353748\n",
      "Epoch 8\n",
      "Loss = 8.9964e-04, PNorm = 64.4653, GNorm = 0.3114, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.193877\n",
      "Epoch 9\n",
      "Loss = 8.1629e-04, PNorm = 66.0938, GNorm = 0.5701, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.314560\n",
      "Epoch 10\n",
      "Loss = 7.1806e-04, PNorm = 67.5648, GNorm = 0.3813, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.059407\n",
      "Epoch 11\n",
      "Loss = 6.4950e-04, PNorm = 68.9970, GNorm = 0.3770, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.990806\n",
      "Epoch 12\n",
      "Loss = 6.1424e-04, PNorm = 70.3563, GNorm = 0.1864, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.860361\n",
      "Epoch 13\n",
      "Loss = 5.6477e-04, PNorm = 71.7307, GNorm = 0.3224, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.497951\n",
      "Epoch 14\n",
      "Loss = 5.1380e-04, PNorm = 72.9913, GNorm = 0.4260, lr_0 = 5.3597e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 4.278009\n",
      "Epoch 15\n",
      "Loss = 4.6900e-04, PNorm = 74.0793, GNorm = 0.1828, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.521656\n",
      "Epoch 16\n",
      "Loss = 4.4647e-04, PNorm = 75.1851, GNorm = 0.1861, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.031135\n",
      "Epoch 17\n",
      "Loss = 4.0134e-04, PNorm = 76.1353, GNorm = 0.1530, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.179458\n",
      "Epoch 18\n",
      "Loss = 3.7819e-04, PNorm = 77.0057, GNorm = 0.1488, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.976755\n",
      "Epoch 19\n",
      "Loss = 3.3598e-04, PNorm = 77.7894, GNorm = 0.1773, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.056293\n",
      "Epoch 20\n",
      "Loss = 3.3423e-04, PNorm = 78.6628, GNorm = 0.1405, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.998115\n",
      "Epoch 21\n",
      "Loss = 2.9802e-04, PNorm = 79.3672, GNorm = 0.0897, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.685602\n",
      "Epoch 22\n",
      "Loss = 2.6328e-04, PNorm = 79.9817, GNorm = 0.1191, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.642038\n",
      "Epoch 23\n",
      "Loss = 2.4530e-04, PNorm = 80.5829, GNorm = 0.1545, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.706590\n",
      "Epoch 24\n",
      "Loss = 2.4460e-04, PNorm = 81.2333, GNorm = 0.1251, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.680920\n",
      "Epoch 25\n",
      "Loss = 2.4407e-04, PNorm = 81.8748, GNorm = 0.1117, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.600624\n",
      "Epoch 26\n",
      "Loss = 1.9624e-04, PNorm = 82.2739, GNorm = 0.1253, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.485404\n",
      "Epoch 27\n",
      "Loss = 1.9078e-04, PNorm = 82.7223, GNorm = 0.1049, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.407154\n",
      "Epoch 28\n",
      "Loss = 1.7437e-04, PNorm = 83.1087, GNorm = 0.1580, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.451065\n",
      "Epoch 29\n",
      "Loss = 1.6320e-04, PNorm = 83.4845, GNorm = 0.0715, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.485202\n",
      "Epoch 30\n",
      "Loss = 1.5524e-04, PNorm = 83.8449, GNorm = 0.0960, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.453931\n",
      "Epoch 31\n",
      "Loss = 1.4733e-04, PNorm = 84.2168, GNorm = 0.0839, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.435914\n",
      "Epoch 32\n",
      "Loss = 1.3699e-04, PNorm = 84.5353, GNorm = 0.0904, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.395137\n",
      "Epoch 33\n",
      "Loss = 1.2588e-04, PNorm = 84.8175, GNorm = 0.0957, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.346560\n",
      "Epoch 34\n",
      "Loss = 1.2196e-04, PNorm = 85.1002, GNorm = 0.0638, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.346124\n",
      "Epoch 35\n",
      "Loss = 1.1137e-04, PNorm = 85.3471, GNorm = 0.0633, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.300116\n",
      "Epoch 36\n",
      "Loss = 1.0622e-04, PNorm = 85.5750, GNorm = 0.1218, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.375249\n",
      "Epoch 37\n",
      "Loss = 1.0054e-04, PNorm = 85.7914, GNorm = 0.0763, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.313761\n",
      "Epoch 38\n",
      "Loss = 9.3383e-05, PNorm = 85.9918, GNorm = 0.0934, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.311263\n",
      "Epoch 39\n",
      "Loss = 9.0525e-05, PNorm = 86.1853, GNorm = 0.0667, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.282046\n",
      "Epoch 40\n",
      "Loss = 8.5710e-05, PNorm = 86.3606, GNorm = 0.0610, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.278163\n",
      "Epoch 41\n",
      "Loss = 8.1451e-05, PNorm = 86.5211, GNorm = 0.0564, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.260706\n",
      "Epoch 42\n",
      "Loss = 7.8104e-05, PNorm = 86.6709, GNorm = 0.0506, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.229688\n",
      "Epoch 43\n",
      "Loss = 7.3422e-05, PNorm = 86.8072, GNorm = 0.0436, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.239193\n",
      "Epoch 44\n",
      "Loss = 7.0532e-05, PNorm = 86.9352, GNorm = 0.0751, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.245331\n",
      "Epoch 45\n",
      "Loss = 6.8410e-05, PNorm = 87.0661, GNorm = 0.0553, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.209298\n",
      "Epoch 46\n",
      "Loss = 6.6074e-05, PNorm = 87.1812, GNorm = 0.0630, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.207786\n",
      "Epoch 47\n",
      "Loss = 6.3396e-05, PNorm = 87.2938, GNorm = 0.0545, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.227947\n",
      "Epoch 48\n",
      "Loss = 6.2238e-05, PNorm = 87.3967, GNorm = 0.0466, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.202468\n",
      "Epoch 49\n",
      "Loss = 5.9332e-05, PNorm = 87.4907, GNorm = 0.0441, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.192239\n",
      "Model 8 best validation mae = 2.192239 on epoch 49\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.1720e-05, PNorm = 87.3512, GNorm = 0.0318, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.183972\n",
      "SWAG spoch 1\n",
      "Loss = 5.1221e-05, PNorm = 87.2122, GNorm = 0.0480, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.180638\n",
      "SWAG spoch 2\n",
      "Loss = 5.1006e-05, PNorm = 87.0734, GNorm = 0.0316, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.178204\n",
      "SWAG spoch 3\n",
      "Loss = 5.0923e-05, PNorm = 86.9349, GNorm = 0.0285, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.176482\n",
      "SWAG spoch 4\n",
      "Loss = 5.0936e-05, PNorm = 86.7966, GNorm = 0.0302, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.174921\n",
      "SWAG spoch 5\n",
      "Loss = 5.1008e-05, PNorm = 86.6585, GNorm = 0.0286, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.173415\n",
      "SWAG spoch 6\n",
      "Loss = 5.1121e-05, PNorm = 86.5209, GNorm = 0.0346, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.172422\n",
      "SWAG spoch 7\n",
      "Loss = 5.1281e-05, PNorm = 86.3833, GNorm = 0.0352, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.171370\n",
      "SWAG spoch 8\n",
      "Loss = 5.1469e-05, PNorm = 86.2460, GNorm = 0.0431, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.171171\n",
      "SWAG spoch 9\n",
      "Loss = 5.1714e-05, PNorm = 86.1089, GNorm = 0.0284, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.170263\n",
      "SWAG spoch 10\n",
      "Loss = 5.1977e-05, PNorm = 85.9724, GNorm = 0.0279, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169638\n",
      "SWAG spoch 11\n",
      "Loss = 5.2230e-05, PNorm = 85.8359, GNorm = 0.0436, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169581\n",
      "SWAG spoch 12\n",
      "Loss = 5.2543e-05, PNorm = 85.6994, GNorm = 0.0429, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169280\n",
      "SWAG spoch 13\n",
      "Loss = 5.2847e-05, PNorm = 85.5634, GNorm = 0.0397, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169110\n",
      "SWAG spoch 14\n",
      "Loss = 5.3194e-05, PNorm = 85.4276, GNorm = 0.0319, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.168920\n",
      "SWAG spoch 15\n",
      "Loss = 5.3535e-05, PNorm = 85.2922, GNorm = 0.0310, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169254\n",
      "SWAG spoch 16\n",
      "Loss = 5.3923e-05, PNorm = 85.1566, GNorm = 0.1518, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169190\n",
      "SWAG spoch 17\n",
      "Loss = 5.4309e-05, PNorm = 85.0218, GNorm = 0.0322, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169254\n",
      "SWAG spoch 18\n",
      "Loss = 5.4688e-05, PNorm = 84.8869, GNorm = 0.0364, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169601\n",
      "SWAG spoch 19\n",
      "Loss = 5.5094e-05, PNorm = 84.7523, GNorm = 0.0516, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.169844\n",
      "SWAG spoch 20\n",
      "Loss = 5.5535e-05, PNorm = 84.6181, GNorm = 0.0377, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.170289\n",
      "SWAG spoch 21\n",
      "Loss = 5.5972e-05, PNorm = 84.4839, GNorm = 0.0321, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.170799\n",
      "SWAG spoch 22\n",
      "Loss = 5.6430e-05, PNorm = 84.3500, GNorm = 0.0417, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.171480\n",
      "SWAG spoch 23\n",
      "Loss = 5.6882e-05, PNorm = 84.2163, GNorm = 0.0377, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.172425\n",
      "SWAG spoch 24\n",
      "Loss = 5.7401e-05, PNorm = 84.0828, GNorm = 0.0467, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.172715\n",
      "Model 8, sample 0 test mae = 2.250446\n",
      "Model 8, sample 1 test mae = 2.248931\n",
      "Model 8, sample 2 test mae = 2.268651\n",
      "Model 8, sample 3 test mae = 2.251520\n",
      "Model 8, sample 4 test mae = 2.253055\n",
      "Model 8, sample 5 test mae = 2.258576\n",
      "Model 8, sample 6 test mae = 2.264577\n",
      "Model 8, sample 7 test mae = 2.257779\n",
      "Model 8, sample 8 test mae = 2.258995\n",
      "Model 8, sample 9 test mae = 2.261282\n",
      "Model 8, sample 10 test mae = 2.286986\n",
      "Model 8, sample 11 test mae = 2.291464\n",
      "Model 8, sample 12 test mae = 2.251176\n",
      "Model 8, sample 13 test mae = 2.258900\n",
      "Model 8, sample 14 test mae = 2.265445\n",
      "Model 8, sample 15 test mae = 2.268403\n",
      "Model 8, sample 16 test mae = 2.252751\n",
      "Model 8, sample 17 test mae = 2.259753\n",
      "Model 8, sample 18 test mae = 2.264273\n",
      "Model 8, sample 19 test mae = 2.260972\n",
      "Model 8, sample 20 test mae = 2.258962\n",
      "Model 8, sample 21 test mae = 2.258294\n",
      "Model 8, sample 22 test mae = 2.265690\n",
      "Model 8, sample 23 test mae = 2.248015\n",
      "Model 8, sample 24 test mae = 2.250356\n",
      "Model 8, sample 25 test mae = 2.259915\n",
      "Model 8, sample 26 test mae = 2.269274\n",
      "Model 8, sample 27 test mae = 2.266727\n",
      "Model 8, sample 28 test mae = 2.254432\n",
      "Model 8, sample 29 test mae = 2.256902\n",
      "Building model 9\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 6.7903e-03, PNorm = 49.4287, GNorm = 2.5941, lr_0 = 5.5056e-04\n",
      "Validation mae = 11.660155\n",
      "Epoch 1\n",
      "Loss = 3.5380e-03, PNorm = 51.8989, GNorm = 0.6957, lr_0 = 9.9994e-04\n",
      "Validation mae = 10.461161\n",
      "Epoch 2\n",
      "Loss = 2.2145e-03, PNorm = 54.4673, GNorm = 0.8444, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.192851\n",
      "Epoch 3\n",
      "Loss = 1.5791e-03, PNorm = 56.3921, GNorm = 0.7354, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.862951\n",
      "Epoch 4\n",
      "Loss = 1.3517e-03, PNorm = 58.2182, GNorm = 0.6728, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.079397\n",
      "Epoch 5\n",
      "Loss = 1.1813e-03, PNorm = 59.9781, GNorm = 0.4320, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.042920\n",
      "Epoch 6\n",
      "Loss = 1.0241e-03, PNorm = 61.5920, GNorm = 0.7003, lr_0 = 7.8670e-04\n",
      "Validation mae = 5.369423\n",
      "Epoch 7\n",
      "Loss = 9.6873e-04, PNorm = 63.3511, GNorm = 0.4516, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.403677\n",
      "Epoch 8\n",
      "Loss = 8.7348e-04, PNorm = 65.0385, GNorm = 0.3686, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.298822\n",
      "Epoch 9\n",
      "Loss = 7.9075e-04, PNorm = 66.5866, GNorm = 0.6391, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.466157\n",
      "Epoch 10\n",
      "Loss = 7.2740e-04, PNorm = 68.0375, GNorm = 0.2470, lr_0 = 6.4934e-04\n",
      "Validation mae = 3.700726\n",
      "Epoch 11\n",
      "Loss = 6.5785e-04, PNorm = 69.4861, GNorm = 0.4329, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.844874\n",
      "Epoch 12\n",
      "Loss = 6.3600e-04, PNorm = 70.9226, GNorm = 0.1712, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.426395\n",
      "Epoch 13\n",
      "Loss = 5.6002e-04, PNorm = 72.1693, GNorm = 0.2587, lr_0 = 5.6231e-04\n",
      "Validation mae = 4.155719\n",
      "Epoch 14\n",
      "Loss = 5.3797e-04, PNorm = 73.4441, GNorm = 0.3633, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.547180\n",
      "Epoch 15\n",
      "Loss = 4.6901e-04, PNorm = 74.4485, GNorm = 0.4377, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.277084\n",
      "Epoch 16\n",
      "Loss = 4.6451e-04, PNorm = 75.6165, GNorm = 0.2101, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.219040\n",
      "Epoch 17\n",
      "Loss = 3.9201e-04, PNorm = 76.4710, GNorm = 0.1184, lr_0 = 4.6413e-04\n",
      "Validation mae = 2.825391\n",
      "Epoch 18\n",
      "Loss = 3.9067e-04, PNorm = 77.4621, GNorm = 0.2657, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.982374\n",
      "Epoch 19\n",
      "Loss = 3.5505e-04, PNorm = 78.2708, GNorm = 0.1566, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.150072\n",
      "Epoch 20\n",
      "Loss = 3.4738e-04, PNorm = 79.0789, GNorm = 0.1142, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.713765\n",
      "Epoch 21\n",
      "Loss = 2.9360e-04, PNorm = 79.7302, GNorm = 0.2051, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.737147\n",
      "Epoch 22\n",
      "Loss = 2.9158e-04, PNorm = 80.4407, GNorm = 0.2199, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.848152\n",
      "Epoch 23\n",
      "Loss = 2.5668e-04, PNorm = 81.0093, GNorm = 0.2144, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.705440\n",
      "Epoch 24\n",
      "Loss = 2.3573e-04, PNorm = 81.5632, GNorm = 0.3022, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.629921\n",
      "Epoch 25\n",
      "Loss = 2.2273e-04, PNorm = 82.0771, GNorm = 0.1073, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.658355\n",
      "Epoch 26\n",
      "Loss = 2.0909e-04, PNorm = 82.5901, GNorm = 0.1194, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.608380\n",
      "Epoch 27\n",
      "Loss = 1.8856e-04, PNorm = 83.0383, GNorm = 0.2072, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.569816\n",
      "Epoch 28\n",
      "Loss = 1.8688e-04, PNorm = 83.5092, GNorm = 0.0902, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.420597\n",
      "Epoch 29\n",
      "Loss = 1.6900e-04, PNorm = 83.8951, GNorm = 0.1194, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.626605\n",
      "Epoch 30\n",
      "Loss = 1.6315e-04, PNorm = 84.2601, GNorm = 0.0844, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.450972\n",
      "Epoch 31\n",
      "Loss = 1.4956e-04, PNorm = 84.5938, GNorm = 0.1098, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.828274\n",
      "Epoch 32\n",
      "Loss = 1.5192e-04, PNorm = 84.9321, GNorm = 0.0989, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.482635\n",
      "Epoch 33\n",
      "Loss = 1.3145e-04, PNorm = 85.2113, GNorm = 0.0889, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.286268\n",
      "Epoch 34\n",
      "Loss = 1.2358e-04, PNorm = 85.4694, GNorm = 0.0982, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.363257\n",
      "Epoch 35\n",
      "Loss = 1.2010e-04, PNorm = 85.7221, GNorm = 0.0865, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.392584\n",
      "Epoch 36\n",
      "Loss = 1.1353e-04, PNorm = 85.9515, GNorm = 0.0644, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.302557\n",
      "Epoch 37\n",
      "Loss = 1.0790e-04, PNorm = 86.1699, GNorm = 0.0640, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.337923\n",
      "Epoch 38\n",
      "Loss = 1.0273e-04, PNorm = 86.3819, GNorm = 0.0722, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.217064\n",
      "Epoch 39\n",
      "Loss = 9.7992e-05, PNorm = 86.5636, GNorm = 0.0774, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.264985\n",
      "Epoch 40\n",
      "Loss = 9.3069e-05, PNorm = 86.7503, GNorm = 0.0480, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.271683\n",
      "Epoch 41\n",
      "Loss = 9.0690e-05, PNorm = 86.9164, GNorm = 0.0721, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.204153\n",
      "Epoch 42\n",
      "Loss = 8.4226e-05, PNorm = 87.0681, GNorm = 0.0912, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.202433\n",
      "Epoch 43\n",
      "Loss = 8.1567e-05, PNorm = 87.2187, GNorm = 0.0685, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.222839\n",
      "Epoch 44\n",
      "Loss = 7.8145e-05, PNorm = 87.3520, GNorm = 0.0776, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.201590\n",
      "Epoch 45\n",
      "Loss = 7.5748e-05, PNorm = 87.4879, GNorm = 0.0463, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.270289\n",
      "Epoch 46\n",
      "Loss = 7.4075e-05, PNorm = 87.6047, GNorm = 0.0539, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.179839\n",
      "Epoch 47\n",
      "Loss = 6.8756e-05, PNorm = 87.7162, GNorm = 0.0406, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.184482\n",
      "Epoch 48\n",
      "Loss = 6.6084e-05, PNorm = 87.8225, GNorm = 0.0412, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.179681\n",
      "Epoch 49\n",
      "Loss = 6.3683e-05, PNorm = 87.9227, GNorm = 0.0518, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.186410\n",
      "Model 9 best validation mae = 2.179681 on epoch 48\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.7442e-05, PNorm = 87.6828, GNorm = 0.0373, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.158576\n",
      "SWAG spoch 1\n",
      "Loss = 5.6479e-05, PNorm = 87.5428, GNorm = 0.2162, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.154795\n",
      "SWAG spoch 2\n",
      "Loss = 5.6061e-05, PNorm = 87.4035, GNorm = 0.0334, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.151768\n",
      "SWAG spoch 3\n",
      "Loss = 5.5878e-05, PNorm = 87.2645, GNorm = 0.0332, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.150154\n",
      "SWAG spoch 4\n",
      "Loss = 5.5779e-05, PNorm = 87.1259, GNorm = 0.0393, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.147888\n",
      "SWAG spoch 5\n",
      "Loss = 5.5761e-05, PNorm = 86.9872, GNorm = 0.0298, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.147049\n",
      "SWAG spoch 6\n",
      "Loss = 5.5838e-05, PNorm = 86.8490, GNorm = 0.2300, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.145710\n",
      "SWAG spoch 7\n",
      "Loss = 5.5977e-05, PNorm = 86.7109, GNorm = 0.0362, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.144897\n",
      "SWAG spoch 8\n",
      "Loss = 5.6090e-05, PNorm = 86.5732, GNorm = 0.0405, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.144633\n",
      "SWAG spoch 9\n",
      "Loss = 5.6318e-05, PNorm = 86.4356, GNorm = 0.0490, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.144480\n",
      "SWAG spoch 10\n",
      "Loss = 5.6537e-05, PNorm = 86.2979, GNorm = 0.0324, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.144657\n",
      "SWAG spoch 11\n",
      "Loss = 5.6779e-05, PNorm = 86.1611, GNorm = 0.0372, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.144625\n",
      "SWAG spoch 12\n",
      "Loss = 5.7068e-05, PNorm = 86.0243, GNorm = 0.0341, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.145064\n",
      "SWAG spoch 13\n",
      "Loss = 5.7350e-05, PNorm = 85.8877, GNorm = 0.0432, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.145021\n",
      "SWAG spoch 14\n",
      "Loss = 5.7673e-05, PNorm = 85.7513, GNorm = 0.0450, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.145646\n",
      "SWAG spoch 15\n",
      "Loss = 5.8013e-05, PNorm = 85.6154, GNorm = 0.0616, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.146023\n",
      "SWAG spoch 16\n",
      "Loss = 5.8349e-05, PNorm = 85.4792, GNorm = 0.0408, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.147243\n",
      "SWAG spoch 17\n",
      "Loss = 5.8724e-05, PNorm = 85.3438, GNorm = 0.0540, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.147823\n",
      "SWAG spoch 18\n",
      "Loss = 5.9125e-05, PNorm = 85.2084, GNorm = 0.0704, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.148380\n",
      "SWAG spoch 19\n",
      "Loss = 5.9531e-05, PNorm = 85.0734, GNorm = 0.0315, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.149382\n",
      "SWAG spoch 20\n",
      "Loss = 5.9914e-05, PNorm = 84.9387, GNorm = 0.0718, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.149476\n",
      "SWAG spoch 21\n",
      "Loss = 6.0368e-05, PNorm = 84.8039, GNorm = 0.0302, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.151125\n",
      "SWAG spoch 22\n",
      "Loss = 6.0784e-05, PNorm = 84.6695, GNorm = 0.0537, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.152248\n",
      "SWAG spoch 23\n",
      "Loss = 6.1242e-05, PNorm = 84.5354, GNorm = 0.0289, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.152438\n",
      "SWAG spoch 24\n",
      "Loss = 6.1677e-05, PNorm = 84.4011, GNorm = 0.0355, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.154607\n",
      "Model 9, sample 0 test mae = 2.233449\n",
      "Model 9, sample 1 test mae = 2.223725\n",
      "Model 9, sample 2 test mae = 2.217570\n",
      "Model 9, sample 3 test mae = 2.205351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 9, sample 4 test mae = 2.208659\n",
      "Model 9, sample 5 test mae = 2.213815\n",
      "Model 9, sample 6 test mae = 2.209237\n",
      "Model 9, sample 7 test mae = 2.224004\n",
      "Model 9, sample 8 test mae = 2.237787\n",
      "Model 9, sample 9 test mae = 2.206944\n",
      "Model 9, sample 10 test mae = 2.229206\n",
      "Model 9, sample 11 test mae = 2.226884\n",
      "Model 9, sample 12 test mae = 2.208873\n",
      "Model 9, sample 13 test mae = 2.212614\n",
      "Model 9, sample 14 test mae = 2.213330\n",
      "Model 9, sample 15 test mae = 2.213504\n",
      "Model 9, sample 16 test mae = 2.217474\n",
      "Model 9, sample 17 test mae = 2.240712\n",
      "Model 9, sample 18 test mae = 2.223078\n",
      "Model 9, sample 19 test mae = 2.212541\n",
      "Model 9, sample 20 test mae = 2.219615\n",
      "Model 9, sample 21 test mae = 2.220876\n",
      "Model 9, sample 22 test mae = 2.226904\n",
      "Model 9, sample 23 test mae = 2.206670\n",
      "Model 9, sample 24 test mae = 2.254723\n",
      "Model 9, sample 25 test mae = 2.218153\n",
      "Model 9, sample 26 test mae = 2.204825\n",
      "Model 9, sample 27 test mae = 2.214759\n",
      "Model 9, sample 28 test mae = 2.247722\n",
      "Model 9, sample 29 test mae = 2.207730\n",
      "BMA test mae = 1.690673\n"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 10\n",
    "args.samples = 30\n",
    "args.dropout = 0\n",
    "args.test_dropout = False\n",
    "\n",
    "args.swag = True # SWAG switch\n",
    "args.cov_mat = True # whether to compute deviations and then covariance\n",
    "args.block = False # whether to compute covariances layer by layer\n",
    "args.max_num_models = 30 # max number of columns of deviations matrix\n",
    "\n",
    "args.epochs_swag = 25 # number of epochs\n",
    "args.c_swag = 200 # how frequently to collect a model (in batches)\n",
    "\n",
    "args.lr_swag = 1e-3\n",
    "args.wd_swag = 0.001\n",
    "args.momentum_swag = 0.5\n",
    "\n",
    "results_swagM = run_training(args)\n",
    "#np.savez(args.save_dir+'/results_swagM', results_swagM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-832232a1-7bee-48f8-801a-d6a9460319ca.json\n",
      "Args\n",
      "{'RMS': False,\n",
      " 'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'batch_size_sgld': 200,\n",
      " 'bias': False,\n",
      " 'block': True,\n",
      " 'burnin_epochs': 5,\n",
      " 'c_swag': 0,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0.0,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_swag': 0,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise': -2,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'log_frequency_sgld': 200,\n",
      " 'lr_sgld': 0.0001,\n",
      " 'lr_swag': 0.0001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 0,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'mix_epochs': 1,\n",
      " 'momentum_swag': 0.9,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': 12,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 20,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': True,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': False,\n",
      " 'target_columns': None,\n",
      " 'task_names': ['mu',\n",
      "                'alpha',\n",
      "                'homo',\n",
      "                'lumo',\n",
      "                'gap',\n",
      "                'r2',\n",
      "                'zpve',\n",
      "                'cv',\n",
      "                'u0',\n",
      "                'u298',\n",
      "                'h298',\n",
      "                'g298'],\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': 40000,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0,\n",
      " 'weight_decay_sgld': 0.1}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42730it [00:00, 74453.10it/s]\n",
      "100%|██████████| 50000/50000 [00:01<00:00, 26714.08it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 14011.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,524\n",
      "Model 0 best validation mae = inf on epoch 0\n",
      "----------SGLD training----------\n",
      "SGLD spoch 0\n",
      "Loss = -1.1616e-01, PNorm = 87.7865, GNorm = 11.6135, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.139044\n",
      "SGLD spoch 1\n",
      "Loss = -1.1711e-01, PNorm = 87.7081, GNorm = 11.3291, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.140181\n",
      "SGLD spoch 2\n",
      "Loss = -1.1800e-01, PNorm = 87.6289, GNorm = 11.0146, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.139686\n",
      "SGLD spoch 3\n",
      "Loss = -1.1887e-01, PNorm = 87.5517, GNorm = 10.8907, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.135874\n",
      "SGLD spoch 4\n",
      "Loss = -1.1972e-01, PNorm = 87.4741, GNorm = 11.1529, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.132881\n",
      "SGLD spoch 5\n",
      "Loss = -1.2056e-01, PNorm = 87.3959, GNorm = 15.1208, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.131860\n",
      "Collecting sample 0\n",
      "SGLD spoch 6\n",
      "Loss = -1.2138e-01, PNorm = 87.3180, GNorm = 11.0264, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.131269\n",
      "Collecting sample 1\n",
      "SGLD spoch 7\n",
      "Loss = -1.2219e-01, PNorm = 87.2403, GNorm = 11.6864, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.127288\n",
      "Collecting sample 2\n",
      "SGLD spoch 8\n",
      "Loss = -1.2299e-01, PNorm = 87.1628, GNorm = 13.3733, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.128873\n",
      "Collecting sample 3\n",
      "SGLD spoch 9\n",
      "Loss = -1.2377e-01, PNorm = 87.0848, GNorm = 22.0179, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.129527\n",
      "Collecting sample 4\n",
      "SGLD spoch 10\n",
      "Loss = -1.2453e-01, PNorm = 87.0081, GNorm = 11.8448, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.128999\n",
      "Collecting sample 5\n",
      "SGLD spoch 11\n",
      "Loss = -1.2529e-01, PNorm = 86.9307, GNorm = 12.1170, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.130142\n",
      "Collecting sample 6\n",
      "SGLD spoch 12\n",
      "Loss = -1.2603e-01, PNorm = 86.8526, GNorm = 13.2838, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.128649\n",
      "Collecting sample 7\n",
      "SGLD spoch 13\n",
      "Loss = -1.2676e-01, PNorm = 86.7746, GNorm = 14.0068, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.131869\n",
      "Collecting sample 8\n",
      "SGLD spoch 14\n",
      "Loss = -1.2748e-01, PNorm = 86.6980, GNorm = 12.9399, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.129168\n",
      "Collecting sample 9\n",
      "SGLD spoch 15\n",
      "Loss = -1.2818e-01, PNorm = 86.6210, GNorm = 13.2663, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.129511\n",
      "Collecting sample 10\n",
      "SGLD spoch 16\n",
      "Loss = -1.2887e-01, PNorm = 86.5450, GNorm = 13.7488, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.132142\n",
      "Collecting sample 11\n",
      "SGLD spoch 17\n",
      "Loss = -1.2955e-01, PNorm = 86.4678, GNorm = 14.3415, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.131351\n",
      "Collecting sample 12\n",
      "SGLD spoch 18\n",
      "Loss = -1.3020e-01, PNorm = 86.3907, GNorm = 13.7108, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.131153\n",
      "Collecting sample 13\n",
      "SGLD spoch 19\n",
      "Loss = -1.3085e-01, PNorm = 86.3149, GNorm = 14.7481, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.126677\n",
      "Collecting sample 14\n",
      "SGLD spoch 20\n",
      "Loss = -1.3148e-01, PNorm = 86.2391, GNorm = 14.2842, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.129315\n",
      "Collecting sample 15\n",
      "SGLD spoch 21\n",
      "Loss = -1.3210e-01, PNorm = 86.1627, GNorm = 15.9764, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.131279\n",
      "Collecting sample 16\n",
      "SGLD spoch 22\n",
      "Loss = -1.3271e-01, PNorm = 86.0871, GNorm = 14.9881, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.132198\n",
      "Collecting sample 17\n",
      "SGLD spoch 23\n",
      "Loss = -1.3330e-01, PNorm = 86.0111, GNorm = 16.8117, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.132092\n",
      "Collecting sample 18\n",
      "SGLD spoch 24\n",
      "Loss = -1.3387e-01, PNorm = 85.9352, GNorm = 16.1734, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.125233\n",
      "Collecting sample 19\n",
      "Model 0, sample 0 test mae = 2.216423\n",
      "Model 0, sample 1 test mae = 2.214768\n",
      "Model 0, sample 2 test mae = 2.214678\n",
      "Model 0, sample 3 test mae = 2.215776\n",
      "Model 0, sample 4 test mae = 2.216860\n",
      "Model 0, sample 5 test mae = 2.212918\n",
      "Model 0, sample 6 test mae = 2.214346\n",
      "Model 0, sample 7 test mae = 2.212072\n",
      "Model 0, sample 8 test mae = 2.214308\n",
      "Model 0, sample 9 test mae = 2.214378\n",
      "Model 0, sample 10 test mae = 2.215519\n",
      "Model 0, sample 11 test mae = 2.216780\n",
      "Model 0, sample 12 test mae = 2.216569\n",
      "Model 0, sample 13 test mae = 2.217422\n",
      "Model 0, sample 14 test mae = 2.215564\n",
      "Model 0, sample 15 test mae = 2.217052\n",
      "Model 0, sample 16 test mae = 2.217730\n",
      "Model 0, sample 17 test mae = 2.218406\n",
      "Model 0, sample 18 test mae = 2.217886\n",
      "Model 0, sample 19 test mae = 2.215175\n",
      "BMA test mae = 2.207475\n"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 20\n",
    "\n",
    "\n",
    "# sgld\n",
    "\n",
    "args.sgld = True\n",
    "args.init_log_noise = -2\n",
    "\n",
    "args.lr_sgld = 1e-4\n",
    "args.weight_decay_sgld = 0.1\n",
    "\n",
    "args.batch_size_sgld = 200\n",
    "args.log_frequency_sgld = 200\n",
    "\n",
    "args.burnin_epochs = 5\n",
    "args.mix_epochs = 1\n",
    "\n",
    "\n",
    "results_sgld = run_training(args)\n",
    "#np.savez(args.save_dir+'/results_sgld', results_sgld)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10108it [00:00, 101075.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-6a41b4ab-3f98-4bbb-82d2-2b0cc542ed11.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'batch_size_gp': 100,\n",
      " 'batch_size_sgld': 50,\n",
      " 'bias': False,\n",
      " 'block': True,\n",
      " 'burnin_epochs': 10,\n",
      " 'c_swag': 0,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0.0,\n",
      " 'dropout_FFNonly': False,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_gp': 50,\n",
      " 'epochs_swag': 0,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'final_lr_gp': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'gp': True,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise_sgld': -2,\n",
      " 'init_lr': 0.0001,\n",
      " 'init_lr_gp': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'log_frequency_gp': 400,\n",
      " 'log_frequency_sgld': 0,\n",
      " 'lr_sgld': 0.0001,\n",
      " 'lr_swag': 0.0001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_lr_gp': 0.001,\n",
      " 'max_num_models': 0,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'mix_epochs': 1,\n",
      " 'momentum_swag': 0.9,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_inducing_points': 800,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': None,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 1,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': False,\n",
      " 'target_columns': None,\n",
      " 'task_names': None,\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': None,\n",
      " 'undirected': False,\n",
      " 'unfreeze_epoch_gp': 20,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'warmup_epochs_gp': 2,\n",
      " 'wd_swag': 0,\n",
      " 'weight_decay_sgld': 0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46579it [00:00, 78221.22it/s] \n",
      "100%|██████████| 50000/50000 [00:00<00:00, 262106.65it/s]\n",
      " 56%|█████▌    | 28082/50000 [00:01<00:01, 12414.44it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d17c1cf971df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mresults_gp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#np.savez(args.save_dir+'/results_gp', results_gp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/train/run_training.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_columns\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mget_task_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/data/utils.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(path, smiles_column, target_columns, skip_invalid_smiles, args, features_path, features_generator, max_data_size, logger)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mskip_invalid_smiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0moriginal_data_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_invalid_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0moriginal_data_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/data/utils.py\u001b[0m in \u001b[0;36mfilter_invalid_smiles\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mMoleculeDataset\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0monly\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mmolecules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     return MoleculeDataset([datapoint for datapoint in tqdm(data)\n\u001b[0m\u001b[1;32m     94\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdatapoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmiles\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdatapoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                             and datapoint.mol.GetNumHeavyAtoms() > 0])\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/data/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     return MoleculeDataset([datapoint for datapoint in tqdm(data)\n\u001b[0;32m---> 94\u001b[0;31m                             \u001b[0;32mif\u001b[0m \u001b[0mdatapoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmiles\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdatapoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                             and datapoint.mol.GetNumHeavyAtoms() > 0])\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/data/data.py\u001b[0m in \u001b[0;36mmol\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;34m\"\"\"Get the RDKit molecule for the SMILES string (with lazy loading).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolFromSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 1\n",
    "args.epochs = 50\n",
    "\n",
    "\n",
    "# GP args\n",
    "args.gp = True\n",
    "args.num_inducing_points = 800\n",
    "    \n",
    "args.batch_size_gp = 100\n",
    "args.log_frequency_gp = 400\n",
    "    \n",
    "args.epochs_gp = 50\n",
    "args.warmup_epochs_gp = 2\n",
    "args.unfreeze_epoch_gp = 20\n",
    "    \n",
    "args.init_lr_gp = 1e-4\n",
    "args.max_lr_gp = 1e-3\n",
    "args.final_lr_gp = 1e-4\n",
    "\n",
    "\n",
    "results_gp = run_training(args)\n",
    "#np.savez(args.save_dir+'/results_gp', results_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-6a41b4ab-3f98-4bbb-82d2-2b0cc542ed11.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'batch_size': 200,\n",
      " 'batch_size_gp': 100,\n",
      " 'batch_size_sgld': 50,\n",
      " 'bbp': True,\n",
      " 'bias': False,\n",
      " 'block': True,\n",
      " 'burnin_epochs': 10,\n",
      " 'c_swag': 0,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0.0,\n",
      " 'dropout_FFNonly': False,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_gp': 100,\n",
      " 'epochs_phase1_bbp': 10,\n",
      " 'epochs_phase2_bbp': 20,\n",
      " 'epochs_phase3_bbp': 10,\n",
      " 'epochs_swag': 0,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'final_lr_gp': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'gp': False,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise_bbp': -2.9,\n",
      " 'init_log_noise_sgld': -2,\n",
      " 'init_lr': 0.0001,\n",
      " 'init_lr_gp': 0.0001,\n",
      " 'log_frequency': 10,\n",
      " 'log_frequency_gp': 100,\n",
      " 'log_frequency_sgld': 0,\n",
      " 'lr_phase1_bbp': 0.0005,\n",
      " 'lr_phase2_bbp': 1e-05,\n",
      " 'lr_phase3_bbp': 1e-06,\n",
      " 'lr_sgld': 0.0001,\n",
      " 'lr_swag': 0.0001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_lr_gp': 0.001,\n",
      " 'max_num_models': 0,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'mix_epochs': 1,\n",
      " 'momentum_swag': 0.9,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_inducing_points': 2000,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': 12,\n",
      " 'prior_sig_bbp': 0.1,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'rho_max_bbp': -4,\n",
      " 'rho_min_bbp': -5,\n",
      " 'samples': 1,\n",
      " 'samples_bbp': 5,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': False,\n",
      " 'target_columns': None,\n",
      " 'task_names': ['mu',\n",
      "                'alpha',\n",
      "                'homo',\n",
      "                'lumo',\n",
      "                'gap',\n",
      "                'r2',\n",
      "                'zpve',\n",
      "                'cv',\n",
      "                'u0',\n",
      "                'u298',\n",
      "                'h298',\n",
      "                'g298'],\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': 40000,\n",
      " 'undirected': False,\n",
      " 'unfreeze_epoch_gp': 50,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'warmup_epochs_gp': 4,\n",
      " 'wd_swag': 0,\n",
      " 'weight_decay_sgld': 0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41995it [00:02, 17967.74it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 132661.28it/s]\n",
      "100%|██████████| 50000/50000 [00:04<00:00, 11373.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0.0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Model 0 best validation mae = inf on epoch 0\n",
      "----------BBP training PHASE 1: learning log noise----------\n",
      "loading log noise trained model\n",
      "----------BBP training PHASE 2: learning means with regularisation----------\n",
      "loading BEST reg trained model\n",
      "----------BBP training PHASE 3: learning a bandwidth for each weight----------\n",
      "BBP epoch 0\n",
      "Loss = 1.1800e+00, PNorm = 4831.3825, GNorm = 2618.5747, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6535, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(145.3546, grad_fn=<DivBackward0>)\n",
      "Loss = 1.0038e+00, PNorm = 4831.3780, GNorm = 2564.6048, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6530, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(170.9183, grad_fn=<DivBackward0>)\n",
      "Loss = 1.1054e+00, PNorm = 4831.3726, GNorm = 2758.3233, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6525, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(191.8840, grad_fn=<DivBackward0>)\n",
      "Loss = 1.2879e+00, PNorm = 4831.3681, GNorm = 2693.5600, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6521, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(214.3055, grad_fn=<DivBackward0>)\n",
      "Loss = 1.3214e+00, PNorm = 4831.3639, GNorm = 2535.9186, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6516, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(186.1942, grad_fn=<DivBackward0>)\n",
      "Loss = 9.6873e-01, PNorm = 4831.3586, GNorm = 2571.7305, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6512, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(168.9799, grad_fn=<DivBackward0>)\n",
      "Loss = 1.0611e+00, PNorm = 4831.3540, GNorm = 1807.1816, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6508, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(127.0847, grad_fn=<DivBackward0>)\n",
      "Loss = 1.0150e+00, PNorm = 4831.3488, GNorm = 1836.7588, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6503, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(123.7026, grad_fn=<DivBackward0>)\n",
      "Loss = 9.0026e-01, PNorm = 4831.3434, GNorm = 1755.5316, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6499, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(99.4603, grad_fn=<DivBackward0>)\n",
      "Loss = 9.3456e-01, PNorm = 4831.3382, GNorm = 1774.7785, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6494, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(131.8408, grad_fn=<DivBackward0>)\n",
      "Loss = 1.1115e+00, PNorm = 4831.3331, GNorm = 4475.5268, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6490, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(360.5333, grad_fn=<DivBackward0>)\n",
      "Loss = 9.6504e-01, PNorm = 4831.3278, GNorm = 2380.9134, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6486, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(140.4667, grad_fn=<DivBackward0>)\n",
      "Loss = 8.7388e-01, PNorm = 4831.3227, GNorm = 1487.6276, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6481, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(91.6569, grad_fn=<DivBackward0>)\n",
      "Loss = 9.0530e-01, PNorm = 4831.3174, GNorm = 1869.1259, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6477, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(119.6507, grad_fn=<DivBackward0>)\n",
      "Loss = 9.1276e-01, PNorm = 4831.3121, GNorm = 1904.9310, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6473, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(108.7621, grad_fn=<DivBackward0>)\n",
      "Loss = 8.3413e-01, PNorm = 4831.3066, GNorm = 1592.3739, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6468, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(94.3678, grad_fn=<DivBackward0>)\n",
      "Loss = 1.0528e+00, PNorm = 4831.3009, GNorm = 2388.7759, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6464, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(145.4527, grad_fn=<DivBackward0>)\n",
      "Loss = 1.0382e+00, PNorm = 4831.2961, GNorm = 1863.2915, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6459, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(104.4501, grad_fn=<DivBackward0>)\n",
      "Loss = 8.0873e-01, PNorm = 4831.2909, GNorm = 2048.1970, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6455, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(132.0197, grad_fn=<DivBackward0>)\n",
      "Loss = 1.0666e+00, PNorm = 4831.2861, GNorm = 1485.7800, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6451, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(79.3909, grad_fn=<DivBackward0>)\n",
      "Validation mae = 2.687972\n",
      "BBP epoch 1\n",
      "Loss = 7.3751e-01, PNorm = 4831.2805, GNorm = 1475.4998, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6446, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(90.7271, grad_fn=<DivBackward0>)\n",
      "Loss = 7.0633e-01, PNorm = 4831.2758, GNorm = 1257.9526, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6442, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(92.7849, grad_fn=<DivBackward0>)\n",
      "Loss = 1.2742e+00, PNorm = 4831.2706, GNorm = 1874.8525, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6438, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(108.9879, grad_fn=<DivBackward0>)\n",
      "Loss = 6.9276e-01, PNorm = 4831.2655, GNorm = 1084.5190, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6433, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(61.9299, grad_fn=<DivBackward0>)\n",
      "Loss = 8.7084e-01, PNorm = 4831.2601, GNorm = 1316.6402, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6429, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(99.0164, grad_fn=<DivBackward0>)\n",
      "Loss = 7.1008e-01, PNorm = 4831.2546, GNorm = 1145.1820, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6425, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(79.5150, grad_fn=<DivBackward0>)\n",
      "Loss = 8.0635e-01, PNorm = 4831.2501, GNorm = 1097.5015, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6421, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(68.2574, grad_fn=<DivBackward0>)\n",
      "Loss = 7.3502e-01, PNorm = 4831.2446, GNorm = 1909.2920, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6416, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(111.6394, grad_fn=<DivBackward0>)\n",
      "Loss = 8.8898e-01, PNorm = 4831.2387, GNorm = 1074.2042, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6412, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(53.2415, grad_fn=<DivBackward0>)\n",
      "Loss = 8.9349e-01, PNorm = 4831.2337, GNorm = 1738.2709, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6408, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(78.9901, grad_fn=<DivBackward0>)\n",
      "Loss = 8.0538e-01, PNorm = 4831.2286, GNorm = 1226.9554, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6403, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(67.8300, grad_fn=<DivBackward0>)\n",
      "Loss = 7.0411e-01, PNorm = 4831.2240, GNorm = 2137.9196, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6399, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(158.4129, grad_fn=<DivBackward0>)\n",
      "Loss = 7.3708e-01, PNorm = 4831.2190, GNorm = 1855.0889, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6395, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(108.9699, grad_fn=<DivBackward0>)\n",
      "Loss = 6.4156e-01, PNorm = 4831.2138, GNorm = 884.5942, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6390, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(53.6833, grad_fn=<DivBackward0>)\n",
      "Loss = 6.1720e-01, PNorm = 4831.2084, GNorm = 817.1961, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6386, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(41.0035, grad_fn=<DivBackward0>)\n",
      "Loss = 5.9370e-01, PNorm = 4831.2027, GNorm = 1377.5432, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6382, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(82.5657, grad_fn=<DivBackward0>)\n",
      "Loss = 6.4289e-01, PNorm = 4831.1974, GNorm = 822.2867, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6378, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(51.7628, grad_fn=<DivBackward0>)\n",
      "Loss = 6.3740e-01, PNorm = 4831.1924, GNorm = 681.5441, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6373, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(30.2236, grad_fn=<DivBackward0>)\n",
      "Loss = 6.9052e-01, PNorm = 4831.1875, GNorm = 1054.0609, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6369, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(55.8800, grad_fn=<DivBackward0>)\n",
      "Loss = 5.6929e-01, PNorm = 4831.1826, GNorm = 1368.9766, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6365, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(66.8952, grad_fn=<DivBackward0>)\n",
      "Validation mae = 3.430274\n",
      "BBP epoch 2\n",
      "Loss = 6.5539e-01, PNorm = 4831.1780, GNorm = 1108.2402, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6361, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(66.6772, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 6.1114e-01, PNorm = 4831.1731, GNorm = 1400.7493, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6356, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(94.7497, grad_fn=<DivBackward0>)\n",
      "Loss = 6.6431e-01, PNorm = 4831.1684, GNorm = 1216.0151, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6352, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(67.0776, grad_fn=<DivBackward0>)\n",
      "Loss = 6.1587e-01, PNorm = 4831.1635, GNorm = 557.8955, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6348, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(31.5280, grad_fn=<DivBackward0>)\n",
      "Loss = 6.4431e-01, PNorm = 4831.1589, GNorm = 1399.5858, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6344, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(82.8841, grad_fn=<DivBackward0>)\n",
      "Loss = 9.8808e-01, PNorm = 4831.1537, GNorm = 1702.9593, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6340, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(63.9207, grad_fn=<DivBackward0>)\n",
      "Loss = 5.8394e-01, PNorm = 4831.1485, GNorm = 785.8672, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6335, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(32.7249, grad_fn=<DivBackward0>)\n",
      "Loss = 5.2982e-01, PNorm = 4831.1437, GNorm = 991.5226, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6331, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(55.6443, grad_fn=<DivBackward0>)\n",
      "Loss = 6.0727e-01, PNorm = 4831.1389, GNorm = 613.3835, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6327, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(33.5212, grad_fn=<DivBackward0>)\n",
      "Loss = 5.4974e-01, PNorm = 4831.1334, GNorm = 677.7460, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6322, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(23.4790, grad_fn=<DivBackward0>)\n",
      "Loss = 5.4108e-01, PNorm = 4831.1280, GNorm = 744.5943, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6318, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(31.3395, grad_fn=<DivBackward0>)\n",
      "Loss = 6.0077e-01, PNorm = 4831.1232, GNorm = 1612.6313, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6314, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(87.7496, grad_fn=<DivBackward0>)\n",
      "Loss = 5.8920e-01, PNorm = 4831.1182, GNorm = 1488.1889, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6310, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(71.6706, grad_fn=<DivBackward0>)\n",
      "Loss = 5.5348e-01, PNorm = 4831.1126, GNorm = 707.3146, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6305, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(43.7037, grad_fn=<DivBackward0>)\n",
      "Loss = 6.1632e-01, PNorm = 4831.1072, GNorm = 718.4063, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6301, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(39.0887, grad_fn=<DivBackward0>)\n",
      "Loss = 5.3373e-01, PNorm = 4831.1024, GNorm = 1398.3419, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6297, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(71.4899, grad_fn=<DivBackward0>)\n",
      "Loss = 6.0158e-01, PNorm = 4831.0970, GNorm = 918.6527, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6292, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(45.1724, grad_fn=<DivBackward0>)\n",
      "Loss = 7.4260e-01, PNorm = 4831.0916, GNorm = 543.8205, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6288, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(17.3291, grad_fn=<DivBackward0>)\n",
      "Loss = 6.2571e-01, PNorm = 4831.0869, GNorm = 511.8967, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6284, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(17.3200, grad_fn=<DivBackward0>)\n",
      "Loss = 5.0871e-01, PNorm = 4831.0817, GNorm = 614.7263, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6280, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(28.1063, grad_fn=<DivBackward0>)\n",
      "Validation mae = 4.044608\n",
      "BBP epoch 3\n",
      "Loss = 6.4260e-01, PNorm = 4831.0762, GNorm = 1035.9149, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6276, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(49.0062, grad_fn=<DivBackward0>)\n",
      "Loss = 4.9018e-01, PNorm = 4831.0705, GNorm = 744.0015, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6271, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(45.3624, grad_fn=<DivBackward0>)\n",
      "Loss = 6.8234e-01, PNorm = 4831.0651, GNorm = 705.8870, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6267, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(44.3331, grad_fn=<DivBackward0>)\n",
      "Loss = 5.1982e-01, PNorm = 4831.0595, GNorm = 729.3238, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6263, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(31.7910, grad_fn=<DivBackward0>)\n",
      "Loss = 4.6392e-01, PNorm = 4831.0543, GNorm = 731.2991, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6259, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(35.2396, grad_fn=<DivBackward0>)\n",
      "Loss = 4.9777e-01, PNorm = 4831.0499, GNorm = 1050.6663, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6254, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(36.8647, grad_fn=<DivBackward0>)\n",
      "Loss = 4.5104e-01, PNorm = 4831.0446, GNorm = 460.1269, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6250, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(12.8326, grad_fn=<DivBackward0>)\n",
      "Loss = 4.9061e-01, PNorm = 4831.0390, GNorm = 445.6214, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6246, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(15.6475, grad_fn=<DivBackward0>)\n",
      "Loss = 5.2613e-01, PNorm = 4831.0336, GNorm = 1036.1331, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6242, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(96.9555, grad_fn=<DivBackward0>)\n",
      "Loss = 4.8230e-01, PNorm = 4831.0286, GNorm = 413.1634, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6237, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(10.3474, grad_fn=<DivBackward0>)\n",
      "Loss = 5.7057e-01, PNorm = 4831.0234, GNorm = 1950.1666, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6233, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(158.0920, grad_fn=<DivBackward0>)\n",
      "Loss = 4.8970e-01, PNorm = 4831.0184, GNorm = 999.1879, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6229, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(68.9600, grad_fn=<DivBackward0>)\n",
      "Loss = 4.4842e-01, PNorm = 4831.0127, GNorm = 722.0576, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6225, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(21.2080, grad_fn=<DivBackward0>)\n",
      "Loss = 5.0075e-01, PNorm = 4831.0073, GNorm = 706.9633, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6221, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(33.3446, grad_fn=<DivBackward0>)\n",
      "Loss = 7.1887e-01, PNorm = 4831.0017, GNorm = 685.3112, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6216, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(23.9925, grad_fn=<DivBackward0>)\n",
      "Loss = 4.6130e-01, PNorm = 4830.9962, GNorm = 815.4142, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6212, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(51.2842, grad_fn=<DivBackward0>)\n",
      "Loss = 4.4718e-01, PNorm = 4830.9904, GNorm = 392.8191, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6208, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(16.1558, grad_fn=<DivBackward0>)\n",
      "Loss = 4.8241e-01, PNorm = 4830.9859, GNorm = 394.2032, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6204, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(9.9483, grad_fn=<DivBackward0>)\n",
      "Loss = 4.6917e-01, PNorm = 4830.9799, GNorm = 472.2014, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6199, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(25.5314, grad_fn=<DivBackward0>)\n",
      "Loss = 4.9435e-01, PNorm = 4830.9744, GNorm = 550.1436, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6195, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(19.5615, grad_fn=<DivBackward0>)\n",
      "Validation mae = 4.489590\n",
      "BBP epoch 4\n",
      "Loss = 4.1150e-01, PNorm = 4830.9697, GNorm = 611.0372, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6191, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(21.0649, grad_fn=<DivBackward0>)\n",
      "Loss = 4.9302e-01, PNorm = 4830.9646, GNorm = 675.8540, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6187, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(61.4340, grad_fn=<DivBackward0>)\n",
      "Loss = 5.1035e-01, PNorm = 4830.9601, GNorm = 726.9741, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6182, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(48.8086, grad_fn=<DivBackward0>)\n",
      "Loss = 4.9460e-01, PNorm = 4830.9556, GNorm = 685.5657, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6178, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(30.0935, grad_fn=<DivBackward0>)\n",
      "Loss = 4.0676e-01, PNorm = 4830.9505, GNorm = 506.5878, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6174, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(20.5684, grad_fn=<DivBackward0>)\n",
      "Loss = 5.1287e-01, PNorm = 4830.9448, GNorm = 358.1067, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6170, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(10.9085, grad_fn=<DivBackward0>)\n",
      "Loss = 4.5557e-01, PNorm = 4830.9393, GNorm = 662.9772, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6166, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(51.2907, grad_fn=<DivBackward0>)\n",
      "Loss = 5.3818e-01, PNorm = 4830.9339, GNorm = 846.2042, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6161, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(39.1918, grad_fn=<DivBackward0>)\n",
      "Loss = 4.0805e-01, PNorm = 4830.9284, GNorm = 329.9147, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6157, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(6.0181, grad_fn=<DivBackward0>)\n",
      "Loss = 4.0179e-01, PNorm = 4830.9234, GNorm = 436.1045, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6153, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(11.5683, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 4.6965e-01, PNorm = 4830.9180, GNorm = 474.8772, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6149, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(19.7585, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1240e-01, PNorm = 4830.9131, GNorm = 513.5326, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6144, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(18.4350, grad_fn=<DivBackward0>)\n",
      "Loss = 4.3185e-01, PNorm = 4830.9074, GNorm = 774.7456, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6140, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(40.0981, grad_fn=<DivBackward0>)\n",
      "Loss = 6.7257e-01, PNorm = 4830.9022, GNorm = 432.6186, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6136, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(13.6440, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1393e-01, PNorm = 4830.8962, GNorm = 444.0232, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6132, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(17.5653, grad_fn=<DivBackward0>)\n",
      "Loss = 4.8508e-01, PNorm = 4830.8909, GNorm = 934.5856, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6127, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(50.3018, grad_fn=<DivBackward0>)\n",
      "Loss = 6.3344e-01, PNorm = 4830.8859, GNorm = 383.5854, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6123, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(13.7648, grad_fn=<DivBackward0>)\n",
      "Loss = 4.2462e-01, PNorm = 4830.8804, GNorm = 554.2943, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6119, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(24.5447, grad_fn=<DivBackward0>)\n",
      "Loss = 4.6555e-01, PNorm = 4830.8748, GNorm = 417.2248, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6115, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(19.8286, grad_fn=<DivBackward0>)\n",
      "Loss = 4.6545e-01, PNorm = 4830.8698, GNorm = 905.2982, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6111, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(71.3063, grad_fn=<DivBackward0>)\n",
      "Validation mae = 4.812517\n",
      "BBP epoch 5\n",
      "Loss = 4.3105e-01, PNorm = 4830.8647, GNorm = 707.2840, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6106, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(29.5260, grad_fn=<DivBackward0>)\n",
      "Loss = 4.4556e-01, PNorm = 4830.8590, GNorm = 1126.3737, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6102, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(50.6246, grad_fn=<DivBackward0>)\n",
      "Loss = 4.5861e-01, PNorm = 4830.8535, GNorm = 2095.7308, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6098, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(130.7480, grad_fn=<DivBackward0>)\n",
      "Loss = 3.9223e-01, PNorm = 4830.8481, GNorm = 497.4895, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6094, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(17.2473, grad_fn=<DivBackward0>)\n",
      "Loss = 6.6215e-01, PNorm = 4830.8430, GNorm = 355.8866, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6090, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(9.7711, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1110e-01, PNorm = 4830.8373, GNorm = 578.9507, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6085, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(27.9789, grad_fn=<DivBackward0>)\n",
      "Loss = 4.3104e-01, PNorm = 4830.8321, GNorm = 657.1073, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6081, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(70.6716, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8230e-01, PNorm = 4830.8272, GNorm = 818.9158, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6077, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(16.9909, grad_fn=<DivBackward0>)\n",
      "Loss = 3.7031e-01, PNorm = 4830.8221, GNorm = 428.1424, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6073, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(14.2457, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1448e-01, PNorm = 4830.8169, GNorm = 485.8131, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6068, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(19.4801, grad_fn=<DivBackward0>)\n",
      "Loss = 5.4208e-01, PNorm = 4830.8113, GNorm = 462.1161, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6064, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(11.7570, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8703e-01, PNorm = 4830.8058, GNorm = 368.0828, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6060, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(12.6330, grad_fn=<DivBackward0>)\n",
      "Loss = 4.0184e-01, PNorm = 4830.8004, GNorm = 355.9934, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6056, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(8.1537, grad_fn=<DivBackward0>)\n",
      "Loss = 4.0911e-01, PNorm = 4830.7950, GNorm = 451.0805, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6052, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(15.6122, grad_fn=<DivBackward0>)\n",
      "Loss = 4.3349e-01, PNorm = 4830.7897, GNorm = 384.2411, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6047, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(12.6291, grad_fn=<DivBackward0>)\n",
      "Loss = 4.5267e-01, PNorm = 4830.7851, GNorm = 2254.6403, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6043, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(137.6514, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1154e-01, PNorm = 4830.7805, GNorm = 409.2291, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6039, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(17.0932, grad_fn=<DivBackward0>)\n",
      "Loss = 4.8600e-01, PNorm = 4830.7749, GNorm = 1031.0116, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6035, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(37.7867, grad_fn=<DivBackward0>)\n",
      "Loss = 3.9161e-01, PNorm = 4830.7699, GNorm = 3439.7998, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6031, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(75.1709, grad_fn=<DivBackward0>)\n",
      "Loss = 4.4844e-01, PNorm = 4830.7648, GNorm = 348.6557, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6026, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(22.7023, grad_fn=<DivBackward0>)\n",
      "Validation mae = 4.971965\n",
      "BBP epoch 6\n",
      "Loss = 4.2316e-01, PNorm = 4830.7594, GNorm = 326.9690, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6022, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(6.8080, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1178e-01, PNorm = 4830.7535, GNorm = 433.3604, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6018, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(12.7134, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8415e-01, PNorm = 4830.7484, GNorm = 303.5745, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6014, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(41.5904, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8725e-01, PNorm = 4830.7434, GNorm = 341.7966, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6010, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(8.7549, grad_fn=<DivBackward0>)\n",
      "Loss = 3.9594e-01, PNorm = 4830.7375, GNorm = 322.4416, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6005, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(4.4360, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8681e-01, PNorm = 4830.7324, GNorm = 288.4184, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.6001, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(7.2713, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8774e-01, PNorm = 4830.7275, GNorm = 413.4056, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5997, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(12.0113, grad_fn=<DivBackward0>)\n",
      "Loss = 6.8923e-01, PNorm = 4830.7221, GNorm = 634.3839, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5993, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(27.3257, grad_fn=<DivBackward0>)\n",
      "Loss = 4.4723e-01, PNorm = 4830.7168, GNorm = 1129.5149, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5989, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(35.8390, grad_fn=<DivBackward0>)\n",
      "Loss = 4.0493e-01, PNorm = 4830.7117, GNorm = 363.0608, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5984, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(11.2650, grad_fn=<DivBackward0>)\n",
      "Loss = 5.1692e-01, PNorm = 4830.7065, GNorm = 421.1725, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5980, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(15.3664, grad_fn=<DivBackward0>)\n",
      "Loss = 3.7171e-01, PNorm = 4830.7008, GNorm = 575.9001, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5976, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(13.8088, grad_fn=<DivBackward0>)\n",
      "Loss = 3.7824e-01, PNorm = 4830.6959, GNorm = 355.0303, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5972, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(3.6401, grad_fn=<DivBackward0>)\n",
      "Loss = 5.0218e-01, PNorm = 4830.6904, GNorm = 351.6356, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5968, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(18.9489, grad_fn=<DivBackward0>)\n",
      "Loss = 4.3659e-01, PNorm = 4830.6850, GNorm = 673.9013, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5963, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(20.8656, grad_fn=<DivBackward0>)\n",
      "Loss = 3.7189e-01, PNorm = 4830.6796, GNorm = 390.8187, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5959, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(9.4478, grad_fn=<DivBackward0>)\n",
      "Loss = 3.9674e-01, PNorm = 4830.6737, GNorm = 531.4350, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5955, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(16.1850, grad_fn=<DivBackward0>)\n",
      "Loss = 3.6101e-01, PNorm = 4830.6681, GNorm = 439.0103, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5951, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(15.9280, grad_fn=<DivBackward0>)\n",
      "Loss = 3.4651e-01, PNorm = 4830.6627, GNorm = 299.4948, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5947, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(0.4712, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 3.6848e-01, PNorm = 4830.6573, GNorm = 334.2470, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5942, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(9.7208, grad_fn=<DivBackward0>)\n",
      "Validation mae = 5.061095\n",
      "BBP epoch 7\n",
      "Loss = 5.8894e-01, PNorm = 4830.6529, GNorm = 283.5411, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5938, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(2.9355, grad_fn=<DivBackward0>)\n",
      "Loss = 4.2190e-01, PNorm = 4830.6472, GNorm = 1602.2752, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5934, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(43.2326, grad_fn=<DivBackward0>)\n",
      "Loss = 4.5149e-01, PNorm = 4830.6423, GNorm = 516.4380, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5930, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(10.8104, grad_fn=<DivBackward0>)\n",
      "Loss = 3.4991e-01, PNorm = 4830.6365, GNorm = 714.1657, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5926, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(19.7591, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5128e-01, PNorm = 4830.6308, GNorm = 497.1803, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5922, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(22.4191, grad_fn=<DivBackward0>)\n",
      "Loss = 3.3832e-01, PNorm = 4830.6258, GNorm = 459.9313, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5917, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(16.2533, grad_fn=<DivBackward0>)\n",
      "Loss = 3.3570e-01, PNorm = 4830.6207, GNorm = 287.6142, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5913, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(3.9887, grad_fn=<DivBackward0>)\n",
      "Loss = 3.9838e-01, PNorm = 4830.6155, GNorm = 316.7207, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5909, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(2.5651, grad_fn=<DivBackward0>)\n",
      "Loss = 3.6927e-01, PNorm = 4830.6099, GNorm = 404.3273, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5905, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(12.0414, grad_fn=<DivBackward0>)\n",
      "Loss = 3.7807e-01, PNorm = 4830.6048, GNorm = 228.9046, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5901, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(1.0581, grad_fn=<DivBackward0>)\n",
      "Loss = 3.6565e-01, PNorm = 4830.5996, GNorm = 351.9455, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5896, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(8.3018, grad_fn=<DivBackward0>)\n",
      "Loss = 3.7679e-01, PNorm = 4830.5947, GNorm = 340.3090, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5892, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(1.7034, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8760e-01, PNorm = 4830.5892, GNorm = 366.4978, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5888, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(11.2241, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5604e-01, PNorm = 4830.5843, GNorm = 266.2026, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5884, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(3.9977, grad_fn=<DivBackward0>)\n",
      "Loss = 4.5334e-01, PNorm = 4830.5787, GNorm = 318.7793, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5879, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(6.9747, grad_fn=<DivBackward0>)\n",
      "Loss = 3.4279e-01, PNorm = 4830.5743, GNorm = 267.0111, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5875, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(7.1471, grad_fn=<DivBackward0>)\n",
      "Loss = 4.3844e-01, PNorm = 4830.5687, GNorm = 350.7145, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5871, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(5.9239, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5305e-01, PNorm = 4830.5626, GNorm = 327.3739, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5867, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(1.8935, grad_fn=<DivBackward0>)\n",
      "Loss = 4.6748e-01, PNorm = 4830.5569, GNorm = 293.0701, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5863, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(5.3745, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1532e-01, PNorm = 4830.5512, GNorm = 266.0779, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5859, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(4.9377, grad_fn=<DivBackward0>)\n",
      "Validation mae = 5.086206\n",
      "BBP epoch 8\n",
      "Loss = 3.3165e-01, PNorm = 4830.5461, GNorm = 343.0681, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5854, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(7.9120, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5039e-01, PNorm = 4830.5411, GNorm = 358.2060, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5850, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(11.6046, grad_fn=<DivBackward0>)\n",
      "Loss = 3.4248e-01, PNorm = 4830.5360, GNorm = 242.4582, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5846, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(0.3569, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1474e-01, PNorm = 4830.5299, GNorm = 695.1170, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5842, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(29.2168, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5466e-01, PNorm = 4830.5251, GNorm = 438.2090, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5838, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(9.7826, grad_fn=<DivBackward0>)\n",
      "Loss = 4.9626e-01, PNorm = 4830.5198, GNorm = 383.6142, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5834, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(8.2191, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8674e-01, PNorm = 4830.5145, GNorm = 251.2270, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5829, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(1.0576, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8176e-01, PNorm = 4830.5092, GNorm = 288.0983, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5825, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(7.0823, grad_fn=<DivBackward0>)\n",
      "Loss = 3.9785e-01, PNorm = 4830.5048, GNorm = 474.6224, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5821, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(17.7466, grad_fn=<DivBackward0>)\n",
      "Loss = 3.2956e-01, PNorm = 4830.4996, GNorm = 365.3617, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5817, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(2.3033, grad_fn=<DivBackward0>)\n",
      "Loss = 3.6087e-01, PNorm = 4830.4944, GNorm = 346.5179, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5813, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(20.5622, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1457e-01, PNorm = 4830.4891, GNorm = 233.0493, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5808, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(-0.0059, grad_fn=<DivBackward0>)\n",
      "Loss = 3.4584e-01, PNorm = 4830.4832, GNorm = 463.3251, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5804, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(11.9853, grad_fn=<DivBackward0>)\n",
      "Loss = 3.7401e-01, PNorm = 4830.4780, GNorm = 527.4440, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5800, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(31.3208, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5642e-01, PNorm = 4830.4722, GNorm = 245.6873, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5796, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(2.3534, grad_fn=<DivBackward0>)\n",
      "Loss = 4.1917e-01, PNorm = 4830.4664, GNorm = 3135.3663, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5792, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(138.4729, grad_fn=<DivBackward0>)\n",
      "Loss = 3.4968e-01, PNorm = 4830.4608, GNorm = 454.2216, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5788, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(14.5997, grad_fn=<DivBackward0>)\n",
      "Loss = 4.0539e-01, PNorm = 4830.4551, GNorm = 467.1546, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5783, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(56.5651, grad_fn=<DivBackward0>)\n",
      "Loss = 3.3097e-01, PNorm = 4830.4500, GNorm = 533.2498, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5779, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(6.4997, grad_fn=<DivBackward0>)\n",
      "Loss = 5.5318e-01, PNorm = 4830.4443, GNorm = 413.9077, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5775, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(16.1373, grad_fn=<DivBackward0>)\n",
      "Validation mae = 5.077148\n",
      "BBP epoch 9\n",
      "Loss = 3.4868e-01, PNorm = 4830.4386, GNorm = 295.0454, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5771, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(11.3202, grad_fn=<DivBackward0>)\n",
      "Loss = 3.8826e-01, PNorm = 4830.4327, GNorm = 465.3789, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5767, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(11.4968, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5748e-01, PNorm = 4830.4272, GNorm = 299.5459, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5763, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(4.1100, grad_fn=<DivBackward0>)\n",
      "Loss = 3.2863e-01, PNorm = 4830.4214, GNorm = 498.4212, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5758, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(8.5774, grad_fn=<DivBackward0>)\n",
      "Loss = 3.4723e-01, PNorm = 4830.4162, GNorm = 352.5982, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5754, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(11.0171, grad_fn=<DivBackward0>)\n",
      "Loss = 3.6104e-01, PNorm = 4830.4102, GNorm = 280.6757, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5750, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(-0.5173, grad_fn=<DivBackward0>)\n",
      "Loss = 3.2888e-01, PNorm = 4830.4042, GNorm = 765.6780, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5746, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(9.5215, grad_fn=<DivBackward0>)\n",
      "Loss = 3.3587e-01, PNorm = 4830.3990, GNorm = 299.3720, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5742, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(3.7660, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 3.5850e-01, PNorm = 4830.3940, GNorm = 294.9494, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5738, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(4.0714, grad_fn=<DivBackward0>)\n",
      "Loss = 4.4897e-01, PNorm = 4830.3890, GNorm = 294.3955, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5733, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(3.1817, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5669e-01, PNorm = 4830.3836, GNorm = 406.8137, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5729, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(41.5458, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5024e-01, PNorm = 4830.3780, GNorm = 417.2203, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5725, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(16.5873, grad_fn=<DivBackward0>)\n",
      "Loss = 4.5755e-01, PNorm = 4830.3728, GNorm = 267.0359, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5721, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(2.4451, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5439e-01, PNorm = 4830.3674, GNorm = 407.0830, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5717, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(53.5416, grad_fn=<DivBackward0>)\n",
      "Loss = 3.5696e-01, PNorm = 4830.3619, GNorm = 471.3035, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5712, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(7.8003, grad_fn=<DivBackward0>)\n",
      "Loss = 3.9392e-01, PNorm = 4830.3565, GNorm = 315.9804, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5708, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(5.9362, grad_fn=<DivBackward0>)\n",
      "Loss = 3.4498e-01, PNorm = 4830.3514, GNorm = 228.1290, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5704, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(-0.2310, grad_fn=<DivBackward0>)\n",
      "Loss = 3.2816e-01, PNorm = 4830.3466, GNorm = 420.5125, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5700, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(10.1696, grad_fn=<DivBackward0>)\n",
      "Loss = 3.3919e-01, PNorm = 4830.3416, GNorm = 573.7483, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5696, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(22.2717, grad_fn=<DivBackward0>)\n",
      "Loss = 5.7564e-01, PNorm = 4830.3365, GNorm = 473.6522, lr_0 = 1.0000e-06\n",
      "kl term\n",
      "tensor(58.5692, grad_fn=<DivBackward0>)\n",
      "data\n",
      "tensor(41.3669, grad_fn=<DivBackward0>)\n",
      "Validation mae = 5.079474\n",
      "loading reg trained model with best val acc\n",
      "Warning: Pretrained parameter \"encoder.encoder.W_i.W_p\" cannot be found in model parameters.\n",
      "Warning: Pretrained parameter \"encoder.encoder.W_h.W_p\" cannot be found in model parameters.\n",
      "Warning: Pretrained parameter \"encoder.encoder.W_o.W_p\" cannot be found in model parameters.\n",
      "Warning: Pretrained parameter \"encoder.encoder.W_o.b_p\" cannot be found in model parameters.\n",
      "Warning: Pretrained parameter \"layer_in.W_p\" cannot be found in model parameters.\n",
      "Warning: Pretrained parameter \"layer_in.b_p\" cannot be found in model parameters.\n",
      "Warning: Pretrained parameter \"layer_hid_1.W_p\" cannot be found in model parameters.\n",
      "Warning: Pretrained parameter \"layer_hid_1.b_p\" cannot be found in model parameters.\n",
      "Warning: Pretrained parameter \"layer_out.W_p\" cannot be found in model parameters.\n",
      "Warning: Pretrained parameter \"layer_out.b_p\" cannot be found in model parameters.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'children'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e61fee4b78e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mresults_bbp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/train/run_training.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mbbp_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             )\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/train/predict.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, data_loader, args, disable_progress_bar, scaler, test_data, bbp_sample)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mbbp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBayesLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mbbp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'children'"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 1\n",
    "args.epochs = 50\n",
    "\n",
    "\n",
    "### BBP arguments ###\n",
    "\n",
    "args.bbp = True\n",
    "args.batch_size = 200\n",
    "args.log_frequency = 10\n",
    "\n",
    "\n",
    "### PHASE 1 ###\n",
    "\n",
    "args.init_log_noise_bbp = -2.9\n",
    "\n",
    "args.epochs_phase1_bbp = 10\n",
    "args.lr_phase1_bbp = 5e-4\n",
    "\n",
    "\n",
    "### PHASE 2 ###\n",
    "\n",
    "args.prior_sig_bbp = 0.1\n",
    "\n",
    "args.epochs_phase2_bbp = 20\n",
    "args.lr_phase2_bbp = 1e-5\n",
    "\n",
    "\n",
    "### PHASE 3 ###\n",
    "\n",
    "args.rho_min_bbp = -5\n",
    "args.rho_max_bbp = -4\n",
    "\n",
    "args.samples_bbp = 5\n",
    "\n",
    "args.lr_phase3_bbp = 1e-6\n",
    "args.epochs_phase3_bbp = 10\n",
    "\n",
    "\n",
    "\n",
    "results_bbp = run_training(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/georgelamb/Documents/GitHub/chempropBayes/chemprop/bayes/bbp.py\u001b[0m(75)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     73 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m            \u001b[0mstd_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute stds for weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 75 \u001b[0;31m            \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m            \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     77 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> np.all(np.isfinite(W_p.detach().numpy()))\n",
      "*** NameError: name 'W_p' is not defined\n",
      "ipdb> np.all(np.isfinite(self.W_p.detach().numpy()))\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load npz files\n",
    "results_MAP = np.load(args.save_dir+'/results_MAP.npz')['arr_0']\n",
    "results_ens = np.load(args.save_dir+'/results_ens.npz')['arr_0']\n",
    "results_MCdrop = np.load(args.save_dir+'/results_MCdrop.npz')['arr_0']\n",
    "results_swagD = np.load(args.save_dir+'/results_swagD.npz')['arr_0']\n",
    "results_swag = np.load(args.save_dir+'/results_swag.npz')['arr_0']\n",
    "results_swagM = np.load(args.save_dir+'/results_swagM.npz')['arr_0']\n",
    "results_sgld = np.load(args.save_dir+'/results_sgld.npz')['arr_0']\n",
    "results_gp = np.load(args.save_dir+'/results_gp.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row and column names\n",
    "row_names = get_task_names(args.data_path)+['AVG']\n",
    "col_names = ['MAP',\n",
    "             'MAP ens',\n",
    "             'MC-drop',\n",
    "             'SWAG-D',\n",
    "             'SWAG',\n",
    "             'MultiSWAG',\n",
    "             'SGLD',\n",
    "             'GP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP ens</th>\n",
       "      <th>MC-drop</th>\n",
       "      <th>SWAG-D</th>\n",
       "      <th>SWAG</th>\n",
       "      <th>MultiSWAG</th>\n",
       "      <th>SGLD</th>\n",
       "      <th>GP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>0.387702</td>\n",
       "      <td>0.336056</td>\n",
       "      <td>0.350818</td>\n",
       "      <td>0.383174</td>\n",
       "      <td>0.383320</td>\n",
       "      <td>0.335484</td>\n",
       "      <td>0.384913</td>\n",
       "      <td>0.390236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>0.477786</td>\n",
       "      <td>0.347660</td>\n",
       "      <td>0.514025</td>\n",
       "      <td>0.476499</td>\n",
       "      <td>0.476623</td>\n",
       "      <td>0.362156</td>\n",
       "      <td>0.475074</td>\n",
       "      <td>0.464999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homo</th>\n",
       "      <td>0.003441</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>0.003225</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.003403</td>\n",
       "      <td>0.003471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lumo</th>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.003375</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.003387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gap</th>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.004482</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.003620</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.004519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>20.577695</td>\n",
       "      <td>15.856580</td>\n",
       "      <td>20.239128</td>\n",
       "      <td>20.251981</td>\n",
       "      <td>20.245412</td>\n",
       "      <td>16.006372</td>\n",
       "      <td>20.287025</td>\n",
       "      <td>20.406845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zpve</th>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.000993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv</th>\n",
       "      <td>0.222967</td>\n",
       "      <td>0.156884</td>\n",
       "      <td>0.228869</td>\n",
       "      <td>0.220962</td>\n",
       "      <td>0.220682</td>\n",
       "      <td>0.163157</td>\n",
       "      <td>0.220081</td>\n",
       "      <td>0.226035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u0</th>\n",
       "      <td>1.307139</td>\n",
       "      <td>0.772746</td>\n",
       "      <td>1.404273</td>\n",
       "      <td>1.281087</td>\n",
       "      <td>1.283080</td>\n",
       "      <td>0.852363</td>\n",
       "      <td>1.278532</td>\n",
       "      <td>1.193313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u298</th>\n",
       "      <td>1.304406</td>\n",
       "      <td>0.772038</td>\n",
       "      <td>1.404264</td>\n",
       "      <td>1.278419</td>\n",
       "      <td>1.277628</td>\n",
       "      <td>0.852620</td>\n",
       "      <td>1.276673</td>\n",
       "      <td>1.194174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h298</th>\n",
       "      <td>1.298653</td>\n",
       "      <td>0.773239</td>\n",
       "      <td>1.404264</td>\n",
       "      <td>1.277357</td>\n",
       "      <td>1.275773</td>\n",
       "      <td>0.852605</td>\n",
       "      <td>1.276214</td>\n",
       "      <td>1.194325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g298</th>\n",
       "      <td>1.302806</td>\n",
       "      <td>0.771541</td>\n",
       "      <td>1.404294</td>\n",
       "      <td>1.279689</td>\n",
       "      <td>1.280840</td>\n",
       "      <td>0.853580</td>\n",
       "      <td>1.278837</td>\n",
       "      <td>1.193616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG</th>\n",
       "      <td>2.240973</td>\n",
       "      <td>1.649702</td>\n",
       "      <td>2.246839</td>\n",
       "      <td>2.205125</td>\n",
       "      <td>2.204640</td>\n",
       "      <td>1.690673</td>\n",
       "      <td>2.207475</td>\n",
       "      <td>2.189659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MAP    MAP ens    MC-drop     SWAG-D       SWAG  MultiSWAG  \\\n",
       "mu      0.387702   0.336056   0.350818   0.383174   0.383320   0.335484   \n",
       "alpha   0.477786   0.347660   0.514025   0.476499   0.476623   0.362156   \n",
       "homo    0.003441   0.002780   0.003225   0.003389   0.003388   0.002781   \n",
       "lumo    0.003418   0.002671   0.003418   0.003375   0.003374   0.002690   \n",
       "gap     0.004547   0.003631   0.004301   0.004482   0.004478   0.003620   \n",
       "r2     20.577695  15.856580  20.239128  20.251981  20.245412  16.006372   \n",
       "zpve    0.001121   0.000599   0.001183   0.001088   0.001085   0.000652   \n",
       "cv      0.222967   0.156884   0.228869   0.220962   0.220682   0.163157   \n",
       "u0      1.307139   0.772746   1.404273   1.281087   1.283080   0.852363   \n",
       "u298    1.304406   0.772038   1.404264   1.278419   1.277628   0.852620   \n",
       "h298    1.298653   0.773239   1.404264   1.277357   1.275773   0.852605   \n",
       "g298    1.302806   0.771541   1.404294   1.279689   1.280840   0.853580   \n",
       "AVG     2.240973   1.649702   2.246839   2.205125   2.204640   1.690673   \n",
       "\n",
       "            SGLD         GP  \n",
       "mu      0.384913   0.390236  \n",
       "alpha   0.475074   0.464999  \n",
       "homo    0.003403   0.003471  \n",
       "lumo    0.003373   0.003387  \n",
       "gap     0.004493   0.004519  \n",
       "r2     20.287025  20.406845  \n",
       "zpve    0.001083   0.000993  \n",
       "cv      0.220081   0.226035  \n",
       "u0      1.278532   1.193313  \n",
       "u298    1.276673   1.194174  \n",
       "h298    1.276214   1.194325  \n",
       "g298    1.278837   1.193616  \n",
       "AVG     2.207475   2.189659  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build df for absolute MAE\n",
    "results = np.array([\n",
    "    results_MAP,\n",
    "    results_ens,\n",
    "    results_MCdrop,\n",
    "    results_swagD,\n",
    "    results_swag,\n",
    "    results_swagM,\n",
    "    results_sgld,\n",
    "    results_gp\n",
    "    ]).T\n",
    "averages = np.mean(results,0)\n",
    "df = pd.DataFrame(np.vstack([results,averages]), columns=col_names, index=row_names)\n",
    "df.round(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
