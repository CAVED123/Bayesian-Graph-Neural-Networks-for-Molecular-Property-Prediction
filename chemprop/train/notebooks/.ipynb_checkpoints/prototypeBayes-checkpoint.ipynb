{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs prototype versions of the following methods:\n",
    "- MAP\n",
    "- MAP Ensemble\n",
    "- MC-Dropout\n",
    "- Featurisation + GP\n",
    "- SWAG-Diag\n",
    "- SWAG\n",
    "- SGLD\n",
    "- Mean field\n",
    "\n",
    "The methods are run with:\n",
    "- A smaller version of QM9 (50,000 examples)\n",
    "- A single split of the data (random)\n",
    "- A D-MPNN optimised through crude grid-search (hidden size 500, depth 5, layers 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "from tqdm import trange\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch_geometric.datasets import QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/georgelamb/Documents/GitHub/chempropBayes\n"
     ]
    }
   ],
   "source": [
    "# cd to chempropBayes\n",
    "%cd /Users/georgelamb/Documents/GitHub/chempropBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import run_training\n",
    "from chemprop.train.run_training import run_training\n",
    "from chemprop.args import TrainArgs\n",
    "\n",
    "from chemprop.data.utils import get_class_sizes, get_data, get_task_names, split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate args class and load from dict\n",
    "args = TrainArgs()\n",
    "args.from_dict({\n",
    "    'dataset_type': 'regression',\n",
    "    'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv'\n",
    "})\n",
    "\n",
    "# location for model checkpoints to be saved\n",
    "args.save_dir = '/Users/georgelamb/Documents/GitHub/chempropBayes/log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### args (non-model)\n",
    "\n",
    "# seed for splitting and loading data\n",
    "args.seed = 0\n",
    "\n",
    "# data\n",
    "args.max_data_size = 50000\n",
    "args.features_path = None\n",
    "args.features_generator = None\n",
    "\n",
    "# splitting data\n",
    "args.split_type = 'random'\n",
    "args.split_sizes = (0.8, 0.1, 0.1)\n",
    "\n",
    "# evaluation metric\n",
    "args.metric = 'mae'\n",
    "\n",
    "# epochs and logging\n",
    "args.epochs = 50\n",
    "args.log_frequency = 800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### args (model)\n",
    "\n",
    "# seed for random initial weights\n",
    "args.pytorch_seed = 0\n",
    "\n",
    "# message passing\n",
    "args.atom_messages = False\n",
    "args.undirected = False\n",
    "args.bias = False\n",
    "args.hidden_size = 500\n",
    "args.depth = 5\n",
    "\n",
    "# FFN\n",
    "args.ffn_hidden_size = args.hidden_size\n",
    "args.ffn_num_layers = 3\n",
    "\n",
    "# shared\n",
    "args.activation = 'ReLU'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11509it [00:00, 115084.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-10e6b1d6-82cf-42c2-a480-94ba8ab56c0b.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'bias': False,\n",
      " 'block': True,\n",
      " 'c_swag': 0,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_swag': 0,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise': 0,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'lr_swag': 0.0001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 0,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'momentum_swag': 0.9,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': None,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 1,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': False,\n",
      " 'target_columns': None,\n",
      " 'task_names': None,\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': None,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49520it [00:00, 85266.11it/s] \n",
      "100%|██████████| 50000/50000 [00:00<00:00, 256126.63it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 13243.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "tensor(1.0182, grad_fn=<DivBackward0>)\n",
      "tensor(1.0182, grad_fn=<DivBackward0>)\n",
      "tensor(1.0545, grad_fn=<DivBackward0>)\n",
      "tensor(1.0545, grad_fn=<DivBackward0>)\n",
      "tensor(0.9962, grad_fn=<DivBackward0>)\n",
      "tensor(0.9962, grad_fn=<DivBackward0>)\n",
      "tensor(0.8283, grad_fn=<DivBackward0>)\n",
      "tensor(0.8283, grad_fn=<DivBackward0>)\n",
      "tensor(1.0135, grad_fn=<DivBackward0>)\n",
      "tensor(1.0135, grad_fn=<DivBackward0>)\n",
      "tensor(0.8262, grad_fn=<DivBackward0>)\n",
      "tensor(0.8262, grad_fn=<DivBackward0>)\n",
      "tensor(0.8847, grad_fn=<DivBackward0>)\n",
      "tensor(0.8847, grad_fn=<DivBackward0>)\n",
      "tensor(0.9699, grad_fn=<DivBackward0>)\n",
      "tensor(0.9699, grad_fn=<DivBackward0>)\n",
      "tensor(0.8067, grad_fn=<DivBackward0>)\n",
      "tensor(0.8067, grad_fn=<DivBackward0>)\n",
      "tensor(0.9521, grad_fn=<DivBackward0>)\n",
      "tensor(0.9521, grad_fn=<DivBackward0>)\n",
      "tensor(0.8276, grad_fn=<DivBackward0>)\n",
      "tensor(0.8276, grad_fn=<DivBackward0>)\n",
      "tensor(0.7138, grad_fn=<DivBackward0>)\n",
      "tensor(0.7138, grad_fn=<DivBackward0>)\n",
      "tensor(0.9659, grad_fn=<DivBackward0>)\n",
      "tensor(0.9659, grad_fn=<DivBackward0>)\n",
      "tensor(0.6593, grad_fn=<DivBackward0>)\n",
      "tensor(0.6593, grad_fn=<DivBackward0>)\n",
      "tensor(1.0108, grad_fn=<DivBackward0>)\n",
      "tensor(1.0108, grad_fn=<DivBackward0>)\n",
      "tensor(0.7268, grad_fn=<DivBackward0>)\n",
      "tensor(0.7268, grad_fn=<DivBackward0>)\n",
      "tensor(0.7364, grad_fn=<DivBackward0>)\n",
      "tensor(0.7364, grad_fn=<DivBackward0>)\n",
      "tensor(0.8199, grad_fn=<DivBackward0>)\n",
      "tensor(0.8199, grad_fn=<DivBackward0>)\n",
      "tensor(0.7526, grad_fn=<DivBackward0>)\n",
      "tensor(0.7526, grad_fn=<DivBackward0>)\n",
      "tensor(0.7137, grad_fn=<DivBackward0>)\n",
      "tensor(0.7137, grad_fn=<DivBackward0>)\n",
      "tensor(0.6197, grad_fn=<DivBackward0>)\n",
      "tensor(0.6197, grad_fn=<DivBackward0>)\n",
      "tensor(0.6507, grad_fn=<DivBackward0>)\n",
      "tensor(0.6507, grad_fn=<DivBackward0>)\n",
      "tensor(0.7599, grad_fn=<DivBackward0>)\n",
      "tensor(0.7599, grad_fn=<DivBackward0>)\n",
      "tensor(0.9155, grad_fn=<DivBackward0>)\n",
      "tensor(0.9155, grad_fn=<DivBackward0>)\n",
      "tensor(0.7670, grad_fn=<DivBackward0>)\n",
      "tensor(0.7670, grad_fn=<DivBackward0>)\n",
      "tensor(0.6513, grad_fn=<DivBackward0>)\n",
      "tensor(0.6513, grad_fn=<DivBackward0>)\n",
      "tensor(0.7964, grad_fn=<DivBackward0>)\n",
      "tensor(0.7964, grad_fn=<DivBackward0>)\n",
      "tensor(0.7219, grad_fn=<DivBackward0>)\n",
      "tensor(0.7219, grad_fn=<DivBackward0>)\n",
      "tensor(0.7594, grad_fn=<DivBackward0>)\n",
      "tensor(0.7594, grad_fn=<DivBackward0>)\n",
      "tensor(0.8425, grad_fn=<DivBackward0>)\n",
      "tensor(0.8425, grad_fn=<DivBackward0>)\n",
      "tensor(0.5931, grad_fn=<DivBackward0>)\n",
      "tensor(0.5931, grad_fn=<DivBackward0>)\n",
      "tensor(0.5414, grad_fn=<DivBackward0>)\n",
      "tensor(0.5414, grad_fn=<DivBackward0>)\n",
      "tensor(0.6029, grad_fn=<DivBackward0>)\n",
      "tensor(0.6029, grad_fn=<DivBackward0>)\n",
      "tensor(0.5923, grad_fn=<DivBackward0>)\n",
      "tensor(0.5923, grad_fn=<DivBackward0>)\n",
      "tensor(0.5472, grad_fn=<DivBackward0>)\n",
      "tensor(0.5472, grad_fn=<DivBackward0>)\n",
      "tensor(0.8015, grad_fn=<DivBackward0>)\n",
      "tensor(0.8015, grad_fn=<DivBackward0>)\n",
      "tensor(1.0219, grad_fn=<DivBackward0>)\n",
      "tensor(1.0219, grad_fn=<DivBackward0>)\n",
      "tensor(0.5520, grad_fn=<DivBackward0>)\n",
      "tensor(0.5520, grad_fn=<DivBackward0>)\n",
      "tensor(0.6996, grad_fn=<DivBackward0>)\n",
      "tensor(0.6996, grad_fn=<DivBackward0>)\n",
      "tensor(0.6145, grad_fn=<DivBackward0>)\n",
      "tensor(0.6145, grad_fn=<DivBackward0>)\n",
      "tensor(0.6889, grad_fn=<DivBackward0>)\n",
      "tensor(0.6889, grad_fn=<DivBackward0>)\n",
      "tensor(0.6186, grad_fn=<DivBackward0>)\n",
      "tensor(0.6186, grad_fn=<DivBackward0>)\n",
      "tensor(0.6502, grad_fn=<DivBackward0>)\n",
      "tensor(0.6502, grad_fn=<DivBackward0>)\n",
      "tensor(0.6828, grad_fn=<DivBackward0>)\n",
      "tensor(0.6828, grad_fn=<DivBackward0>)\n",
      "tensor(0.6401, grad_fn=<DivBackward0>)\n",
      "tensor(0.6401, grad_fn=<DivBackward0>)\n",
      "tensor(0.5141, grad_fn=<DivBackward0>)\n",
      "tensor(0.5141, grad_fn=<DivBackward0>)\n",
      "tensor(0.7364, grad_fn=<DivBackward0>)\n",
      "tensor(0.7364, grad_fn=<DivBackward0>)\n",
      "tensor(0.5202, grad_fn=<DivBackward0>)\n",
      "tensor(0.5202, grad_fn=<DivBackward0>)\n",
      "tensor(0.5718, grad_fn=<DivBackward0>)\n",
      "tensor(0.5718, grad_fn=<DivBackward0>)\n",
      "tensor(0.6179, grad_fn=<DivBackward0>)\n",
      "tensor(0.6179, grad_fn=<DivBackward0>)\n",
      "tensor(0.5796, grad_fn=<DivBackward0>)\n",
      "tensor(0.5796, grad_fn=<DivBackward0>)\n",
      "tensor(0.5924, grad_fn=<DivBackward0>)\n",
      "tensor(0.5924, grad_fn=<DivBackward0>)\n",
      "tensor(0.4462, grad_fn=<DivBackward0>)\n",
      "tensor(0.4462, grad_fn=<DivBackward0>)\n",
      "tensor(0.6387, grad_fn=<DivBackward0>)\n",
      "tensor(0.6387, grad_fn=<DivBackward0>)\n",
      "tensor(0.4267, grad_fn=<DivBackward0>)\n",
      "tensor(0.4267, grad_fn=<DivBackward0>)\n",
      "tensor(0.7805, grad_fn=<DivBackward0>)\n",
      "tensor(0.7805, grad_fn=<DivBackward0>)\n",
      "tensor(0.6370, grad_fn=<DivBackward0>)\n",
      "tensor(0.6370, grad_fn=<DivBackward0>)\n",
      "tensor(0.4718, grad_fn=<DivBackward0>)\n",
      "tensor(0.4718, grad_fn=<DivBackward0>)\n",
      "tensor(0.5562, grad_fn=<DivBackward0>)\n",
      "tensor(0.5562, grad_fn=<DivBackward0>)\n",
      "tensor(0.4896, grad_fn=<DivBackward0>)\n",
      "tensor(0.4896, grad_fn=<DivBackward0>)\n",
      "tensor(0.4152, grad_fn=<DivBackward0>)\n",
      "tensor(0.4152, grad_fn=<DivBackward0>)\n",
      "tensor(0.7860, grad_fn=<DivBackward0>)\n",
      "tensor(0.7860, grad_fn=<DivBackward0>)\n",
      "tensor(0.5878, grad_fn=<DivBackward0>)\n",
      "tensor(0.5878, grad_fn=<DivBackward0>)\n",
      "tensor(0.4680, grad_fn=<DivBackward0>)\n",
      "tensor(0.4680, grad_fn=<DivBackward0>)\n",
      "tensor(0.5600, grad_fn=<DivBackward0>)\n",
      "tensor(0.5600, grad_fn=<DivBackward0>)\n",
      "tensor(0.4495, grad_fn=<DivBackward0>)\n",
      "tensor(0.4495, grad_fn=<DivBackward0>)\n",
      "tensor(0.3794, grad_fn=<DivBackward0>)\n",
      "tensor(0.3794, grad_fn=<DivBackward0>)\n",
      "tensor(0.5797, grad_fn=<DivBackward0>)\n",
      "tensor(0.5797, grad_fn=<DivBackward0>)\n",
      "tensor(0.6811, grad_fn=<DivBackward0>)\n",
      "tensor(0.6811, grad_fn=<DivBackward0>)\n",
      "tensor(0.4003, grad_fn=<DivBackward0>)\n",
      "tensor(0.4003, grad_fn=<DivBackward0>)\n",
      "tensor(0.5052, grad_fn=<DivBackward0>)\n",
      "tensor(0.5052, grad_fn=<DivBackward0>)\n",
      "tensor(0.4068, grad_fn=<DivBackward0>)\n",
      "tensor(0.4068, grad_fn=<DivBackward0>)\n",
      "tensor(0.4965, grad_fn=<DivBackward0>)\n",
      "tensor(0.4965, grad_fn=<DivBackward0>)\n",
      "tensor(0.6161, grad_fn=<DivBackward0>)\n",
      "tensor(0.6161, grad_fn=<DivBackward0>)\n",
      "tensor(0.5057, grad_fn=<DivBackward0>)\n",
      "tensor(0.5057, grad_fn=<DivBackward0>)\n",
      "tensor(0.3997, grad_fn=<DivBackward0>)\n",
      "tensor(0.3997, grad_fn=<DivBackward0>)\n",
      "tensor(0.3930, grad_fn=<DivBackward0>)\n",
      "tensor(0.3930, grad_fn=<DivBackward0>)\n",
      "tensor(1.1126, grad_fn=<DivBackward0>)\n",
      "tensor(1.1126, grad_fn=<DivBackward0>)\n",
      "tensor(0.5463, grad_fn=<DivBackward0>)\n",
      "tensor(0.5463, grad_fn=<DivBackward0>)\n",
      "tensor(0.4269, grad_fn=<DivBackward0>)\n",
      "tensor(0.4269, grad_fn=<DivBackward0>)\n",
      "tensor(0.7249, grad_fn=<DivBackward0>)\n",
      "tensor(0.7249, grad_fn=<DivBackward0>)\n",
      "tensor(0.4242, grad_fn=<DivBackward0>)\n",
      "tensor(0.4242, grad_fn=<DivBackward0>)\n",
      "tensor(0.4725, grad_fn=<DivBackward0>)\n",
      "tensor(0.4725, grad_fn=<DivBackward0>)\n",
      "tensor(0.7210, grad_fn=<DivBackward0>)\n",
      "tensor(0.7210, grad_fn=<DivBackward0>)\n",
      "tensor(0.3882, grad_fn=<DivBackward0>)\n",
      "tensor(0.3882, grad_fn=<DivBackward0>)\n",
      "tensor(0.5015, grad_fn=<DivBackward0>)\n",
      "tensor(0.5015, grad_fn=<DivBackward0>)\n",
      "tensor(0.4277, grad_fn=<DivBackward0>)\n",
      "tensor(0.4277, grad_fn=<DivBackward0>)\n",
      "tensor(0.4158, grad_fn=<DivBackward0>)\n",
      "tensor(0.4158, grad_fn=<DivBackward0>)\n",
      "tensor(0.7952, grad_fn=<DivBackward0>)\n",
      "tensor(0.7952, grad_fn=<DivBackward0>)\n",
      "tensor(0.4259, grad_fn=<DivBackward0>)\n",
      "tensor(0.4259, grad_fn=<DivBackward0>)\n",
      "tensor(0.6069, grad_fn=<DivBackward0>)\n",
      "tensor(0.6069, grad_fn=<DivBackward0>)\n",
      "tensor(0.4144, grad_fn=<DivBackward0>)\n",
      "tensor(0.4144, grad_fn=<DivBackward0>)\n",
      "tensor(0.4733, grad_fn=<DivBackward0>)\n",
      "tensor(0.4733, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-6:\n",
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-8:\n",
      "Process Process-7:\n",
      "Process Process-3:\n",
      "Process Process-2:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6716, grad_fn=<DivBackward0>)\n",
      "tensor(0.6716, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa450f565f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/Applications/anaconda3/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6ef21c93d655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresults_MAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#np.savez(args.save_dir+'/results_MAP', results_MAP)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/train/run_training.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(args, logger)\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             )\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExponentialLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/train/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, loss_func, optimizer, scheduler, args, n_iter, logger, writer, swag_model)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Move tensors to correct device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/models/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Don't apply sigmoid during training b/c using BCEWithLogitsLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/models/mpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, features_batch)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmol2graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/chempropBayes/chemprop/models/mpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mol_graph, features_batch)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0ma_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnei_a_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# num_atoms x hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mrev_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb2revb\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# num_bonds x hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_message\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb2a\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrev_message\u001b[0m  \u001b[0;31m# num_bonds x hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 1\n",
    "\n",
    "args.dropout = 0\n",
    "args.test_dropout = False\n",
    "\n",
    "results_MAP = run_training(args)\n",
    "#np.savez(args.save_dir+'/results_MAP', results_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10384it [00:00, 103832.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-606c20c4-d37f-4749-82ec-61f9fbcab2fd.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'bias': False,\n",
      " 'block': True,\n",
      " 'c_swag': 0,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0,\n",
      " 'ensemble_size': 10,\n",
      " 'epochs': 50,\n",
      " 'epochs_swag': 0,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'lr_swag': 0.0001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 0,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'momentum_swag': 0.9,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': None,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 1,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': False,\n",
      " 'target_columns': None,\n",
      " 'task_names': None,\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': None,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45784it [00:00, 52858.32it/s] \n",
      "100%|██████████| 50000/50000 [00:00<00:00, 125553.38it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 13144.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.6179e-03, PNorm = 49.3502, GNorm = 3.9430, lr_0 = 5.5056e-04\n",
      "Validation mae = 12.860934\n",
      "Epoch 1\n",
      "Loss = 3.3376e-03, PNorm = 51.5382, GNorm = 1.6459, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.169757\n",
      "Epoch 2\n",
      "Loss = 2.2128e-03, PNorm = 54.0087, GNorm = 0.6652, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.177531\n",
      "Epoch 3\n",
      "Loss = 1.6222e-03, PNorm = 55.9070, GNorm = 0.8129, lr_0 = 9.0846e-04\n",
      "Validation mae = 7.748067\n",
      "Epoch 4\n",
      "Loss = 1.4080e-03, PNorm = 57.8874, GNorm = 0.7858, lr_0 = 8.6591e-04\n",
      "Validation mae = 7.995050\n",
      "Epoch 5\n",
      "Loss = 1.1951e-03, PNorm = 59.7011, GNorm = 0.9493, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.806514\n",
      "Epoch 6\n",
      "Loss = 1.0894e-03, PNorm = 61.5091, GNorm = 0.3030, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.281439\n",
      "Epoch 7\n",
      "Loss = 1.0005e-03, PNorm = 63.3134, GNorm = 0.5665, lr_0 = 7.4985e-04\n",
      "Validation mae = 5.099877\n",
      "Epoch 8\n",
      "Loss = 8.9264e-04, PNorm = 64.9155, GNorm = 0.3422, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.292120\n",
      "Epoch 9\n",
      "Loss = 7.8944e-04, PNorm = 66.3597, GNorm = 0.2399, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.346213\n",
      "Epoch 10\n",
      "Loss = 7.6237e-04, PNorm = 67.9968, GNorm = 0.5543, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.487941\n",
      "Epoch 11\n",
      "Loss = 6.6571e-04, PNorm = 69.3753, GNorm = 0.2699, lr_0 = 6.1893e-04\n",
      "Validation mae = 4.052892\n",
      "Epoch 12\n",
      "Loss = 5.9497e-04, PNorm = 70.6016, GNorm = 0.3558, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.280318\n",
      "Epoch 13\n",
      "Loss = 5.4275e-04, PNorm = 71.8562, GNorm = 0.1506, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.238623\n",
      "Epoch 14\n",
      "Loss = 5.5829e-04, PNorm = 73.2488, GNorm = 0.1540, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.346532\n",
      "Epoch 15\n",
      "Loss = 5.2317e-04, PNorm = 74.4343, GNorm = 0.4223, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.235020\n",
      "Epoch 16\n",
      "Loss = 4.5317e-04, PNorm = 75.4005, GNorm = 0.1824, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.485054\n",
      "Epoch 17\n",
      "Loss = 3.9069e-04, PNorm = 76.2208, GNorm = 0.3152, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.391913\n",
      "Epoch 18\n",
      "Loss = 4.0170e-04, PNorm = 77.2611, GNorm = 0.3298, lr_0 = 4.4239e-04\n",
      "Validation mae = 3.120937\n",
      "Epoch 19\n",
      "Loss = 3.5528e-04, PNorm = 78.0467, GNorm = 0.1210, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.811184\n",
      "Epoch 20\n",
      "Loss = 3.1973e-04, PNorm = 78.7542, GNorm = 0.2209, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.733041\n",
      "Epoch 21\n",
      "Loss = 3.0105e-04, PNorm = 79.4718, GNorm = 0.1581, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.750101\n",
      "Epoch 22\n",
      "Loss = 2.7726e-04, PNorm = 80.1164, GNorm = 0.1703, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.736319\n",
      "Epoch 23\n",
      "Loss = 2.5734e-04, PNorm = 80.7267, GNorm = 0.1092, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.627317\n",
      "Epoch 24\n",
      "Loss = 2.4745e-04, PNorm = 81.3700, GNorm = 0.1553, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.801777\n",
      "Epoch 25\n",
      "Loss = 2.4464e-04, PNorm = 81.9136, GNorm = 0.1295, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.550626\n",
      "Epoch 26\n",
      "Loss = 2.0595e-04, PNorm = 82.3519, GNorm = 0.1275, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.526890\n",
      "Epoch 27\n",
      "Loss = 1.8733e-04, PNorm = 82.7512, GNorm = 0.1006, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.436306\n",
      "Epoch 28\n",
      "Loss = 1.7636e-04, PNorm = 83.1732, GNorm = 0.0876, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.464180\n",
      "Epoch 29\n",
      "Loss = 1.7206e-04, PNorm = 83.6087, GNorm = 0.1431, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.511821\n",
      "Epoch 30\n",
      "Loss = 1.5775e-04, PNorm = 83.9577, GNorm = 0.1254, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.391637\n",
      "Epoch 31\n",
      "Loss = 1.6260e-04, PNorm = 84.3189, GNorm = 0.1218, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.390429\n",
      "Epoch 32\n",
      "Loss = 1.3568e-04, PNorm = 84.6040, GNorm = 0.0955, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.354117\n",
      "Epoch 33\n",
      "Loss = 1.2337e-04, PNorm = 84.8708, GNorm = 0.0874, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.355782\n",
      "Epoch 34\n",
      "Loss = 1.1948e-04, PNorm = 85.1333, GNorm = 0.0964, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.357777\n",
      "Epoch 35\n",
      "Loss = 1.0906e-04, PNorm = 85.3728, GNorm = 0.0875, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.277918\n",
      "Epoch 36\n",
      "Loss = 1.0578e-04, PNorm = 85.6323, GNorm = 0.0614, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.396424\n",
      "Epoch 37\n",
      "Loss = 1.0383e-04, PNorm = 85.8613, GNorm = 0.0748, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.255885\n",
      "Epoch 38\n",
      "Loss = 9.2029e-05, PNorm = 86.0428, GNorm = 0.0672, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.219827\n",
      "Epoch 39\n",
      "Loss = 8.9530e-05, PNorm = 86.2335, GNorm = 0.0504, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.224348\n",
      "Epoch 40\n",
      "Loss = 8.6073e-05, PNorm = 86.4148, GNorm = 0.0689, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.216106\n",
      "Epoch 41\n",
      "Loss = 8.3960e-05, PNorm = 86.5933, GNorm = 0.0671, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.222690\n",
      "Epoch 42\n",
      "Loss = 7.7010e-05, PNorm = 86.7432, GNorm = 0.0491, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.205915\n",
      "Epoch 43\n",
      "Loss = 7.4229e-05, PNorm = 86.8854, GNorm = 0.0636, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.176927\n",
      "Epoch 44\n",
      "Loss = 7.0844e-05, PNorm = 87.0141, GNorm = 0.0635, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.319141\n",
      "Epoch 45\n",
      "Loss = 6.8878e-05, PNorm = 87.1449, GNorm = 0.0439, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.164977\n",
      "Epoch 46\n",
      "Loss = 6.5918e-05, PNorm = 87.2662, GNorm = 0.0709, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.168121\n",
      "Epoch 47\n",
      "Loss = 6.4436e-05, PNorm = 87.3904, GNorm = 0.0771, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.202392\n",
      "Epoch 48\n",
      "Loss = 6.1387e-05, PNorm = 87.4890, GNorm = 0.2994, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.198103\n",
      "Epoch 49\n",
      "Loss = 5.9817e-05, PNorm = 87.5923, GNorm = 0.0597, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.152893\n",
      "Model 0 best validation mae = 2.152893 on epoch 49\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "Model 0, sample 0 test mae = 2.240973\n",
      "Building model 1\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.8084e-03, PNorm = 49.3422, GNorm = 1.4219, lr_0 = 5.5056e-04\n",
      "Validation mae = 14.116870\n",
      "Epoch 1\n",
      "Loss = 3.3837e-03, PNorm = 51.6524, GNorm = 0.8506, lr_0 = 9.9994e-04\n",
      "Validation mae = 10.298059\n",
      "Epoch 2\n",
      "Loss = 2.1636e-03, PNorm = 54.0847, GNorm = 1.3159, lr_0 = 9.5310e-04\n",
      "Validation mae = 6.843976\n",
      "Epoch 3\n",
      "Loss = 1.6508e-03, PNorm = 56.1273, GNorm = 0.6646, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.955238\n",
      "Epoch 4\n",
      "Loss = 1.3244e-03, PNorm = 57.8782, GNorm = 0.6759, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.385604\n",
      "Epoch 5\n",
      "Loss = 1.1285e-03, PNorm = 59.6612, GNorm = 0.3661, lr_0 = 8.2535e-04\n",
      "Validation mae = 4.635380\n",
      "Epoch 6\n",
      "Loss = 1.0610e-03, PNorm = 61.5823, GNorm = 0.4345, lr_0 = 7.8670e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 5.418133\n",
      "Epoch 7\n",
      "Loss = 9.1105e-04, PNorm = 63.1882, GNorm = 0.5211, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.637251\n",
      "Epoch 8\n",
      "Loss = 9.3345e-04, PNorm = 65.1066, GNorm = 0.4089, lr_0 = 7.1473e-04\n",
      "Validation mae = 3.973786\n",
      "Epoch 9\n",
      "Loss = 7.6584e-04, PNorm = 66.6188, GNorm = 0.7726, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.061332\n",
      "Epoch 10\n",
      "Loss = 6.9761e-04, PNorm = 68.1124, GNorm = 0.6240, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.181092\n",
      "Epoch 11\n",
      "Loss = 6.5528e-04, PNorm = 69.5777, GNorm = 0.2318, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.531819\n",
      "Epoch 12\n",
      "Loss = 5.9359e-04, PNorm = 70.9326, GNorm = 0.3537, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.435850\n",
      "Epoch 13\n",
      "Loss = 5.3609e-04, PNorm = 72.2036, GNorm = 0.1721, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.336639\n",
      "Epoch 14\n",
      "Loss = 5.2606e-04, PNorm = 73.5360, GNorm = 0.2169, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.291054\n",
      "Epoch 15\n",
      "Loss = 4.7101e-04, PNorm = 74.6723, GNorm = 0.2456, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.473915\n",
      "Epoch 16\n",
      "Loss = 4.3407e-04, PNorm = 75.6640, GNorm = 0.2697, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.712384\n",
      "Epoch 17\n",
      "Loss = 4.2397e-04, PNorm = 76.7471, GNorm = 0.1460, lr_0 = 4.6413e-04\n",
      "Validation mae = 2.966270\n",
      "Epoch 18\n",
      "Loss = 3.6957e-04, PNorm = 77.5758, GNorm = 0.1757, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.692250\n",
      "Epoch 19\n",
      "Loss = 3.3741e-04, PNorm = 78.4063, GNorm = 0.1881, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.095839\n",
      "Epoch 20\n",
      "Loss = 3.1518e-04, PNorm = 79.1533, GNorm = 0.2982, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.803373\n",
      "Epoch 21\n",
      "Loss = 2.8574e-04, PNorm = 79.8943, GNorm = 0.1664, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.949890\n",
      "Epoch 22\n",
      "Loss = 2.7980e-04, PNorm = 80.6407, GNorm = 0.1805, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.686609\n",
      "Epoch 23\n",
      "Loss = 2.5763e-04, PNorm = 81.2710, GNorm = 0.1059, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.923734\n",
      "Epoch 24\n",
      "Loss = 2.4317e-04, PNorm = 81.9133, GNorm = 0.1402, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.750305\n",
      "Epoch 25\n",
      "Loss = 2.2323e-04, PNorm = 82.4860, GNorm = 0.1635, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.820157\n",
      "Epoch 26\n",
      "Loss = 2.0707e-04, PNorm = 82.9901, GNorm = 0.1298, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.521773\n",
      "Epoch 27\n",
      "Loss = 1.7734e-04, PNorm = 83.4070, GNorm = 0.1111, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.569918\n",
      "Epoch 28\n",
      "Loss = 1.6818e-04, PNorm = 83.8328, GNorm = 0.0996, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.489120\n",
      "Epoch 29\n",
      "Loss = 1.6481e-04, PNorm = 84.2540, GNorm = 0.0828, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.494049\n",
      "Epoch 30\n",
      "Loss = 1.8697e-04, PNorm = 84.7595, GNorm = 0.1063, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.525952\n",
      "Epoch 31\n",
      "Loss = 1.4098e-04, PNorm = 85.0443, GNorm = 0.0864, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.320901\n",
      "Epoch 32\n",
      "Loss = 1.2267e-04, PNorm = 85.3026, GNorm = 0.0874, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.325656\n",
      "Epoch 33\n",
      "Loss = 1.1824e-04, PNorm = 85.5957, GNorm = 0.0647, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.302424\n",
      "Epoch 34\n",
      "Loss = 1.1478e-04, PNorm = 85.8618, GNorm = 0.0784, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.338173\n",
      "Epoch 35\n",
      "Loss = 1.0720e-04, PNorm = 86.1151, GNorm = 0.0645, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.264286\n",
      "Epoch 36\n",
      "Loss = 1.0280e-04, PNorm = 86.3520, GNorm = 0.0623, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.255937\n",
      "Epoch 37\n",
      "Loss = 9.7410e-05, PNorm = 86.5760, GNorm = 0.0703, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.304665\n",
      "Epoch 38\n",
      "Loss = 9.6597e-05, PNorm = 86.8071, GNorm = 0.0715, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.308779\n",
      "Epoch 39\n",
      "Loss = 8.8348e-05, PNorm = 86.9879, GNorm = 0.0550, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.248668\n",
      "Epoch 40\n",
      "Loss = 8.1345e-05, PNorm = 87.1617, GNorm = 0.0617, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.214408\n",
      "Epoch 41\n",
      "Loss = 7.8394e-05, PNorm = 87.3309, GNorm = 0.0450, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.204201\n",
      "Epoch 42\n",
      "Loss = 7.4505e-05, PNorm = 87.4896, GNorm = 0.0595, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.217156\n",
      "Epoch 43\n",
      "Loss = 7.2436e-05, PNorm = 87.6335, GNorm = 0.0579, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.239622\n",
      "Epoch 44\n",
      "Loss = 7.0276e-05, PNorm = 87.7806, GNorm = 0.0427, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.192252\n",
      "Epoch 45\n",
      "Loss = 6.7100e-05, PNorm = 87.9119, GNorm = 0.0553, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.191167\n",
      "Epoch 46\n",
      "Loss = 6.4776e-05, PNorm = 88.0300, GNorm = 0.0459, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.169433\n",
      "Epoch 47\n",
      "Loss = 6.2813e-05, PNorm = 88.1498, GNorm = 0.0493, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.153947\n",
      "Epoch 48\n",
      "Loss = 6.0079e-05, PNorm = 88.2596, GNorm = 0.0412, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.145061\n",
      "Epoch 49\n",
      "Loss = 5.9117e-05, PNorm = 88.3665, GNorm = 0.0460, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.184777\n",
      "Model 1 best validation mae = 2.145061 on epoch 48\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "Model 1, sample 0 test mae = 2.224714\n",
      "Building model 2\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.8118e-03, PNorm = 49.2598, GNorm = 3.6852, lr_0 = 5.5056e-04\n",
      "Validation mae = 14.712115\n",
      "Epoch 1\n",
      "Loss = 3.5812e-03, PNorm = 51.6707, GNorm = 1.7590, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.276626\n",
      "Epoch 2\n",
      "Loss = 2.1848e-03, PNorm = 54.0679, GNorm = 0.5034, lr_0 = 9.5310e-04\n",
      "Validation mae = 6.510492\n",
      "Epoch 3\n",
      "Loss = 1.6076e-03, PNorm = 55.8710, GNorm = 0.5359, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.380683\n",
      "Epoch 4\n",
      "Loss = 1.3291e-03, PNorm = 57.5723, GNorm = 0.7324, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.002590\n",
      "Epoch 5\n",
      "Loss = 1.1981e-03, PNorm = 59.3246, GNorm = 0.6126, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.717666\n",
      "Epoch 6\n",
      "Loss = 1.0760e-03, PNorm = 61.1263, GNorm = 0.5710, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.789345\n",
      "Epoch 7\n",
      "Loss = 9.6973e-04, PNorm = 62.7810, GNorm = 0.2930, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.214593\n",
      "Epoch 8\n",
      "Loss = 8.5943e-04, PNorm = 64.4222, GNorm = 0.2041, lr_0 = 7.1473e-04\n",
      "Validation mae = 5.127343\n",
      "Epoch 9\n",
      "Loss = 8.1064e-04, PNorm = 65.9780, GNorm = 0.4401, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.528618\n",
      "Epoch 10\n",
      "Loss = 7.4218e-04, PNorm = 67.4638, GNorm = 0.3771, lr_0 = 6.4934e-04\n",
      "Validation mae = 3.873827\n",
      "Epoch 11\n",
      "Loss = 6.5053e-04, PNorm = 68.7789, GNorm = 0.2282, lr_0 = 6.1893e-04\n",
      "Validation mae = 4.028772\n",
      "Epoch 12\n",
      "Loss = 5.9840e-04, PNorm = 70.0023, GNorm = 0.2730, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.449576\n",
      "Epoch 13\n",
      "Loss = 5.5199e-04, PNorm = 71.3144, GNorm = 0.7193, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.368684\n",
      "Epoch 14\n",
      "Loss = 5.4749e-04, PNorm = 72.5725, GNorm = 0.1736, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.324569\n",
      "Epoch 15\n",
      "Loss = 4.5818e-04, PNorm = 73.5656, GNorm = 0.2311, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.610319\n",
      "Epoch 16\n",
      "Loss = 4.4090e-04, PNorm = 74.6365, GNorm = 0.6826, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.686868\n",
      "Epoch 17\n",
      "Loss = 4.0499e-04, PNorm = 75.6224, GNorm = 0.3068, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.856650\n",
      "Epoch 18\n",
      "Loss = 3.7225e-04, PNorm = 76.5536, GNorm = 0.2139, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.881593\n",
      "Epoch 19\n",
      "Loss = 3.4619e-04, PNorm = 77.4077, GNorm = 0.1842, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.061398\n",
      "Epoch 20\n",
      "Loss = 3.1049e-04, PNorm = 78.1165, GNorm = 0.1952, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.672312\n",
      "Epoch 21\n",
      "Loss = 2.8918e-04, PNorm = 78.8835, GNorm = 0.1508, lr_0 = 3.8310e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 2.857732\n",
      "Epoch 22\n",
      "Loss = 2.8779e-04, PNorm = 79.5695, GNorm = 0.1108, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.638450\n",
      "Epoch 23\n",
      "Loss = 2.4072e-04, PNorm = 80.1503, GNorm = 0.1596, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.549877\n",
      "Epoch 24\n",
      "Loss = 2.4152e-04, PNorm = 80.7690, GNorm = 0.1796, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.824660\n",
      "Epoch 25\n",
      "Loss = 2.3756e-04, PNorm = 81.3469, GNorm = 0.1121, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.495622\n",
      "Epoch 26\n",
      "Loss = 1.9598e-04, PNorm = 81.7554, GNorm = 0.0827, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.813755\n",
      "Epoch 27\n",
      "Loss = 1.8387e-04, PNorm = 82.2008, GNorm = 0.1231, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.414657\n",
      "Epoch 28\n",
      "Loss = 1.6862e-04, PNorm = 82.5940, GNorm = 0.1038, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.482092\n",
      "Epoch 29\n",
      "Loss = 1.5884e-04, PNorm = 82.9942, GNorm = 0.1192, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.481631\n",
      "Epoch 30\n",
      "Loss = 1.5358e-04, PNorm = 83.3802, GNorm = 0.0949, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.502800\n",
      "Epoch 31\n",
      "Loss = 1.5098e-04, PNorm = 83.7808, GNorm = 0.0797, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.426174\n",
      "Epoch 32\n",
      "Loss = 1.4243e-04, PNorm = 84.1057, GNorm = 0.1036, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.492026\n",
      "Epoch 33\n",
      "Loss = 1.3911e-04, PNorm = 84.3886, GNorm = 0.0730, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.325264\n",
      "Epoch 34\n",
      "Loss = 1.1887e-04, PNorm = 84.6406, GNorm = 0.0767, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.247362\n",
      "Epoch 35\n",
      "Loss = 1.1100e-04, PNorm = 84.8833, GNorm = 0.0813, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.324851\n",
      "Epoch 36\n",
      "Loss = 1.0916e-04, PNorm = 85.0980, GNorm = 0.0959, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.222236\n",
      "Epoch 37\n",
      "Loss = 1.0096e-04, PNorm = 85.3129, GNorm = 0.0790, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.264335\n",
      "Epoch 38\n",
      "Loss = 9.7053e-05, PNorm = 85.5235, GNorm = 0.0535, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.227402\n",
      "Epoch 39\n",
      "Loss = 9.3352e-05, PNorm = 85.7069, GNorm = 0.0967, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.271345\n",
      "Epoch 40\n",
      "Loss = 9.0242e-05, PNorm = 85.8914, GNorm = 0.0588, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.190968\n",
      "Epoch 41\n",
      "Loss = 8.5945e-05, PNorm = 86.0580, GNorm = 0.0992, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.211011\n",
      "Epoch 42\n",
      "Loss = 8.2454e-05, PNorm = 86.2127, GNorm = 0.0515, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.190530\n",
      "Epoch 43\n",
      "Loss = 7.7451e-05, PNorm = 86.3505, GNorm = 0.0485, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.175061\n",
      "Epoch 44\n",
      "Loss = 7.4960e-05, PNorm = 86.4944, GNorm = 0.0665, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.156172\n",
      "Epoch 45\n",
      "Loss = 7.2932e-05, PNorm = 86.6225, GNorm = 0.0475, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.147455\n",
      "Epoch 46\n",
      "Loss = 7.1607e-05, PNorm = 86.7571, GNorm = 0.0574, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.171358\n",
      "Epoch 47\n",
      "Loss = 6.8036e-05, PNorm = 86.8652, GNorm = 0.0517, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.198052\n",
      "Epoch 48\n",
      "Loss = 6.4156e-05, PNorm = 86.9656, GNorm = 0.0416, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.196338\n",
      "Epoch 49\n",
      "Loss = 6.1621e-05, PNorm = 87.0595, GNorm = 0.0580, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.142218\n",
      "Model 2 best validation mae = 2.142218 on epoch 49\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "Model 2, sample 0 test mae = 2.263242\n",
      "Building model 3\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.6867e-03, PNorm = 49.3543, GNorm = 1.6929, lr_0 = 5.5056e-04\n",
      "Validation mae = 14.075143\n",
      "Epoch 1\n",
      "Loss = 3.5511e-03, PNorm = 51.6758, GNorm = 1.6932, lr_0 = 9.9994e-04\n",
      "Validation mae = 11.320375\n",
      "Epoch 2\n",
      "Loss = 2.2192e-03, PNorm = 54.0976, GNorm = 1.1272, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.482575\n",
      "Epoch 3\n",
      "Loss = 1.6349e-03, PNorm = 56.0082, GNorm = 0.6068, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.850963\n",
      "Epoch 4\n",
      "Loss = 1.3563e-03, PNorm = 57.7429, GNorm = 0.4531, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.714741\n",
      "Epoch 5\n",
      "Loss = 1.2070e-03, PNorm = 59.5116, GNorm = 0.3753, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.032113\n",
      "Epoch 6\n",
      "Loss = 1.0938e-03, PNorm = 61.3803, GNorm = 0.9837, lr_0 = 7.8670e-04\n",
      "Validation mae = 6.549669\n",
      "Epoch 7\n",
      "Loss = 9.7374e-04, PNorm = 63.0245, GNorm = 0.4131, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.759948\n",
      "Epoch 8\n",
      "Loss = 8.6859e-04, PNorm = 64.6370, GNorm = 1.0091, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.488930\n",
      "Epoch 9\n",
      "Loss = 7.7641e-04, PNorm = 66.0858, GNorm = 0.4487, lr_0 = 6.8125e-04\n",
      "Validation mae = 3.807726\n",
      "Epoch 10\n",
      "Loss = 7.3747e-04, PNorm = 67.5983, GNorm = 0.4139, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.798297\n",
      "Epoch 11\n",
      "Loss = 6.4832e-04, PNorm = 68.9802, GNorm = 0.2530, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.533286\n",
      "Epoch 12\n",
      "Loss = 6.9427e-04, PNorm = 70.6275, GNorm = 0.2834, lr_0 = 5.8994e-04\n",
      "Validation mae = 4.051918\n",
      "Epoch 13\n",
      "Loss = 5.5513e-04, PNorm = 71.7499, GNorm = 0.5468, lr_0 = 5.6231e-04\n",
      "Validation mae = 4.347200\n",
      "Epoch 14\n",
      "Loss = 4.9696e-04, PNorm = 72.8234, GNorm = 0.1853, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.201518\n",
      "Epoch 15\n",
      "Loss = 5.0849e-04, PNorm = 74.1376, GNorm = 0.1748, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.564629\n",
      "Epoch 16\n",
      "Loss = 4.6252e-04, PNorm = 75.0803, GNorm = 0.1705, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.419692\n",
      "Epoch 17\n",
      "Loss = 4.0905e-04, PNorm = 75.9908, GNorm = 0.1759, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.167566\n",
      "Epoch 18\n",
      "Loss = 4.0028e-04, PNorm = 76.9456, GNorm = 0.1808, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.985725\n",
      "Epoch 19\n",
      "Loss = 3.4091e-04, PNorm = 77.6791, GNorm = 0.1415, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.696127\n",
      "Epoch 20\n",
      "Loss = 3.1558e-04, PNorm = 78.4067, GNorm = 0.1197, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.603941\n",
      "Epoch 21\n",
      "Loss = 3.0350e-04, PNorm = 79.2320, GNorm = 0.1353, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.815857\n",
      "Epoch 22\n",
      "Loss = 2.9165e-04, PNorm = 79.9257, GNorm = 0.1084, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.608202\n",
      "Epoch 23\n",
      "Loss = 2.5628e-04, PNorm = 80.5341, GNorm = 0.1551, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.828170\n",
      "Epoch 24\n",
      "Loss = 2.4739e-04, PNorm = 81.1263, GNorm = 0.1565, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.708417\n",
      "Epoch 25\n",
      "Loss = 2.0955e-04, PNorm = 81.5974, GNorm = 0.1357, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.527948\n",
      "Epoch 26\n",
      "Loss = 2.0036e-04, PNorm = 82.0922, GNorm = 0.1125, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.493362\n",
      "Epoch 27\n",
      "Loss = 1.8879e-04, PNorm = 82.5861, GNorm = 0.1223, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.566285\n",
      "Epoch 28\n",
      "Loss = 1.9259e-04, PNorm = 83.0513, GNorm = 0.1236, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.668887\n",
      "Epoch 29\n",
      "Loss = 1.8021e-04, PNorm = 83.4910, GNorm = 0.1293, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.603362\n",
      "Epoch 30\n",
      "Loss = 1.5919e-04, PNorm = 83.7972, GNorm = 0.1186, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.439143\n",
      "Epoch 31\n",
      "Loss = 1.4312e-04, PNorm = 84.1189, GNorm = 0.1224, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.369433\n",
      "Epoch 32\n",
      "Loss = 1.4263e-04, PNorm = 84.4385, GNorm = 0.0920, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.377989\n",
      "Epoch 33\n",
      "Loss = 1.2572e-04, PNorm = 84.7012, GNorm = 0.0851, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.345362\n",
      "Epoch 34\n",
      "Loss = 1.1705e-04, PNorm = 84.9656, GNorm = 0.0644, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.285624\n",
      "Epoch 35\n",
      "Loss = 1.1068e-04, PNorm = 85.2139, GNorm = 0.0664, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.291260\n",
      "Epoch 36\n",
      "Loss = 1.0434e-04, PNorm = 85.4428, GNorm = 0.1822, lr_0 = 1.8656e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 2.232485\n",
      "Epoch 37\n",
      "Loss = 9.9624e-05, PNorm = 85.6756, GNorm = 0.0633, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.261329\n",
      "Epoch 38\n",
      "Loss = 9.6446e-05, PNorm = 85.8848, GNorm = 0.0682, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.268441\n",
      "Epoch 39\n",
      "Loss = 8.8953e-05, PNorm = 86.0693, GNorm = 0.0734, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.226356\n",
      "Epoch 40\n",
      "Loss = 8.3750e-05, PNorm = 86.2332, GNorm = 0.0626, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.259395\n",
      "Epoch 41\n",
      "Loss = 8.0772e-05, PNorm = 86.4121, GNorm = 0.0944, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.223182\n",
      "Epoch 42\n",
      "Loss = 7.8234e-05, PNorm = 86.5618, GNorm = 0.0573, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.202277\n",
      "Epoch 43\n",
      "Loss = 7.3810e-05, PNorm = 86.7052, GNorm = 0.0535, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.247926\n",
      "Epoch 44\n",
      "Loss = 7.1847e-05, PNorm = 86.8389, GNorm = 0.0493, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.181484\n",
      "Epoch 45\n",
      "Loss = 6.8471e-05, PNorm = 86.9701, GNorm = 0.0533, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.172491\n",
      "Epoch 46\n",
      "Loss = 6.6110e-05, PNorm = 87.0865, GNorm = 0.0615, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.200782\n",
      "Epoch 47\n",
      "Loss = 6.3333e-05, PNorm = 87.2031, GNorm = 0.0547, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.178014\n",
      "Epoch 48\n",
      "Loss = 6.2142e-05, PNorm = 87.3058, GNorm = 0.0440, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.171775\n",
      "Epoch 49\n",
      "Loss = 5.9902e-05, PNorm = 87.4103, GNorm = 0.0370, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.163329\n",
      "Model 3 best validation mae = 2.163329 on epoch 49\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "Model 3, sample 0 test mae = 2.248590\n",
      "Building model 4\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7299e-03, PNorm = 49.3007, GNorm = 1.9231, lr_0 = 5.5056e-04\n",
      "Validation mae = 12.939214\n",
      "Epoch 1\n",
      "Loss = 3.3329e-03, PNorm = 51.5027, GNorm = 1.0661, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.631859\n",
      "Epoch 2\n",
      "Loss = 2.1836e-03, PNorm = 53.9524, GNorm = 0.9716, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.737084\n",
      "Epoch 3\n",
      "Loss = 1.5935e-03, PNorm = 55.8498, GNorm = 1.0225, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.759793\n",
      "Epoch 4\n",
      "Loss = 1.3293e-03, PNorm = 57.6926, GNorm = 0.3918, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.240013\n",
      "Epoch 5\n",
      "Loss = 1.2345e-03, PNorm = 59.7122, GNorm = 0.8956, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.855725\n",
      "Epoch 6\n",
      "Loss = 1.0484e-03, PNorm = 61.2861, GNorm = 0.6647, lr_0 = 7.8670e-04\n",
      "Validation mae = 5.067825\n",
      "Epoch 7\n",
      "Loss = 9.3548e-04, PNorm = 62.8800, GNorm = 0.4044, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.917915\n",
      "Epoch 8\n",
      "Loss = 9.0515e-04, PNorm = 64.7843, GNorm = 0.3550, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.863127\n",
      "Epoch 9\n",
      "Loss = 7.9125e-04, PNorm = 66.3464, GNorm = 0.2149, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.162275\n",
      "Epoch 10\n",
      "Loss = 7.3757e-04, PNorm = 67.8275, GNorm = 0.2170, lr_0 = 6.4934e-04\n",
      "Validation mae = 3.633394\n",
      "Epoch 11\n",
      "Loss = 6.3582e-04, PNorm = 69.1734, GNorm = 0.5324, lr_0 = 6.1893e-04\n",
      "Validation mae = 4.996988\n",
      "Epoch 12\n",
      "Loss = 6.2377e-04, PNorm = 70.5443, GNorm = 0.1907, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.517883\n",
      "Epoch 13\n",
      "Loss = 5.4605e-04, PNorm = 71.7914, GNorm = 0.3246, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.777729\n",
      "Epoch 14\n",
      "Loss = 5.3059e-04, PNorm = 73.1108, GNorm = 0.2584, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.135321\n",
      "Epoch 15\n",
      "Loss = 4.7231e-04, PNorm = 74.2206, GNorm = 0.1418, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.115048\n",
      "Epoch 16\n",
      "Loss = 4.3241e-04, PNorm = 75.1941, GNorm = 0.2038, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.368717\n",
      "Epoch 17\n",
      "Loss = 3.9645e-04, PNorm = 76.1298, GNorm = 0.1346, lr_0 = 4.6413e-04\n",
      "Validation mae = 2.972340\n",
      "Epoch 18\n",
      "Loss = 3.7969e-04, PNorm = 77.0891, GNorm = 0.1749, lr_0 = 4.4239e-04\n",
      "Validation mae = 3.187413\n",
      "Epoch 19\n",
      "Loss = 3.3760e-04, PNorm = 77.8773, GNorm = 0.1530, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.686576\n",
      "Epoch 20\n",
      "Loss = 3.1899e-04, PNorm = 78.6992, GNorm = 0.1074, lr_0 = 4.0192e-04\n",
      "Validation mae = 3.450125\n",
      "Epoch 21\n",
      "Loss = 3.2854e-04, PNorm = 79.5566, GNorm = 0.1426, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.646775\n",
      "Epoch 22\n",
      "Loss = 2.8852e-04, PNorm = 80.1952, GNorm = 0.2217, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.659080\n",
      "Epoch 23\n",
      "Loss = 2.4176e-04, PNorm = 80.7120, GNorm = 0.1032, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.476167\n",
      "Epoch 24\n",
      "Loss = 2.2833e-04, PNorm = 81.2203, GNorm = 0.1010, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.808175\n",
      "Epoch 25\n",
      "Loss = 2.2102e-04, PNorm = 81.7947, GNorm = 0.0965, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.580488\n",
      "Epoch 26\n",
      "Loss = 2.0110e-04, PNorm = 82.2730, GNorm = 0.1738, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.469493\n",
      "Epoch 27\n",
      "Loss = 1.8481e-04, PNorm = 82.6974, GNorm = 0.0902, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.542224\n",
      "Epoch 28\n",
      "Loss = 1.7221e-04, PNorm = 83.1058, GNorm = 0.0889, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.387923\n",
      "Epoch 29\n",
      "Loss = 1.6714e-04, PNorm = 83.5004, GNorm = 0.0888, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.339556\n",
      "Epoch 30\n",
      "Loss = 1.5924e-04, PNorm = 83.9198, GNorm = 0.1011, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.529985\n",
      "Epoch 31\n",
      "Loss = 1.5382e-04, PNorm = 84.2407, GNorm = 0.0848, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.366261\n",
      "Epoch 32\n",
      "Loss = 1.2918e-04, PNorm = 84.5333, GNorm = 0.0746, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.330988\n",
      "Epoch 33\n",
      "Loss = 1.2625e-04, PNorm = 84.7977, GNorm = 0.0857, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.394289\n",
      "Epoch 34\n",
      "Loss = 1.2342e-04, PNorm = 85.0842, GNorm = 0.0649, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.330647\n",
      "Epoch 35\n",
      "Loss = 1.1785e-04, PNorm = 85.3428, GNorm = 0.0648, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.364180\n",
      "Epoch 36\n",
      "Loss = 1.0580e-04, PNorm = 85.5625, GNorm = 0.0618, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.278730\n",
      "Epoch 37\n",
      "Loss = 9.9978e-05, PNorm = 85.7715, GNorm = 0.0687, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.318154\n",
      "Epoch 38\n",
      "Loss = 9.4069e-05, PNorm = 85.9662, GNorm = 0.3138, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.341800\n",
      "Epoch 39\n",
      "Loss = 9.3111e-05, PNorm = 86.1734, GNorm = 0.0740, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.238412\n",
      "Epoch 40\n",
      "Loss = 8.6388e-05, PNorm = 86.3545, GNorm = 0.0438, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.221232\n",
      "Epoch 41\n",
      "Loss = 8.2125e-05, PNorm = 86.5227, GNorm = 0.0989, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.240037\n",
      "Epoch 42\n",
      "Loss = 7.7930e-05, PNorm = 86.6647, GNorm = 0.0466, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.183207\n",
      "Epoch 43\n",
      "Loss = 7.5870e-05, PNorm = 86.8154, GNorm = 0.0544, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.269252\n",
      "Epoch 44\n",
      "Loss = 7.5009e-05, PNorm = 86.9645, GNorm = 0.0842, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.208096\n",
      "Epoch 45\n",
      "Loss = 7.0469e-05, PNorm = 87.0859, GNorm = 0.0422, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.173153\n",
      "Epoch 46\n",
      "Loss = 6.6124e-05, PNorm = 87.2025, GNorm = 0.0503, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.214260\n",
      "Epoch 47\n",
      "Loss = 6.4319e-05, PNorm = 87.3177, GNorm = 0.0441, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.183809\n",
      "Epoch 48\n",
      "Loss = 6.1949e-05, PNorm = 87.4222, GNorm = 0.0466, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.172339\n",
      "Epoch 49\n",
      "Loss = 5.9497e-05, PNorm = 87.5214, GNorm = 0.0880, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.185397\n",
      "Model 4 best validation mae = 2.172339 on epoch 48\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4, sample 0 test mae = 2.241527\n",
      "Building model 5\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7953e-03, PNorm = 49.3925, GNorm = 1.9692, lr_0 = 5.5056e-04\n",
      "Validation mae = 12.866874\n",
      "Epoch 1\n",
      "Loss = 3.4457e-03, PNorm = 51.7070, GNorm = 0.9029, lr_0 = 9.9994e-04\n",
      "Validation mae = 12.724946\n",
      "Epoch 2\n",
      "Loss = 2.2025e-03, PNorm = 54.1068, GNorm = 0.7117, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.312783\n",
      "Epoch 3\n",
      "Loss = 1.5844e-03, PNorm = 55.8004, GNorm = 0.7259, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.647182\n",
      "Epoch 4\n",
      "Loss = 1.3561e-03, PNorm = 57.5276, GNorm = 0.4084, lr_0 = 8.6591e-04\n",
      "Validation mae = 4.780144\n",
      "Epoch 5\n",
      "Loss = 1.1726e-03, PNorm = 59.1170, GNorm = 0.3824, lr_0 = 8.2535e-04\n",
      "Validation mae = 4.967569\n",
      "Epoch 6\n",
      "Loss = 1.0385e-03, PNorm = 60.7593, GNorm = 0.3254, lr_0 = 7.8670e-04\n",
      "Validation mae = 5.198650\n",
      "Epoch 7\n",
      "Loss = 9.4522e-04, PNorm = 62.4627, GNorm = 0.5840, lr_0 = 7.4985e-04\n",
      "Validation mae = 5.265455\n",
      "Epoch 8\n",
      "Loss = 8.6637e-04, PNorm = 64.0246, GNorm = 0.2677, lr_0 = 7.1473e-04\n",
      "Validation mae = 3.753106\n",
      "Epoch 9\n",
      "Loss = 7.6708e-04, PNorm = 65.4367, GNorm = 0.4773, lr_0 = 6.8125e-04\n",
      "Validation mae = 5.050859\n",
      "Epoch 10\n",
      "Loss = 7.0821e-04, PNorm = 66.8844, GNorm = 0.4300, lr_0 = 6.4934e-04\n",
      "Validation mae = 5.033571\n",
      "Epoch 11\n",
      "Loss = 7.1083e-04, PNorm = 68.4718, GNorm = 0.2475, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.729628\n",
      "Epoch 12\n",
      "Loss = 6.0574e-04, PNorm = 69.6991, GNorm = 0.2259, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.369846\n",
      "Epoch 13\n",
      "Loss = 5.4789e-04, PNorm = 70.8696, GNorm = 0.2143, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.573152\n",
      "Epoch 14\n",
      "Loss = 5.1981e-04, PNorm = 72.1130, GNorm = 0.1431, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.370367\n",
      "Epoch 15\n",
      "Loss = 4.8344e-04, PNorm = 73.2925, GNorm = 0.2077, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.203270\n",
      "Epoch 16\n",
      "Loss = 4.6479e-04, PNorm = 74.4290, GNorm = 0.1683, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.197090\n",
      "Epoch 17\n",
      "Loss = 3.9901e-04, PNorm = 75.2990, GNorm = 0.1318, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.020080\n",
      "Epoch 18\n",
      "Loss = 3.6797e-04, PNorm = 76.1448, GNorm = 0.1655, lr_0 = 4.4239e-04\n",
      "Validation mae = 3.026773\n",
      "Epoch 19\n",
      "Loss = 3.5891e-04, PNorm = 77.0626, GNorm = 0.4388, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.815546\n",
      "Epoch 20\n",
      "Loss = 3.2428e-04, PNorm = 77.8162, GNorm = 0.1360, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.793206\n",
      "Epoch 21\n",
      "Loss = 2.9320e-04, PNorm = 78.5380, GNorm = 0.1639, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.971867\n",
      "Epoch 22\n",
      "Loss = 2.7526e-04, PNorm = 79.2226, GNorm = 0.1527, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.855442\n",
      "Epoch 23\n",
      "Loss = 2.5227e-04, PNorm = 79.8035, GNorm = 0.1284, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.762507\n",
      "Epoch 24\n",
      "Loss = 2.2309e-04, PNorm = 80.3155, GNorm = 0.1054, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.631035\n",
      "Epoch 25\n",
      "Loss = 2.0468e-04, PNorm = 80.8451, GNorm = 0.1356, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.569247\n",
      "Epoch 26\n",
      "Loss = 1.8567e-04, PNorm = 81.2856, GNorm = 0.1179, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.462601\n",
      "Epoch 27\n",
      "Loss = 1.7702e-04, PNorm = 81.7737, GNorm = 0.1550, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.456837\n",
      "Epoch 28\n",
      "Loss = 1.7172e-04, PNorm = 82.2349, GNorm = 0.0801, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.474818\n",
      "Epoch 29\n",
      "Loss = 1.5588e-04, PNorm = 82.6030, GNorm = 0.0771, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.428774\n",
      "Epoch 30\n",
      "Loss = 1.7227e-04, PNorm = 83.0491, GNorm = 0.1325, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.496064\n",
      "Epoch 31\n",
      "Loss = 1.4035e-04, PNorm = 83.3461, GNorm = 0.0855, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.331780\n",
      "Epoch 32\n",
      "Loss = 1.2913e-04, PNorm = 83.6374, GNorm = 0.1075, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.354101\n",
      "Epoch 33\n",
      "Loss = 1.2088e-04, PNorm = 83.9222, GNorm = 0.0705, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.294732\n",
      "Epoch 34\n",
      "Loss = 1.1254e-04, PNorm = 84.1752, GNorm = 0.0689, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.274794\n",
      "Epoch 35\n",
      "Loss = 1.0703e-04, PNorm = 84.4116, GNorm = 0.0771, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.313783\n",
      "Epoch 36\n",
      "Loss = 1.0464e-04, PNorm = 84.6486, GNorm = 0.0706, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.271961\n",
      "Epoch 37\n",
      "Loss = 9.8738e-05, PNorm = 84.8726, GNorm = 0.0791, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.263354\n",
      "Epoch 38\n",
      "Loss = 9.3438e-05, PNorm = 85.0754, GNorm = 0.0765, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.233526\n",
      "Epoch 39\n",
      "Loss = 9.0068e-05, PNorm = 85.2648, GNorm = 0.0621, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.226005\n",
      "Epoch 40\n",
      "Loss = 8.4814e-05, PNorm = 85.4460, GNorm = 0.0768, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.256456\n",
      "Epoch 41\n",
      "Loss = 7.9353e-05, PNorm = 85.5870, GNorm = 0.3782, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.320475\n",
      "Epoch 42\n",
      "Loss = 7.6783e-05, PNorm = 85.7419, GNorm = 0.0598, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.224270\n",
      "Epoch 43\n",
      "Loss = 7.3021e-05, PNorm = 85.8848, GNorm = 0.0604, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.178691\n",
      "Epoch 44\n",
      "Loss = 7.2294e-05, PNorm = 86.0307, GNorm = 0.0674, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.184956\n",
      "Epoch 45\n",
      "Loss = 6.9527e-05, PNorm = 86.1569, GNorm = 0.0525, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.162158\n",
      "Epoch 46\n",
      "Loss = 6.4521e-05, PNorm = 86.2732, GNorm = 0.0355, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.156419\n",
      "Epoch 47\n",
      "Loss = 6.2373e-05, PNorm = 86.3812, GNorm = 0.0525, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.155973\n",
      "Epoch 48\n",
      "Loss = 6.1052e-05, PNorm = 86.4915, GNorm = 0.0591, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.262571\n",
      "Epoch 49\n",
      "Loss = 5.9319e-05, PNorm = 86.5850, GNorm = 0.0684, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.189549\n",
      "Model 5 best validation mae = 2.155973 on epoch 47\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "Model 5, sample 0 test mae = 2.175561\n",
      "Building model 6\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.8189e-03, PNorm = 49.2493, GNorm = 2.2114, lr_0 = 5.5056e-04\n",
      "Validation mae = 12.681904\n",
      "Epoch 1\n",
      "Loss = 3.5258e-03, PNorm = 51.5208, GNorm = 1.3601, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.064073\n",
      "Epoch 2\n",
      "Loss = 2.2022e-03, PNorm = 53.8459, GNorm = 0.7931, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.549739\n",
      "Epoch 3\n",
      "Loss = 1.6384e-03, PNorm = 55.7299, GNorm = 0.9344, lr_0 = 9.0846e-04\n",
      "Validation mae = 6.284680\n",
      "Epoch 4\n",
      "Loss = 1.3426e-03, PNorm = 57.4465, GNorm = 0.5540, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.622361\n",
      "Epoch 5\n",
      "Loss = 1.1999e-03, PNorm = 59.1494, GNorm = 0.3808, lr_0 = 8.2535e-04\n",
      "Validation mae = 4.956992\n",
      "Epoch 6\n",
      "Loss = 1.0949e-03, PNorm = 61.0623, GNorm = 0.6410, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.535388\n",
      "Epoch 7\n",
      "Loss = 9.9198e-04, PNorm = 62.7439, GNorm = 0.3505, lr_0 = 7.4985e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 4.426510\n",
      "Epoch 8\n",
      "Loss = 8.6710e-04, PNorm = 64.2532, GNorm = 0.4474, lr_0 = 7.1473e-04\n",
      "Validation mae = 5.405482\n",
      "Epoch 9\n",
      "Loss = 8.3322e-04, PNorm = 65.8522, GNorm = 0.1829, lr_0 = 6.8125e-04\n",
      "Validation mae = 3.819545\n",
      "Epoch 10\n",
      "Loss = 7.5587e-04, PNorm = 67.3155, GNorm = 0.2051, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.129224\n",
      "Epoch 11\n",
      "Loss = 6.6423e-04, PNorm = 68.6844, GNorm = 0.3013, lr_0 = 6.1893e-04\n",
      "Validation mae = 4.077426\n",
      "Epoch 12\n",
      "Loss = 6.3004e-04, PNorm = 69.9502, GNorm = 0.2445, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.848214\n",
      "Epoch 13\n",
      "Loss = 5.7535e-04, PNorm = 71.2397, GNorm = 0.2153, lr_0 = 5.6231e-04\n",
      "Validation mae = 4.128159\n",
      "Epoch 14\n",
      "Loss = 5.5585e-04, PNorm = 72.4692, GNorm = 0.1844, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.338254\n",
      "Epoch 15\n",
      "Loss = 4.8000e-04, PNorm = 73.5413, GNorm = 0.2090, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.507303\n",
      "Epoch 16\n",
      "Loss = 4.7496e-04, PNorm = 74.6861, GNorm = 0.1358, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.133285\n",
      "Epoch 17\n",
      "Loss = 4.1602e-04, PNorm = 75.5600, GNorm = 0.3413, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.394383\n",
      "Epoch 18\n",
      "Loss = 4.0362e-04, PNorm = 76.5028, GNorm = 0.2022, lr_0 = 4.4239e-04\n",
      "Validation mae = 3.481586\n",
      "Epoch 19\n",
      "Loss = 3.5483e-04, PNorm = 77.2901, GNorm = 0.0966, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.924640\n",
      "Epoch 20\n",
      "Loss = 3.1280e-04, PNorm = 77.9948, GNorm = 0.1300, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.828060\n",
      "Epoch 21\n",
      "Loss = 3.1726e-04, PNorm = 78.8287, GNorm = 0.1154, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.874361\n",
      "Epoch 22\n",
      "Loss = 2.8934e-04, PNorm = 79.4942, GNorm = 0.1405, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.902904\n",
      "Epoch 23\n",
      "Loss = 2.6326e-04, PNorm = 80.0898, GNorm = 0.1017, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.723482\n",
      "Epoch 24\n",
      "Loss = 2.6833e-04, PNorm = 80.7637, GNorm = 0.2399, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.810721\n",
      "Epoch 25\n",
      "Loss = 2.2606e-04, PNorm = 81.2247, GNorm = 0.1259, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.561226\n",
      "Epoch 26\n",
      "Loss = 2.0516e-04, PNorm = 81.6623, GNorm = 0.1467, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.635190\n",
      "Epoch 27\n",
      "Loss = 1.9371e-04, PNorm = 82.1025, GNorm = 0.1326, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.679809\n",
      "Epoch 28\n",
      "Loss = 1.8339e-04, PNorm = 82.5356, GNorm = 0.0866, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.496910\n",
      "Epoch 29\n",
      "Loss = 1.7206e-04, PNorm = 82.9284, GNorm = 0.1262, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.689768\n",
      "Epoch 30\n",
      "Loss = 1.5863e-04, PNorm = 83.2985, GNorm = 0.1300, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.503490\n",
      "Epoch 31\n",
      "Loss = 1.4503e-04, PNorm = 83.6214, GNorm = 0.0832, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.451299\n",
      "Epoch 32\n",
      "Loss = 1.4136e-04, PNorm = 83.9468, GNorm = 0.0936, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.488240\n",
      "Epoch 33\n",
      "Loss = 1.2687e-04, PNorm = 84.2284, GNorm = 0.1049, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.436002\n",
      "Epoch 34\n",
      "Loss = 1.1595e-04, PNorm = 84.4799, GNorm = 0.0799, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.360767\n",
      "Epoch 35\n",
      "Loss = 1.1084e-04, PNorm = 84.7432, GNorm = 0.0843, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.359097\n",
      "Epoch 36\n",
      "Loss = 1.0549e-04, PNorm = 84.9706, GNorm = 0.3218, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.414735\n",
      "Epoch 37\n",
      "Loss = 9.9152e-05, PNorm = 85.1964, GNorm = 0.0682, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.351719\n",
      "Epoch 38\n",
      "Loss = 9.5275e-05, PNorm = 85.4063, GNorm = 0.0670, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.328234\n",
      "Epoch 39\n",
      "Loss = 8.8761e-05, PNorm = 85.5858, GNorm = 0.0777, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.356705\n",
      "Epoch 40\n",
      "Loss = 8.6007e-05, PNorm = 85.7707, GNorm = 0.1191, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.321996\n",
      "Epoch 41\n",
      "Loss = 8.2805e-05, PNorm = 85.9370, GNorm = 0.0711, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.340128\n",
      "Epoch 42\n",
      "Loss = 7.7813e-05, PNorm = 86.0824, GNorm = 0.0543, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.246936\n",
      "Epoch 43\n",
      "Loss = 7.5711e-05, PNorm = 86.2263, GNorm = 0.0551, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.288978\n",
      "Epoch 44\n",
      "Loss = 7.2044e-05, PNorm = 86.3639, GNorm = 0.0700, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.277837\n",
      "Epoch 45\n",
      "Loss = 6.8484e-05, PNorm = 86.4868, GNorm = 0.0471, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.262624\n",
      "Epoch 46\n",
      "Loss = 6.6886e-05, PNorm = 86.6032, GNorm = 0.0593, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.246034\n",
      "Epoch 47\n",
      "Loss = 6.3876e-05, PNorm = 86.7205, GNorm = 0.1005, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.261150\n",
      "Epoch 48\n",
      "Loss = 6.1950e-05, PNorm = 86.8197, GNorm = 0.0491, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.252308\n",
      "Epoch 49\n",
      "Loss = 6.0585e-05, PNorm = 86.9256, GNorm = 0.0466, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.217660\n",
      "Model 6 best validation mae = 2.217660 on epoch 49\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "Model 6, sample 0 test mae = 2.249616\n",
      "Building model 7\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7148e-03, PNorm = 49.3393, GNorm = 1.3623, lr_0 = 5.5056e-04\n",
      "Validation mae = 12.559459\n",
      "Epoch 1\n",
      "Loss = 3.4804e-03, PNorm = 51.6999, GNorm = 1.7920, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.468437\n",
      "Epoch 2\n",
      "Loss = 2.2162e-03, PNorm = 54.1969, GNorm = 0.6527, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.842547\n",
      "Epoch 3\n",
      "Loss = 1.6297e-03, PNorm = 56.0715, GNorm = 0.7142, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.891534\n",
      "Epoch 4\n",
      "Loss = 1.4043e-03, PNorm = 58.0252, GNorm = 0.7918, lr_0 = 8.6591e-04\n",
      "Validation mae = 6.196108\n",
      "Epoch 5\n",
      "Loss = 1.1797e-03, PNorm = 59.6588, GNorm = 0.3953, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.041596\n",
      "Epoch 6\n",
      "Loss = 1.0511e-03, PNorm = 61.3090, GNorm = 0.2787, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.885209\n",
      "Epoch 7\n",
      "Loss = 9.4484e-04, PNorm = 62.9564, GNorm = 0.7352, lr_0 = 7.4985e-04\n",
      "Validation mae = 5.133555\n",
      "Epoch 8\n",
      "Loss = 9.3230e-04, PNorm = 64.8038, GNorm = 0.3331, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.587062\n",
      "Epoch 9\n",
      "Loss = 7.7439e-04, PNorm = 66.2551, GNorm = 0.3622, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.036087\n",
      "Epoch 10\n",
      "Loss = 7.2608e-04, PNorm = 67.7661, GNorm = 0.2110, lr_0 = 6.4934e-04\n",
      "Validation mae = 3.419897\n",
      "Epoch 11\n",
      "Loss = 6.6514e-04, PNorm = 69.1916, GNorm = 0.1959, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.823348\n",
      "Epoch 12\n",
      "Loss = 6.0942e-04, PNorm = 70.5869, GNorm = 0.1849, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.330877\n",
      "Epoch 13\n",
      "Loss = 5.8222e-04, PNorm = 71.9386, GNorm = 0.4365, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.836752\n",
      "Epoch 14\n",
      "Loss = 5.4009e-04, PNorm = 73.2008, GNorm = 0.3118, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.741560\n",
      "Epoch 15\n",
      "Loss = 4.8893e-04, PNorm = 74.3341, GNorm = 0.1202, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.353285\n",
      "Epoch 16\n",
      "Loss = 4.2881e-04, PNorm = 75.3130, GNorm = 0.1914, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.235600\n",
      "Epoch 17\n",
      "Loss = 4.2391e-04, PNorm = 76.3644, GNorm = 0.1479, lr_0 = 4.6413e-04\n",
      "Validation mae = 2.949946\n",
      "Epoch 18\n",
      "Loss = 3.6319e-04, PNorm = 77.1463, GNorm = 0.1689, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.957494\n",
      "Epoch 19\n",
      "Loss = 3.3537e-04, PNorm = 78.0020, GNorm = 0.5075, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.001336\n",
      "Epoch 20\n",
      "Loss = 3.3128e-04, PNorm = 78.8274, GNorm = 0.0947, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.801361\n",
      "Epoch 21\n",
      "Loss = 3.2034e-04, PNorm = 79.6687, GNorm = 0.0977, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.781325\n",
      "Epoch 22\n",
      "Loss = 2.7837e-04, PNorm = 80.2900, GNorm = 0.1299, lr_0 = 3.6515e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 2.739078\n",
      "Epoch 23\n",
      "Loss = 2.5100e-04, PNorm = 80.9029, GNorm = 0.1009, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.988307\n",
      "Epoch 24\n",
      "Loss = 2.3028e-04, PNorm = 81.4098, GNorm = 0.1143, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.694705\n",
      "Epoch 25\n",
      "Loss = 2.1948e-04, PNorm = 81.9356, GNorm = 0.1075, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.628043\n",
      "Epoch 26\n",
      "Loss = 1.9376e-04, PNorm = 82.4154, GNorm = 0.1076, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.505768\n",
      "Epoch 27\n",
      "Loss = 1.8753e-04, PNorm = 82.8811, GNorm = 0.0833, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.579692\n",
      "Epoch 28\n",
      "Loss = 1.6645e-04, PNorm = 83.3062, GNorm = 0.1203, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.466624\n",
      "Epoch 29\n",
      "Loss = 1.5798e-04, PNorm = 83.6827, GNorm = 0.0725, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.455806\n",
      "Epoch 30\n",
      "Loss = 1.4892e-04, PNorm = 84.0552, GNorm = 0.0967, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.433077\n",
      "Epoch 31\n",
      "Loss = 1.4259e-04, PNorm = 84.4070, GNorm = 0.0745, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.322251\n",
      "Epoch 32\n",
      "Loss = 1.2882e-04, PNorm = 84.7360, GNorm = 0.0817, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.333538\n",
      "Epoch 33\n",
      "Loss = 1.1945e-04, PNorm = 85.0225, GNorm = 0.0785, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.327775\n",
      "Epoch 34\n",
      "Loss = 1.1571e-04, PNorm = 85.3040, GNorm = 0.0972, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.341382\n",
      "Epoch 35\n",
      "Loss = 1.0799e-04, PNorm = 85.5753, GNorm = 0.0701, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.284838\n",
      "Epoch 36\n",
      "Loss = 1.0341e-04, PNorm = 85.8095, GNorm = 0.0769, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.300465\n",
      "Epoch 37\n",
      "Loss = 9.8077e-05, PNorm = 86.0451, GNorm = 0.0659, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.289267\n",
      "Epoch 38\n",
      "Loss = 9.6723e-05, PNorm = 86.2597, GNorm = 0.0650, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.286580\n",
      "Epoch 39\n",
      "Loss = 8.7583e-05, PNorm = 86.4534, GNorm = 0.0840, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.243313\n",
      "Epoch 40\n",
      "Loss = 8.2798e-05, PNorm = 86.6304, GNorm = 0.0578, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.198579\n",
      "Epoch 41\n",
      "Loss = 7.9443e-05, PNorm = 86.8002, GNorm = 0.0690, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.253060\n",
      "Epoch 42\n",
      "Loss = 7.6846e-05, PNorm = 86.9635, GNorm = 0.0555, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.221399\n",
      "Epoch 43\n",
      "Loss = 7.3811e-05, PNorm = 87.1100, GNorm = 0.0762, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.205140\n",
      "Epoch 44\n",
      "Loss = 7.0247e-05, PNorm = 87.2494, GNorm = 0.0533, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.204895\n",
      "Epoch 45\n",
      "Loss = 6.8185e-05, PNorm = 87.3851, GNorm = 0.0761, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.178685\n",
      "Epoch 46\n",
      "Loss = 6.5495e-05, PNorm = 87.5090, GNorm = 0.0519, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.186053\n",
      "Epoch 47\n",
      "Loss = 6.3528e-05, PNorm = 87.6239, GNorm = 0.0457, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.165956\n",
      "Epoch 48\n",
      "Loss = 6.0927e-05, PNorm = 87.7318, GNorm = 0.0500, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.175725\n",
      "Epoch 49\n",
      "Loss = 5.9291e-05, PNorm = 87.8343, GNorm = 0.0531, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.162076\n",
      "Model 7 best validation mae = 2.162076 on epoch 49\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "Model 7, sample 0 test mae = 2.293274\n",
      "Building model 8\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.7632e-03, PNorm = 49.2423, GNorm = 1.3291, lr_0 = 5.5056e-04\n",
      "Validation mae = 11.877836\n",
      "Epoch 1\n",
      "Loss = 3.3685e-03, PNorm = 51.5944, GNorm = 1.8770, lr_0 = 9.9994e-04\n",
      "Validation mae = 10.580242\n",
      "Epoch 2\n",
      "Loss = 2.1585e-03, PNorm = 54.0512, GNorm = 1.1206, lr_0 = 9.5310e-04\n",
      "Validation mae = 6.901608\n",
      "Epoch 3\n",
      "Loss = 1.5400e-03, PNorm = 55.8501, GNorm = 0.6169, lr_0 = 9.0846e-04\n",
      "Validation mae = 5.710994\n",
      "Epoch 4\n",
      "Loss = 1.3293e-03, PNorm = 57.7011, GNorm = 0.6124, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.529131\n",
      "Epoch 5\n",
      "Loss = 1.1792e-03, PNorm = 59.5563, GNorm = 0.5467, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.511896\n",
      "Epoch 6\n",
      "Loss = 1.0697e-03, PNorm = 61.3287, GNorm = 0.5042, lr_0 = 7.8670e-04\n",
      "Validation mae = 6.672497\n",
      "Epoch 7\n",
      "Loss = 9.2965e-04, PNorm = 62.9607, GNorm = 0.4567, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.610831\n",
      "Epoch 8\n",
      "Loss = 8.4600e-04, PNorm = 64.5272, GNorm = 0.2707, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.562529\n",
      "Epoch 9\n",
      "Loss = 7.8712e-04, PNorm = 66.0759, GNorm = 0.2099, lr_0 = 6.8125e-04\n",
      "Validation mae = 3.956066\n",
      "Epoch 10\n",
      "Loss = 7.2926e-04, PNorm = 67.6061, GNorm = 0.1674, lr_0 = 6.4934e-04\n",
      "Validation mae = 3.990831\n",
      "Epoch 11\n",
      "Loss = 6.5679e-04, PNorm = 68.9856, GNorm = 0.2090, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.664399\n",
      "Epoch 12\n",
      "Loss = 5.8114e-04, PNorm = 70.2261, GNorm = 0.5438, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.427495\n",
      "Epoch 13\n",
      "Loss = 5.8154e-04, PNorm = 71.7097, GNorm = 0.1833, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.733661\n",
      "Epoch 14\n",
      "Loss = 5.3222e-04, PNorm = 72.8607, GNorm = 0.3466, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.559830\n",
      "Epoch 15\n",
      "Loss = 4.7700e-04, PNorm = 73.9277, GNorm = 0.1614, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.775549\n",
      "Epoch 16\n",
      "Loss = 4.7642e-04, PNorm = 75.1071, GNorm = 0.5808, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.338985\n",
      "Epoch 17\n",
      "Loss = 3.9233e-04, PNorm = 75.9294, GNorm = 0.2825, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.012032\n",
      "Epoch 18\n",
      "Loss = 3.6472e-04, PNorm = 76.8162, GNorm = 0.2943, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.864384\n",
      "Epoch 19\n",
      "Loss = 3.4186e-04, PNorm = 77.6533, GNorm = 0.3126, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.494301\n",
      "Epoch 20\n",
      "Loss = 3.8646e-04, PNorm = 78.6772, GNorm = 0.1984, lr_0 = 4.0192e-04\n",
      "Validation mae = 2.805420\n",
      "Epoch 21\n",
      "Loss = 2.8168e-04, PNorm = 79.2305, GNorm = 0.1169, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.669302\n",
      "Epoch 22\n",
      "Loss = 2.5219e-04, PNorm = 79.8159, GNorm = 0.1097, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.816454\n",
      "Epoch 23\n",
      "Loss = 2.3862e-04, PNorm = 80.3875, GNorm = 0.1390, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.627524\n",
      "Epoch 24\n",
      "Loss = 2.3706e-04, PNorm = 81.0359, GNorm = 0.1659, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.966326\n",
      "Epoch 25\n",
      "Loss = 2.1496e-04, PNorm = 81.5789, GNorm = 0.1475, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.628231\n",
      "Epoch 26\n",
      "Loss = 1.9442e-04, PNorm = 82.0509, GNorm = 0.0835, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.502488\n",
      "Epoch 27\n",
      "Loss = 1.9322e-04, PNorm = 82.5525, GNorm = 0.0842, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.436989\n",
      "Epoch 28\n",
      "Loss = 1.6453e-04, PNorm = 82.9442, GNorm = 0.1044, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.476921\n",
      "Epoch 29\n",
      "Loss = 1.5779e-04, PNorm = 83.3349, GNorm = 0.0942, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.495567\n",
      "Epoch 30\n",
      "Loss = 1.4584e-04, PNorm = 83.6811, GNorm = 0.2587, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.389227\n",
      "Epoch 31\n",
      "Loss = 1.3165e-04, PNorm = 84.0057, GNorm = 0.1157, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.354085\n",
      "Epoch 32\n",
      "Loss = 1.2511e-04, PNorm = 84.2971, GNorm = 0.0981, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.351738\n",
      "Epoch 33\n",
      "Loss = 1.1734e-04, PNorm = 84.5960, GNorm = 0.0899, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.275222\n",
      "Epoch 34\n",
      "Loss = 1.1104e-04, PNorm = 84.8830, GNorm = 0.1111, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.327878\n",
      "Epoch 35\n",
      "Loss = 1.0783e-04, PNorm = 85.1357, GNorm = 0.0959, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.441385\n",
      "Epoch 36\n",
      "Loss = 9.9556e-05, PNorm = 85.3770, GNorm = 0.0571, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.244390\n",
      "Epoch 37\n",
      "Loss = 9.3041e-05, PNorm = 85.5881, GNorm = 0.0486, lr_0 = 1.7782e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mae = 2.201722\n",
      "Epoch 38\n",
      "Loss = 8.7651e-05, PNorm = 85.7977, GNorm = 0.0694, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.224638\n",
      "Epoch 39\n",
      "Loss = 8.4109e-05, PNorm = 85.9912, GNorm = 0.1283, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.229662\n",
      "Epoch 40\n",
      "Loss = 7.9966e-05, PNorm = 86.1651, GNorm = 0.0552, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.242079\n",
      "Epoch 41\n",
      "Loss = 7.6757e-05, PNorm = 86.3285, GNorm = 0.0546, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.226668\n",
      "Epoch 42\n",
      "Loss = 7.2211e-05, PNorm = 86.4905, GNorm = 0.0583, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.166677\n",
      "Epoch 43\n",
      "Loss = 6.9896e-05, PNorm = 86.6291, GNorm = 0.0561, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.171773\n",
      "Epoch 44\n",
      "Loss = 6.7713e-05, PNorm = 86.7652, GNorm = 0.0644, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.202423\n",
      "Epoch 45\n",
      "Loss = 6.4896e-05, PNorm = 86.8964, GNorm = 0.0487, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.167416\n",
      "Epoch 46\n",
      "Loss = 6.2643e-05, PNorm = 87.0152, GNorm = 0.1010, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.210120\n",
      "Epoch 47\n",
      "Loss = 6.0769e-05, PNorm = 87.1299, GNorm = 0.0556, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.155574\n",
      "Epoch 48\n",
      "Loss = 5.8193e-05, PNorm = 87.2289, GNorm = 0.0480, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.166686\n",
      "Epoch 49\n",
      "Loss = 5.6395e-05, PNorm = 87.3277, GNorm = 0.0382, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.166608\n",
      "Model 8 best validation mae = 2.155574 on epoch 47\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "Model 8, sample 0 test mae = 2.229792\n",
      "Building model 9\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 6.6558e-03, PNorm = 49.2937, GNorm = 3.3719, lr_0 = 5.5056e-04\n",
      "Validation mae = 16.842222\n",
      "Epoch 1\n",
      "Loss = 3.3503e-03, PNorm = 51.5579, GNorm = 1.3910, lr_0 = 9.9994e-04\n",
      "Validation mae = 10.265408\n",
      "Epoch 2\n",
      "Loss = 2.1584e-03, PNorm = 53.8808, GNorm = 0.5871, lr_0 = 9.5310e-04\n",
      "Validation mae = 6.511291\n",
      "Epoch 3\n",
      "Loss = 1.5759e-03, PNorm = 55.7167, GNorm = 0.6826, lr_0 = 9.0846e-04\n",
      "Validation mae = 6.081521\n",
      "Epoch 4\n",
      "Loss = 1.3805e-03, PNorm = 57.6192, GNorm = 0.4725, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.446875\n",
      "Epoch 5\n",
      "Loss = 1.1295e-03, PNorm = 59.2762, GNorm = 0.6121, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.404009\n",
      "Epoch 6\n",
      "Loss = 1.0509e-03, PNorm = 61.0181, GNorm = 0.2344, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.599259\n",
      "Epoch 7\n",
      "Loss = 1.0059e-03, PNorm = 62.8953, GNorm = 0.3132, lr_0 = 7.4985e-04\n",
      "Validation mae = 4.580604\n",
      "Epoch 8\n",
      "Loss = 8.7640e-04, PNorm = 64.5431, GNorm = 0.2097, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.098629\n",
      "Epoch 9\n",
      "Loss = 7.5509e-04, PNorm = 65.9088, GNorm = 0.3524, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.416964\n",
      "Epoch 10\n",
      "Loss = 7.5739e-04, PNorm = 67.5705, GNorm = 0.2325, lr_0 = 6.4934e-04\n",
      "Validation mae = 3.406387\n",
      "Epoch 11\n",
      "Loss = 6.8728e-04, PNorm = 69.0233, GNorm = 0.2999, lr_0 = 6.1893e-04\n",
      "Validation mae = 3.837716\n",
      "Epoch 12\n",
      "Loss = 6.0218e-04, PNorm = 70.2620, GNorm = 0.6357, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.699769\n",
      "Epoch 13\n",
      "Loss = 5.7609e-04, PNorm = 71.5217, GNorm = 0.4888, lr_0 = 5.6231e-04\n",
      "Validation mae = 4.223189\n",
      "Epoch 14\n",
      "Loss = 5.3828e-04, PNorm = 72.7486, GNorm = 0.2150, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.699865\n",
      "Epoch 15\n",
      "Loss = 4.6717e-04, PNorm = 73.8194, GNorm = 0.2287, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.381475\n",
      "Epoch 16\n",
      "Loss = 4.3925e-04, PNorm = 74.8461, GNorm = 0.4893, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.108640\n",
      "Epoch 17\n",
      "Loss = 4.2121e-04, PNorm = 75.8732, GNorm = 0.1493, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.125598\n",
      "Epoch 18\n",
      "Loss = 4.0625e-04, PNorm = 76.8808, GNorm = 0.1531, lr_0 = 4.4239e-04\n",
      "Validation mae = 2.809872\n",
      "Epoch 19\n",
      "Loss = 3.4456e-04, PNorm = 77.6403, GNorm = 0.1524, lr_0 = 4.2167e-04\n",
      "Validation mae = 2.968815\n",
      "Epoch 20\n",
      "Loss = 3.4104e-04, PNorm = 78.4627, GNorm = 0.2464, lr_0 = 4.0192e-04\n",
      "Validation mae = 3.089207\n",
      "Epoch 21\n",
      "Loss = 2.9598e-04, PNorm = 79.1366, GNorm = 0.1776, lr_0 = 3.8310e-04\n",
      "Validation mae = 2.643021\n",
      "Epoch 22\n",
      "Loss = 2.7353e-04, PNorm = 79.7540, GNorm = 0.1486, lr_0 = 3.6515e-04\n",
      "Validation mae = 2.565519\n",
      "Epoch 23\n",
      "Loss = 2.6517e-04, PNorm = 80.3898, GNorm = 0.1216, lr_0 = 3.4805e-04\n",
      "Validation mae = 2.713708\n",
      "Epoch 24\n",
      "Loss = 2.4405e-04, PNorm = 80.9870, GNorm = 0.1201, lr_0 = 3.3175e-04\n",
      "Validation mae = 2.761687\n",
      "Epoch 25\n",
      "Loss = 2.1937e-04, PNorm = 81.4971, GNorm = 0.1037, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.430306\n",
      "Epoch 26\n",
      "Loss = 2.1881e-04, PNorm = 82.0599, GNorm = 0.0957, lr_0 = 3.0140e-04\n",
      "Validation mae = 2.607852\n",
      "Epoch 27\n",
      "Loss = 1.9912e-04, PNorm = 82.5282, GNorm = 0.1201, lr_0 = 2.8728e-04\n",
      "Validation mae = 2.583840\n",
      "Epoch 28\n",
      "Loss = 1.7317e-04, PNorm = 82.9066, GNorm = 0.1075, lr_0 = 2.7383e-04\n",
      "Validation mae = 2.443866\n",
      "Epoch 29\n",
      "Loss = 1.6099e-04, PNorm = 83.2815, GNorm = 0.1052, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.516098\n",
      "Epoch 30\n",
      "Loss = 1.5562e-04, PNorm = 83.6381, GNorm = 0.0958, lr_0 = 2.4878e-04\n",
      "Validation mae = 2.464188\n",
      "Epoch 31\n",
      "Loss = 1.5868e-04, PNorm = 84.0272, GNorm = 0.0908, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.468365\n",
      "Epoch 32\n",
      "Loss = 1.3460e-04, PNorm = 84.2923, GNorm = 0.0813, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.316571\n",
      "Epoch 33\n",
      "Loss = 1.2294e-04, PNorm = 84.5559, GNorm = 0.0848, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.289734\n",
      "Epoch 34\n",
      "Loss = 1.1679e-04, PNorm = 84.8197, GNorm = 0.0743, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.268247\n",
      "Epoch 35\n",
      "Loss = 1.0937e-04, PNorm = 85.0652, GNorm = 0.1069, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.489185\n",
      "Epoch 36\n",
      "Loss = 1.0446e-04, PNorm = 85.2939, GNorm = 0.0802, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.236464\n",
      "Epoch 37\n",
      "Loss = 9.8664e-05, PNorm = 85.5086, GNorm = 0.0594, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.239794\n",
      "Epoch 38\n",
      "Loss = 9.6961e-05, PNorm = 85.7129, GNorm = 0.0696, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.246140\n",
      "Epoch 39\n",
      "Loss = 8.7196e-05, PNorm = 85.8844, GNorm = 0.1468, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.275302\n",
      "Epoch 40\n",
      "Loss = 8.3681e-05, PNorm = 86.0606, GNorm = 0.0703, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.199181\n",
      "Epoch 41\n",
      "Loss = 8.0345e-05, PNorm = 86.2230, GNorm = 0.0591, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.198547\n",
      "Epoch 42\n",
      "Loss = 7.7601e-05, PNorm = 86.3739, GNorm = 0.0798, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.291721\n",
      "Epoch 43\n",
      "Loss = 7.5422e-05, PNorm = 86.5259, GNorm = 0.0470, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.194239\n",
      "Epoch 44\n",
      "Loss = 7.1782e-05, PNorm = 86.6595, GNorm = 0.0637, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.275469\n",
      "Epoch 45\n",
      "Loss = 6.8182e-05, PNorm = 86.7827, GNorm = 0.0586, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.157059\n",
      "Epoch 46\n",
      "Loss = 6.5804e-05, PNorm = 86.8981, GNorm = 0.0612, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.199356\n",
      "Epoch 47\n",
      "Loss = 6.4624e-05, PNorm = 87.0141, GNorm = 0.0388, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.122701\n",
      "Epoch 48\n",
      "Loss = 6.1571e-05, PNorm = 87.1199, GNorm = 0.0904, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.220065\n",
      "Epoch 49\n",
      "Loss = 5.9690e-05, PNorm = 87.2165, GNorm = 0.0399, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.195974\n",
      "Model 9 best validation mae = 2.122701 on epoch 47\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 9, sample 0 test mae = 2.201112\n",
      "BMA test mae = 1.649702\n"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 10\n",
    "args.samples = 1\n",
    "\n",
    "args.dropout = 0\n",
    "args.test_dropout = False\n",
    "\n",
    "results_ens = run_training(args)\n",
    "np.savez(args.save_dir+'/results_ens', results_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MC-Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have dropout in the following places:\n",
    "# - after the edge update function (following ReLU)\n",
    "# - after converting to an atomic representation (following ReLU)\n",
    "# - on the global hidden state representation\n",
    "# - after every non-final FFN layer (following ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-606c20c4-d37f-4749-82ec-61f9fbcab2fd.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'bias': False,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0.1,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': 12,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 100,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'target_columns': None,\n",
      " 'task_names': ['mu',\n",
      "                'alpha',\n",
      "                'homo',\n",
      "                'lumo',\n",
      "                'gap',\n",
      "                'r2',\n",
      "                'zpve',\n",
      "                'cv',\n",
      "                'u0',\n",
      "                'u298',\n",
      "                'h298',\n",
      "                'g298'],\n",
      " 'test': False,\n",
      " 'test_dropout': True,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': 40000,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49557it [00:02, 21338.78it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 656184.33it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 14079.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.1, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Epoch 0\n",
      "Loss = 7.5858e-03, PNorm = 49.3504, GNorm = 3.0082, lr_0 = 5.5056e-04\n",
      "Validation mae = 13.446315\n",
      "Epoch 1\n",
      "Loss = 4.0954e-03, PNorm = 51.6300, GNorm = 0.9964, lr_0 = 9.9994e-04\n",
      "Validation mae = 10.057201\n",
      "Epoch 2\n",
      "Loss = 2.9970e-03, PNorm = 54.5358, GNorm = 0.7440, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.776005\n",
      "Epoch 3\n",
      "Loss = 2.2652e-03, PNorm = 56.9290, GNorm = 0.9556, lr_0 = 9.0846e-04\n",
      "Validation mae = 7.022837\n",
      "Epoch 4\n",
      "Loss = 1.9826e-03, PNorm = 59.2762, GNorm = 1.0588, lr_0 = 8.6591e-04\n",
      "Validation mae = 5.796475\n",
      "Epoch 5\n",
      "Loss = 1.7752e-03, PNorm = 61.6823, GNorm = 0.4996, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.741067\n",
      "Epoch 6\n",
      "Loss = 1.6339e-03, PNorm = 64.0161, GNorm = 0.3548, lr_0 = 7.8670e-04\n",
      "Validation mae = 5.258753\n",
      "Epoch 7\n",
      "Loss = 1.5303e-03, PNorm = 66.4686, GNorm = 0.3577, lr_0 = 7.4985e-04\n",
      "Validation mae = 5.010161\n",
      "Epoch 8\n",
      "Loss = 1.4267e-03, PNorm = 68.6834, GNorm = 0.4541, lr_0 = 7.1473e-04\n",
      "Validation mae = 5.300825\n",
      "Epoch 9\n",
      "Loss = 1.3039e-03, PNorm = 70.6337, GNorm = 0.5280, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.786556\n",
      "Epoch 10\n",
      "Loss = 1.2394e-03, PNorm = 72.6431, GNorm = 0.2490, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.813303\n",
      "Epoch 11\n",
      "Loss = 1.1971e-03, PNorm = 74.4687, GNorm = 0.2379, lr_0 = 6.1893e-04\n",
      "Validation mae = 4.162317\n",
      "Epoch 12\n",
      "Loss = 1.1045e-03, PNorm = 76.1508, GNorm = 0.3063, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.981535\n",
      "Epoch 13\n",
      "Loss = 1.0578e-03, PNorm = 77.6820, GNorm = 0.6547, lr_0 = 5.6231e-04\n",
      "Validation mae = 4.156255\n",
      "Epoch 14\n",
      "Loss = 1.0220e-03, PNorm = 79.1257, GNorm = 0.2709, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.750582\n",
      "Epoch 15\n",
      "Loss = 9.8921e-04, PNorm = 80.5351, GNorm = 0.3743, lr_0 = 5.1087e-04\n",
      "Validation mae = 3.832463\n",
      "Epoch 16\n",
      "Loss = 9.4485e-04, PNorm = 81.7722, GNorm = 0.1579, lr_0 = 4.8694e-04\n",
      "Validation mae = 3.641896\n",
      "Epoch 17\n",
      "Loss = 8.8724e-04, PNorm = 82.8462, GNorm = 0.3456, lr_0 = 4.6413e-04\n",
      "Validation mae = 3.697264\n",
      "Epoch 18\n",
      "Loss = 8.7332e-04, PNorm = 83.9311, GNorm = 0.2369, lr_0 = 4.4239e-04\n",
      "Validation mae = 3.549443\n",
      "Epoch 19\n",
      "Loss = 8.3035e-04, PNorm = 84.8711, GNorm = 0.2214, lr_0 = 4.2167e-04\n",
      "Validation mae = 3.348709\n",
      "Epoch 20\n",
      "Loss = 8.2538e-04, PNorm = 85.7721, GNorm = 0.1729, lr_0 = 4.0192e-04\n",
      "Validation mae = 3.487225\n",
      "Epoch 21\n",
      "Loss = 7.7535e-04, PNorm = 86.5501, GNorm = 0.3041, lr_0 = 3.8310e-04\n",
      "Validation mae = 3.547803\n",
      "Epoch 22\n",
      "Loss = 7.7892e-04, PNorm = 87.3390, GNorm = 0.2824, lr_0 = 3.6515e-04\n",
      "Validation mae = 3.325092\n",
      "Epoch 23\n",
      "Loss = 7.4443e-04, PNorm = 88.0486, GNorm = 0.1453, lr_0 = 3.4805e-04\n",
      "Validation mae = 3.383015\n",
      "Epoch 24\n",
      "Loss = 7.2271e-04, PNorm = 88.6987, GNorm = 0.1837, lr_0 = 3.3175e-04\n",
      "Validation mae = 3.167016\n",
      "Epoch 25\n",
      "Loss = 7.0229e-04, PNorm = 89.2752, GNorm = 0.1560, lr_0 = 3.1621e-04\n",
      "Validation mae = 2.960335\n",
      "Epoch 26\n",
      "Loss = 6.8079e-04, PNorm = 89.8370, GNorm = 0.2109, lr_0 = 3.0140e-04\n",
      "Validation mae = 3.148222\n",
      "Epoch 27\n",
      "Loss = 6.6951e-04, PNorm = 90.3775, GNorm = 0.4579, lr_0 = 2.8728e-04\n",
      "Validation mae = 3.093933\n",
      "Epoch 28\n",
      "Loss = 6.5893e-04, PNorm = 90.8623, GNorm = 0.1608, lr_0 = 2.7383e-04\n",
      "Validation mae = 3.167569\n",
      "Epoch 29\n",
      "Loss = 6.3727e-04, PNorm = 91.3026, GNorm = 0.1379, lr_0 = 2.6100e-04\n",
      "Validation mae = 2.951265\n",
      "Epoch 30\n",
      "Loss = 6.3677e-04, PNorm = 91.7423, GNorm = 0.1994, lr_0 = 2.4878e-04\n",
      "Validation mae = 3.041016\n",
      "Epoch 31\n",
      "Loss = 6.2723e-04, PNorm = 92.1585, GNorm = 0.1231, lr_0 = 2.3712e-04\n",
      "Validation mae = 2.840471\n",
      "Epoch 32\n",
      "Loss = 6.0387e-04, PNorm = 92.5082, GNorm = 0.1752, lr_0 = 2.2602e-04\n",
      "Validation mae = 2.828313\n",
      "Epoch 33\n",
      "Loss = 5.9740e-04, PNorm = 92.8545, GNorm = 0.1920, lr_0 = 2.1543e-04\n",
      "Validation mae = 2.895042\n",
      "Epoch 34\n",
      "Loss = 5.8705e-04, PNorm = 93.1866, GNorm = 0.1622, lr_0 = 2.0534e-04\n",
      "Validation mae = 2.840605\n",
      "Epoch 35\n",
      "Loss = 5.7708e-04, PNorm = 93.4825, GNorm = 0.1792, lr_0 = 1.9572e-04\n",
      "Validation mae = 2.816061\n",
      "Epoch 36\n",
      "Loss = 5.6728e-04, PNorm = 93.7438, GNorm = 0.1254, lr_0 = 1.8656e-04\n",
      "Validation mae = 2.860787\n",
      "Epoch 37\n",
      "Loss = 5.6354e-04, PNorm = 94.0237, GNorm = 0.1339, lr_0 = 1.7782e-04\n",
      "Validation mae = 2.749376\n",
      "Epoch 38\n",
      "Loss = 5.5132e-04, PNorm = 94.2537, GNorm = 0.1406, lr_0 = 1.6949e-04\n",
      "Validation mae = 2.725568\n",
      "Epoch 39\n",
      "Loss = 5.4867e-04, PNorm = 94.4759, GNorm = 0.1900, lr_0 = 1.6155e-04\n",
      "Validation mae = 2.737585\n",
      "Epoch 40\n",
      "Loss = 5.4248e-04, PNorm = 94.6988, GNorm = 0.1715, lr_0 = 1.5398e-04\n",
      "Validation mae = 2.718772\n",
      "Epoch 41\n",
      "Loss = 5.2773e-04, PNorm = 94.8868, GNorm = 0.1210, lr_0 = 1.4677e-04\n",
      "Validation mae = 2.695119\n",
      "Epoch 42\n",
      "Loss = 5.2436e-04, PNorm = 95.0791, GNorm = 0.1545, lr_0 = 1.3990e-04\n",
      "Validation mae = 2.662922\n",
      "Epoch 43\n",
      "Loss = 5.2209e-04, PNorm = 95.2440, GNorm = 0.1319, lr_0 = 1.3334e-04\n",
      "Validation mae = 2.677963\n",
      "Epoch 44\n",
      "Loss = 5.1896e-04, PNorm = 95.4106, GNorm = 0.1535, lr_0 = 1.2710e-04\n",
      "Validation mae = 2.641539\n",
      "Epoch 45\n",
      "Loss = 5.1247e-04, PNorm = 95.5672, GNorm = 0.1295, lr_0 = 1.2115e-04\n",
      "Validation mae = 2.678835\n",
      "Epoch 46\n",
      "Loss = 5.0772e-04, PNorm = 95.7167, GNorm = 0.1439, lr_0 = 1.1547e-04\n",
      "Validation mae = 2.631534\n",
      "Epoch 47\n",
      "Loss = 5.0031e-04, PNorm = 95.8531, GNorm = 0.1743, lr_0 = 1.1006e-04\n",
      "Validation mae = 2.646786\n",
      "Epoch 48\n",
      "Loss = 5.0135e-04, PNorm = 95.9696, GNorm = 0.3545, lr_0 = 1.0491e-04\n",
      "Validation mae = 2.587892\n",
      "Epoch 49\n",
      "Loss = 4.9447e-04, PNorm = 96.0932, GNorm = 0.1572, lr_0 = 1.0000e-04\n",
      "Validation mae = 2.661126\n",
      "Model 0 best validation mae = 2.587892 on epoch 48\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "Model 0, sample 0 test mae = 4.064212\n",
      "Model 0, sample 1 test mae = 4.021729\n",
      "Model 0, sample 2 test mae = 4.016767\n",
      "Model 0, sample 3 test mae = 4.011306\n",
      "Model 0, sample 4 test mae = 4.007742\n",
      "Model 0, sample 5 test mae = 4.020288\n",
      "Model 0, sample 6 test mae = 4.019098\n",
      "Model 0, sample 7 test mae = 4.032306\n",
      "Model 0, sample 8 test mae = 3.964633\n",
      "Model 0, sample 9 test mae = 4.038456\n",
      "Model 0, sample 10 test mae = 4.087163\n",
      "Model 0, sample 11 test mae = 3.995029\n",
      "Model 0, sample 12 test mae = 4.009700\n",
      "Model 0, sample 13 test mae = 4.015004\n",
      "Model 0, sample 14 test mae = 4.041546\n",
      "Model 0, sample 15 test mae = 4.040813\n",
      "Model 0, sample 16 test mae = 3.989595\n",
      "Model 0, sample 17 test mae = 4.042810\n",
      "Model 0, sample 18 test mae = 3.999321\n",
      "Model 0, sample 19 test mae = 3.983053\n",
      "Model 0, sample 20 test mae = 4.008615\n",
      "Model 0, sample 21 test mae = 4.041050\n",
      "Model 0, sample 22 test mae = 4.015001\n",
      "Model 0, sample 23 test mae = 4.026292\n",
      "Model 0, sample 24 test mae = 4.024662\n",
      "Model 0, sample 25 test mae = 4.023476\n",
      "Model 0, sample 26 test mae = 3.975242\n",
      "Model 0, sample 27 test mae = 4.029605\n",
      "Model 0, sample 28 test mae = 4.038786\n",
      "Model 0, sample 29 test mae = 3.966207\n",
      "Model 0, sample 30 test mae = 4.022801\n",
      "Model 0, sample 31 test mae = 4.022918\n",
      "Model 0, sample 32 test mae = 4.027970\n",
      "Model 0, sample 33 test mae = 4.000878\n",
      "Model 0, sample 34 test mae = 4.037446\n",
      "Model 0, sample 35 test mae = 3.975263\n",
      "Model 0, sample 36 test mae = 4.021287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0, sample 37 test mae = 4.014528\n",
      "Model 0, sample 38 test mae = 3.997958\n",
      "Model 0, sample 39 test mae = 4.049934\n",
      "Model 0, sample 40 test mae = 4.039319\n",
      "Model 0, sample 41 test mae = 4.014362\n",
      "Model 0, sample 42 test mae = 4.002108\n",
      "Model 0, sample 43 test mae = 4.045861\n",
      "Model 0, sample 44 test mae = 3.946239\n",
      "Model 0, sample 45 test mae = 4.067015\n",
      "Model 0, sample 46 test mae = 4.006694\n",
      "Model 0, sample 47 test mae = 4.009199\n",
      "Model 0, sample 48 test mae = 4.060829\n",
      "Model 0, sample 49 test mae = 3.983080\n",
      "Model 0, sample 50 test mae = 4.037095\n",
      "Model 0, sample 51 test mae = 4.015831\n",
      "Model 0, sample 52 test mae = 4.056687\n",
      "Model 0, sample 53 test mae = 4.061595\n",
      "Model 0, sample 54 test mae = 4.006738\n",
      "Model 0, sample 55 test mae = 3.985277\n",
      "Model 0, sample 56 test mae = 4.009133\n",
      "Model 0, sample 57 test mae = 3.993809\n",
      "Model 0, sample 58 test mae = 3.984574\n",
      "Model 0, sample 59 test mae = 4.069018\n",
      "Model 0, sample 60 test mae = 4.003198\n",
      "Model 0, sample 61 test mae = 4.027813\n",
      "Model 0, sample 62 test mae = 4.031415\n",
      "Model 0, sample 63 test mae = 4.057395\n",
      "Model 0, sample 64 test mae = 4.002590\n",
      "Model 0, sample 65 test mae = 4.027206\n",
      "Model 0, sample 66 test mae = 4.022242\n",
      "Model 0, sample 67 test mae = 4.049479\n",
      "Model 0, sample 68 test mae = 4.031260\n",
      "Model 0, sample 69 test mae = 3.994252\n",
      "Model 0, sample 70 test mae = 4.044653\n",
      "Model 0, sample 71 test mae = 4.004281\n",
      "Model 0, sample 72 test mae = 4.004568\n",
      "Model 0, sample 73 test mae = 4.060915\n",
      "Model 0, sample 74 test mae = 4.021369\n",
      "Model 0, sample 75 test mae = 4.042246\n",
      "Model 0, sample 76 test mae = 4.012181\n",
      "Model 0, sample 77 test mae = 3.999880\n",
      "Model 0, sample 78 test mae = 4.022709\n",
      "Model 0, sample 79 test mae = 4.026598\n",
      "Model 0, sample 80 test mae = 4.012391\n",
      "Model 0, sample 81 test mae = 4.002339\n",
      "Model 0, sample 82 test mae = 4.035636\n",
      "Model 0, sample 83 test mae = 4.073350\n",
      "Model 0, sample 84 test mae = 3.991792\n",
      "Model 0, sample 85 test mae = 3.949360\n",
      "Model 0, sample 86 test mae = 4.035870\n",
      "Model 0, sample 87 test mae = 3.994638\n",
      "Model 0, sample 88 test mae = 4.021517\n",
      "Model 0, sample 89 test mae = 3.997207\n",
      "Model 0, sample 90 test mae = 4.000658\n",
      "Model 0, sample 91 test mae = 4.001469\n",
      "Model 0, sample 92 test mae = 3.996904\n",
      "Model 0, sample 93 test mae = 3.999444\n",
      "Model 0, sample 94 test mae = 3.997601\n",
      "Model 0, sample 95 test mae = 4.027917\n",
      "Model 0, sample 96 test mae = 3.999764\n",
      "Model 0, sample 97 test mae = 3.977254\n",
      "Model 0, sample 98 test mae = 4.053245\n",
      "Model 0, sample 99 test mae = 4.036947\n",
      "BMA test mae = 2.631316\n"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 100\n",
    "\n",
    "args.dropout = 0.1\n",
    "args.test_dropout = True\n",
    "\n",
    "results_MCdrop = run_training(args)\n",
    "np.savez(args.save_dir+'/results_MCdrop', results_MCdrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG-Diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-eec2b1f7-f2f6-4a68-beac-8aee973255b6.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'bias': False,\n",
      " 'block': False,\n",
      " 'c_swag': 200,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_swag': 25,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise': 0,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'lr_sgld': 0.001,\n",
      " 'lr_swag': 0.001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 30,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'momentum_swag': 0.5,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'norm_sigma_sgld': 100,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': None,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 30,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': True,\n",
      " 'target_columns': None,\n",
      " 'task_names': None,\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': None,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0.001}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47074it [00:00, 79154.59it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 259260.70it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 13420.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Model 0 best validation mae = inf on epoch 0\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.2671e-05, PNorm = 87.4527, GNorm = 0.0434, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141925\n",
      "SWAG spoch 1\n",
      "Loss = 5.1794e-05, PNorm = 87.3134, GNorm = 0.0335, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138139\n",
      "SWAG spoch 2\n",
      "Loss = 5.1467e-05, PNorm = 87.1742, GNorm = 0.0327, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135514\n",
      "SWAG spoch 3\n",
      "Loss = 5.1343e-05, PNorm = 87.0355, GNorm = 0.0264, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132543\n",
      "SWAG spoch 4\n",
      "Loss = 5.1268e-05, PNorm = 86.8971, GNorm = 0.0294, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131398\n",
      "SWAG spoch 5\n",
      "Loss = 5.1320e-05, PNorm = 86.7590, GNorm = 0.0304, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130107\n",
      "SWAG spoch 6\n",
      "Loss = 5.1412e-05, PNorm = 86.6211, GNorm = 0.0297, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129003\n",
      "SWAG spoch 7\n",
      "Loss = 5.1565e-05, PNorm = 86.4835, GNorm = 0.0389, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127970\n",
      "SWAG spoch 8\n",
      "Loss = 5.1738e-05, PNorm = 86.3459, GNorm = 0.0407, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127302\n",
      "SWAG spoch 9\n",
      "Loss = 5.1942e-05, PNorm = 86.2089, GNorm = 0.0418, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127138\n",
      "SWAG spoch 10\n",
      "Loss = 5.2183e-05, PNorm = 86.0718, GNorm = 0.0336, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.126791\n",
      "SWAG spoch 11\n",
      "Loss = 5.2442e-05, PNorm = 85.9350, GNorm = 0.0334, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127004\n",
      "SWAG spoch 12\n",
      "Loss = 5.2726e-05, PNorm = 85.7986, GNorm = 0.0406, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127079\n",
      "SWAG spoch 13\n",
      "Loss = 5.3030e-05, PNorm = 85.6623, GNorm = 0.0331, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127515\n",
      "SWAG spoch 14\n",
      "Loss = 5.3359e-05, PNorm = 85.5266, GNorm = 0.0346, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127970\n",
      "SWAG spoch 15\n",
      "Loss = 5.3692e-05, PNorm = 85.3908, GNorm = 0.0499, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.128943\n",
      "SWAG spoch 16\n",
      "Loss = 5.4070e-05, PNorm = 85.2553, GNorm = 0.0376, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129344\n",
      "SWAG spoch 17\n",
      "Loss = 5.4428e-05, PNorm = 85.1202, GNorm = 0.0355, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130124\n",
      "SWAG spoch 18\n",
      "Loss = 5.4811e-05, PNorm = 84.9849, GNorm = 0.0464, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131290\n",
      "SWAG spoch 19\n",
      "Loss = 5.5201e-05, PNorm = 84.8503, GNorm = 0.0346, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132690\n",
      "SWAG spoch 20\n",
      "Loss = 5.5624e-05, PNorm = 84.7155, GNorm = 0.0302, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.133645\n",
      "SWAG spoch 21\n",
      "Loss = 5.6057e-05, PNorm = 84.5810, GNorm = 0.0584, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.134905\n",
      "SWAG spoch 22\n",
      "Loss = 5.6494e-05, PNorm = 84.4470, GNorm = 0.0435, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135948\n",
      "SWAG spoch 23\n",
      "Loss = 5.6962e-05, PNorm = 84.3132, GNorm = 0.0350, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.136968\n",
      "SWAG spoch 24\n",
      "Loss = 5.7404e-05, PNorm = 84.1795, GNorm = 0.0363, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138221\n",
      "Model 0, sample 0 test mae = 2.215999\n",
      "Model 0, sample 1 test mae = 2.211334\n",
      "Model 0, sample 2 test mae = 2.220151\n",
      "Model 0, sample 3 test mae = 2.219173\n",
      "Model 0, sample 4 test mae = 2.205197\n",
      "Model 0, sample 5 test mae = 2.213687\n",
      "Model 0, sample 6 test mae = 2.235530\n",
      "Model 0, sample 7 test mae = 2.213811\n",
      "Model 0, sample 8 test mae = 2.216789\n",
      "Model 0, sample 9 test mae = 2.219743\n",
      "Model 0, sample 10 test mae = 2.223194\n",
      "Model 0, sample 11 test mae = 2.205754\n",
      "Model 0, sample 12 test mae = 2.214806\n",
      "Model 0, sample 13 test mae = 2.220438\n",
      "Model 0, sample 14 test mae = 2.214931\n",
      "Model 0, sample 15 test mae = 2.221605\n",
      "Model 0, sample 16 test mae = 2.212378\n",
      "Model 0, sample 17 test mae = 2.216015\n",
      "Model 0, sample 18 test mae = 2.218594\n",
      "Model 0, sample 19 test mae = 2.229232\n",
      "Model 0, sample 20 test mae = 2.217281\n",
      "Model 0, sample 21 test mae = 2.211613\n",
      "Model 0, sample 22 test mae = 2.220104\n",
      "Model 0, sample 23 test mae = 2.217689\n",
      "Model 0, sample 24 test mae = 2.210539\n",
      "Model 0, sample 25 test mae = 2.213517\n",
      "Model 0, sample 26 test mae = 2.214669\n",
      "Model 0, sample 27 test mae = 2.216382\n",
      "Model 0, sample 28 test mae = 2.210967\n",
      "Model 0, sample 29 test mae = 2.212941\n",
      "BMA test mae = 2.205125\n"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 30\n",
    "args.dropout = 0\n",
    "args.test_dropout = False\n",
    "\n",
    "args.swag = True # SWAG switch\n",
    "args.cov_mat = False # whether to compute deviations and then covariance\n",
    "args.max_num_models = 30 # max number of columns of deviations matrix\n",
    "args.block = False # whether to compute covariances layer by layer\n",
    "\n",
    "args.epochs_swag = 25 # number of epochs\n",
    "args.c_swag = 200 # how frequently to collect a model (in batches)\n",
    "\n",
    "args.lr_swag = 1e-3\n",
    "args.momentum_swag = 0.5\n",
    "args.wd_swag = 0.001\n",
    "\n",
    "results_swagD = run_training(args)\n",
    "np.savez(args.save_dir+'/results_swagD', results_swagD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-eec2b1f7-f2f6-4a68-beac-8aee973255b6.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'bias': False,\n",
      " 'block': False,\n",
      " 'c_swag': 200,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': True,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_swag': 25,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise': 0,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'lr_sgld': 0.001,\n",
      " 'lr_swag': 0.001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 30,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'momentum_swag': 0.5,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'norm_sigma_sgld': 100,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': 12,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 30,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': False,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': True,\n",
      " 'target_columns': None,\n",
      " 'task_names': ['mu',\n",
      "                'alpha',\n",
      "                'homo',\n",
      "                'lumo',\n",
      "                'gap',\n",
      "                'r2',\n",
      "                'zpve',\n",
      "                'cv',\n",
      "                'u0',\n",
      "                'u298',\n",
      "                'h298',\n",
      "                'g298'],\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': 40000,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47263it [00:02, 22363.68it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 227446.72it/s]\n",
      "100%|██████████| 50000/50000 [00:04<00:00, 11755.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,512\n",
      "Model 0 best validation mae = inf on epoch 0\n",
      "Loading pretrained parameter \"encoder.encoder.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.W_o.bias\".\n",
      "Loading pretrained parameter \"ffn.1.weight\".\n",
      "Loading pretrained parameter \"ffn.1.bias\".\n",
      "Loading pretrained parameter \"ffn.4.weight\".\n",
      "Loading pretrained parameter \"ffn.4.bias\".\n",
      "Loading pretrained parameter \"ffn.7.weight\".\n",
      "Loading pretrained parameter \"ffn.7.bias\".\n",
      "----------SWAG training----------\n",
      "SWAG spoch 0\n",
      "Loss = 5.2671e-05, PNorm = 87.4527, GNorm = 0.0434, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.141925\n",
      "SWAG spoch 1\n",
      "Loss = 5.1794e-05, PNorm = 87.3134, GNorm = 0.0335, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138139\n",
      "SWAG spoch 2\n",
      "Loss = 5.1467e-05, PNorm = 87.1742, GNorm = 0.0327, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135514\n",
      "SWAG spoch 3\n",
      "Loss = 5.1343e-05, PNorm = 87.0355, GNorm = 0.0264, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132543\n",
      "SWAG spoch 4\n",
      "Loss = 5.1268e-05, PNorm = 86.8971, GNorm = 0.0294, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131398\n",
      "SWAG spoch 5\n",
      "Loss = 5.1320e-05, PNorm = 86.7590, GNorm = 0.0304, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130107\n",
      "SWAG spoch 6\n",
      "Loss = 5.1412e-05, PNorm = 86.6211, GNorm = 0.0297, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129003\n",
      "SWAG spoch 7\n",
      "Loss = 5.1565e-05, PNorm = 86.4835, GNorm = 0.0389, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127970\n",
      "SWAG spoch 8\n",
      "Loss = 5.1738e-05, PNorm = 86.3459, GNorm = 0.0407, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127302\n",
      "SWAG spoch 9\n",
      "Loss = 5.1942e-05, PNorm = 86.2089, GNorm = 0.0418, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127138\n",
      "SWAG spoch 10\n",
      "Loss = 5.2183e-05, PNorm = 86.0718, GNorm = 0.0336, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.126791\n",
      "SWAG spoch 11\n",
      "Loss = 5.2442e-05, PNorm = 85.9350, GNorm = 0.0334, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127004\n",
      "SWAG spoch 12\n",
      "Loss = 5.2726e-05, PNorm = 85.7986, GNorm = 0.0406, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127079\n",
      "SWAG spoch 13\n",
      "Loss = 5.3030e-05, PNorm = 85.6623, GNorm = 0.0331, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127515\n",
      "SWAG spoch 14\n",
      "Loss = 5.3359e-05, PNorm = 85.5266, GNorm = 0.0346, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.127970\n",
      "SWAG spoch 15\n",
      "Loss = 5.3692e-05, PNorm = 85.3908, GNorm = 0.0499, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.128943\n",
      "SWAG spoch 16\n",
      "Loss = 5.4070e-05, PNorm = 85.2553, GNorm = 0.0376, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.129344\n",
      "SWAG spoch 17\n",
      "Loss = 5.4428e-05, PNorm = 85.1202, GNorm = 0.0355, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.130124\n",
      "SWAG spoch 18\n",
      "Loss = 5.4811e-05, PNorm = 84.9849, GNorm = 0.0464, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.131290\n",
      "SWAG spoch 19\n",
      "Loss = 5.5201e-05, PNorm = 84.8503, GNorm = 0.0346, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.132690\n",
      "SWAG spoch 20\n",
      "Loss = 5.5624e-05, PNorm = 84.7155, GNorm = 0.0302, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.133645\n",
      "SWAG spoch 21\n",
      "Loss = 5.6057e-05, PNorm = 84.5810, GNorm = 0.0584, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.134905\n",
      "SWAG spoch 22\n",
      "Loss = 5.6494e-05, PNorm = 84.4470, GNorm = 0.0435, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.135948\n",
      "SWAG spoch 23\n",
      "Loss = 5.6962e-05, PNorm = 84.3132, GNorm = 0.0350, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.136968\n",
      "SWAG spoch 24\n",
      "Loss = 5.7404e-05, PNorm = 84.1795, GNorm = 0.0363, lr_0 = 1.0000e-03\n",
      "Validation mae = 2.138221\n",
      "Model 0, sample 0 test mae = 2.214307\n",
      "Model 0, sample 1 test mae = 2.215639\n",
      "Model 0, sample 2 test mae = 2.220493\n",
      "Model 0, sample 3 test mae = 2.219553\n",
      "Model 0, sample 4 test mae = 2.228939\n",
      "Model 0, sample 5 test mae = 2.228063\n",
      "Model 0, sample 6 test mae = 2.282192\n",
      "Model 0, sample 7 test mae = 2.215986\n",
      "Model 0, sample 8 test mae = 2.216008\n",
      "Model 0, sample 9 test mae = 2.220143\n",
      "Model 0, sample 10 test mae = 2.226687\n",
      "Model 0, sample 11 test mae = 2.218533\n",
      "Model 0, sample 12 test mae = 2.216789\n",
      "Model 0, sample 13 test mae = 2.227806\n",
      "Model 0, sample 14 test mae = 2.238156\n",
      "Model 0, sample 15 test mae = 2.213463\n",
      "Model 0, sample 16 test mae = 2.211369\n",
      "Model 0, sample 17 test mae = 2.215626\n",
      "Model 0, sample 18 test mae = 2.216234\n",
      "Model 0, sample 19 test mae = 2.208882\n",
      "Model 0, sample 20 test mae = 2.224230\n",
      "Model 0, sample 21 test mae = 2.219651\n",
      "Model 0, sample 22 test mae = 2.219185\n",
      "Model 0, sample 23 test mae = 2.214392\n",
      "Model 0, sample 24 test mae = 2.211128\n",
      "Model 0, sample 25 test mae = 2.210485\n",
      "Model 0, sample 26 test mae = 2.244593\n",
      "Model 0, sample 27 test mae = 2.269718\n",
      "Model 0, sample 28 test mae = 2.213902\n",
      "Model 0, sample 29 test mae = 2.218234\n",
      "BMA test mae = 2.204640\n"
     ]
    }
   ],
   "source": [
    "args.ensemble_size = 1\n",
    "args.samples = 30\n",
    "args.dropout = 0\n",
    "args.test_dropout = False\n",
    "\n",
    "args.swag = True # SWAG switch\n",
    "args.cov_mat = True # whether to compute deviations and then covariance\n",
    "args.max_num_models = 30 # max number of columns of deviations matrix\n",
    "args.block = False # whether to compute covariances layer by layer\n",
    "\n",
    "args.epochs_swag = 25 # number of epochs\n",
    "args.c_swag = 200 # how frequently to collect a model (in batches)\n",
    "\n",
    "args.lr_swag = 1e-3\n",
    "args.momentum_swag = 0.5\n",
    "args.wd_swag = 0.001\n",
    "\n",
    "results_swag = run_training(args)\n",
    "np.savez(args.save_dir+'/results_swag', results_swag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python /Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/georgelamb/Library/Jupyter/runtime/kernel-832232a1-7bee-48f8-801a-d6a9460319ca.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'atom_messages': False,\n",
      " 'batch_size_sgld': 200,\n",
      " 'bias': False,\n",
      " 'block': True,\n",
      " 'burnin_epochs': 10,\n",
      " 'c_swag': 0,\n",
      " 'cache_cutoff': 10000,\n",
      " 'class_balance': False,\n",
      " 'config_path': None,\n",
      " 'cov_mat': False,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'data_path': '/Users/georgelamb/Documents/GitHub/chempropBayes/data/QM9.csv',\n",
      " 'dataset_type': 'regression',\n",
      " 'depth': 5,\n",
      " 'dropout': 0,\n",
      " 'ensemble_size': 1,\n",
      " 'epochs': 50,\n",
      " 'epochs_swag': 0,\n",
      " 'features_generator': None,\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 500,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'hidden_size': 500,\n",
      " 'init_log_noise': -2,\n",
      " 'init_lr': 0.0001,\n",
      " 'log_frequency': 800,\n",
      " 'log_frequency_sgld': 800.0,\n",
      " 'lr_sgld': 0.0001,\n",
      " 'lr_swag': 0.0001,\n",
      " 'max_data_size': 50000,\n",
      " 'max_lr': 0.001,\n",
      " 'max_num_models': 0,\n",
      " 'metric': 'mae',\n",
      " 'minimize_score': True,\n",
      " 'mix_epochs': 1,\n",
      " 'momentum_swag': 0.9,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'norm_sigma_sgld': 1,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': 12,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'samples': 30,\n",
      " 'save_dir': '/Users/georgelamb/Documents/GitHub/chempropBayes/log',\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'sgld': True,\n",
      " 'show_individual_scores': False,\n",
      " 'split_sizes': (0.8, 0.1, 0.1),\n",
      " 'split_type': 'random',\n",
      " 'swag': False,\n",
      " 'target_columns': None,\n",
      " 'task_names': ['mu',\n",
      "                'alpha',\n",
      "                'homo',\n",
      "                'lumo',\n",
      "                'gap',\n",
      "                'r2',\n",
      "                'zpve',\n",
      "                'cv',\n",
      "                'u0',\n",
      "                'u298',\n",
      "                'h298',\n",
      "                'g298'],\n",
      " 'test': False,\n",
      " 'test_dropout': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': 40000,\n",
      " 'undirected': False,\n",
      " 'use_input_features': False,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'wd_swag': 0}\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43202it [00:00, 49844.64it/s] \n",
      "100%|██████████| 50000/50000 [00:00<00:00, 126728.31it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 12697.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 12\n",
      "Splitting data with seed 0\n",
      "Total size = 50,000 | train size = 40,000 | val size = 5,000 | test size = 5,000\n",
      "Fitting scaler\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (encoder): MPN(\n",
      "    (encoder): MPNEncoder(\n",
      "      (dropout_layer): Dropout(p=0, inplace=False)\n",
      "      (act_func): ReLU()\n",
      "      (W_i): Linear(in_features=147, out_features=500, bias=False)\n",
      "      (W_h): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (W_o): Linear(in_features=633, out_features=500, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ffn): Sequential(\n",
      "    (0): Dropout(p=0, inplace=False)\n",
      "    (1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0, inplace=False)\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0, inplace=False)\n",
      "    (7): Linear(in_features=500, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 1,147,524\n",
      "Epoch 0\n",
      "Loss = 6.6179e-03, PNorm = 49.8341, GNorm = 3.9430, lr_0 = 5.5056e-04\n",
      "Validation mae = 12.860934\n",
      "Epoch 1\n",
      "Loss = 3.3376e-03, PNorm = 52.0018, GNorm = 1.6459, lr_0 = 9.9994e-04\n",
      "Validation mae = 9.169757\n",
      "Epoch 2\n",
      "Loss = 2.2128e-03, PNorm = 54.4513, GNorm = 0.6652, lr_0 = 9.5310e-04\n",
      "Validation mae = 7.177531\n",
      "Epoch 3\n",
      "Loss = 1.6222e-03, PNorm = 56.3347, GNorm = 0.8129, lr_0 = 9.0846e-04\n",
      "Validation mae = 7.748067\n",
      "Epoch 4\n",
      "Loss = 1.4080e-03, PNorm = 58.3006, GNorm = 0.7858, lr_0 = 8.6591e-04\n",
      "Validation mae = 7.995050\n",
      "Epoch 5\n",
      "Loss = 1.1951e-03, PNorm = 60.1018, GNorm = 0.9493, lr_0 = 8.2535e-04\n",
      "Validation mae = 5.806514\n",
      "Epoch 6\n",
      "Loss = 1.0894e-03, PNorm = 61.8980, GNorm = 0.3030, lr_0 = 7.8670e-04\n",
      "Validation mae = 4.281439\n",
      "Epoch 7\n",
      "Loss = 1.0005e-03, PNorm = 63.6914, GNorm = 0.5665, lr_0 = 7.4985e-04\n",
      "Validation mae = 5.099877\n",
      "Epoch 8\n",
      "Loss = 8.9264e-04, PNorm = 65.2842, GNorm = 0.3422, lr_0 = 7.1473e-04\n",
      "Validation mae = 4.292120\n",
      "Epoch 9\n",
      "Loss = 7.8944e-04, PNorm = 66.7204, GNorm = 0.2399, lr_0 = 6.8125e-04\n",
      "Validation mae = 4.346213\n",
      "Epoch 10\n",
      "Loss = 7.6237e-04, PNorm = 68.3488, GNorm = 0.5543, lr_0 = 6.4934e-04\n",
      "Validation mae = 4.487941\n",
      "Epoch 11\n",
      "Loss = 6.6571e-04, PNorm = 69.7204, GNorm = 0.2699, lr_0 = 6.1893e-04\n",
      "Validation mae = 4.052892\n",
      "Epoch 12\n",
      "Loss = 5.9497e-04, PNorm = 70.9407, GNorm = 0.3558, lr_0 = 5.8994e-04\n",
      "Validation mae = 3.280318\n",
      "Epoch 13\n",
      "Loss = 5.4275e-04, PNorm = 72.1894, GNorm = 0.1506, lr_0 = 5.6231e-04\n",
      "Validation mae = 3.238623\n",
      "Epoch 14\n",
      "Loss = 5.5829e-04, PNorm = 73.5758, GNorm = 0.1540, lr_0 = 5.3597e-04\n",
      "Validation mae = 3.346532\n",
      "Epoch 15\n"
     ]
    }
   ],
   "source": [
    "# initial training\n",
    "args.epochs = 50\n",
    "args.ensemble_size = 1\n",
    "\n",
    "args.dropout = 0\n",
    "args.test_dropout = False\n",
    "args.swag = False\n",
    "\n",
    "\n",
    "\n",
    "# sgld\n",
    "args.samples = 30\n",
    "args.sgld = True\n",
    "\n",
    "args.init_log_noise = -2\n",
    "args.lr_sgld = 1e-4\n",
    "args.norm_sigma_sgld = 1\n",
    "\n",
    "args.batch_size_sgld = 200\n",
    "args.log_frequency_sgld = 40000/args.batch_size\n",
    "\n",
    "args.burnin_epochs = 10\n",
    "args.mix_epochs = 1\n",
    "\n",
    "results_sgld = run_training(args)\n",
    "np.savez(args.save_dir+'/results_sgld', results_sgld)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collated results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load npz files\n",
    "results_MAP = np.load(args.save_dir+'/results_MAP.npz')['arr_0']\n",
    "results_ens = np.load(args.save_dir+'/results_ens.npz')['arr_0']\n",
    "results_MCdrop = np.load(args.save_dir+'/results_MCdrop.npz')['arr_0']\n",
    "results_swagD = np.load(args.save_dir+'/results_swagD.npz')['arr_0']\n",
    "results_swag = np.load(args.save_dir+'/results_swag.npz')['arr_0']\n",
    "results_sgld = np.load(args.save_dir+'/results_sgld.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38638it [00:01, 19545.14it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 148984.71it/s]\n",
      "100%|██████████| 50000/50000 [00:03<00:00, 13130.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# compute MAEs for test set\n",
    "data = get_data(path=args.data_path, args=args)\n",
    "_, _, test_data = split_data(data=data, split_type=args.split_type, \n",
    "                                             sizes=args.split_sizes, seed=args.seed, args=args, logger=None)\n",
    "test_targets = np.array(test_data.targets())\n",
    "\n",
    "target_means = np.mean(test_targets,0)\n",
    "MAE = np.mean(abs(test_targets - target_means),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row and column names\n",
    "row_names = get_task_names(args.data_path)+['AVG']\n",
    "col_names = ['MAP',\n",
    "             'MAP Ens',\n",
    "             'MC-drop',\n",
    "             'swagD',\n",
    "             'swag',\n",
    "             'sgld']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP Ens</th>\n",
       "      <th>MC-drop</th>\n",
       "      <th>swagD</th>\n",
       "      <th>swag</th>\n",
       "      <th>sgld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>0.38770</td>\n",
       "      <td>0.33606</td>\n",
       "      <td>0.38473</td>\n",
       "      <td>0.38317</td>\n",
       "      <td>0.38332</td>\n",
       "      <td>0.38655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>0.47779</td>\n",
       "      <td>0.34766</td>\n",
       "      <td>0.58186</td>\n",
       "      <td>0.47650</td>\n",
       "      <td>0.47662</td>\n",
       "      <td>0.47703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homo</th>\n",
       "      <td>0.00344</td>\n",
       "      <td>0.00278</td>\n",
       "      <td>0.00368</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.00339</td>\n",
       "      <td>0.00342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lumo</th>\n",
       "      <td>0.00342</td>\n",
       "      <td>0.00267</td>\n",
       "      <td>0.00379</td>\n",
       "      <td>0.00338</td>\n",
       "      <td>0.00337</td>\n",
       "      <td>0.00339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gap</th>\n",
       "      <td>0.00455</td>\n",
       "      <td>0.00363</td>\n",
       "      <td>0.00491</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00448</td>\n",
       "      <td>0.00451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>20.57770</td>\n",
       "      <td>15.85658</td>\n",
       "      <td>23.47600</td>\n",
       "      <td>20.25198</td>\n",
       "      <td>20.24541</td>\n",
       "      <td>20.40927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zpve</th>\n",
       "      <td>0.00112</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.00147</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>0.00108</td>\n",
       "      <td>0.00110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv</th>\n",
       "      <td>0.22297</td>\n",
       "      <td>0.15688</td>\n",
       "      <td>0.26568</td>\n",
       "      <td>0.22096</td>\n",
       "      <td>0.22068</td>\n",
       "      <td>0.22174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u0</th>\n",
       "      <td>1.30714</td>\n",
       "      <td>0.77275</td>\n",
       "      <td>1.71336</td>\n",
       "      <td>1.28109</td>\n",
       "      <td>1.28308</td>\n",
       "      <td>1.29755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u298</th>\n",
       "      <td>1.30441</td>\n",
       "      <td>0.77204</td>\n",
       "      <td>1.71331</td>\n",
       "      <td>1.27842</td>\n",
       "      <td>1.27763</td>\n",
       "      <td>1.29235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h298</th>\n",
       "      <td>1.29865</td>\n",
       "      <td>0.77324</td>\n",
       "      <td>1.71370</td>\n",
       "      <td>1.27736</td>\n",
       "      <td>1.27577</td>\n",
       "      <td>1.29219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g298</th>\n",
       "      <td>1.30281</td>\n",
       "      <td>0.77154</td>\n",
       "      <td>1.71330</td>\n",
       "      <td>1.27969</td>\n",
       "      <td>1.28084</td>\n",
       "      <td>1.29713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG</th>\n",
       "      <td>2.24097</td>\n",
       "      <td>1.64970</td>\n",
       "      <td>2.63132</td>\n",
       "      <td>2.20513</td>\n",
       "      <td>2.20464</td>\n",
       "      <td>2.22385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            MAP   MAP Ens   MC-drop     swagD      swag      sgld\n",
       "mu      0.38770   0.33606   0.38473   0.38317   0.38332   0.38655\n",
       "alpha   0.47779   0.34766   0.58186   0.47650   0.47662   0.47703\n",
       "homo    0.00344   0.00278   0.00368   0.00339   0.00339   0.00342\n",
       "lumo    0.00342   0.00267   0.00379   0.00338   0.00337   0.00339\n",
       "gap     0.00455   0.00363   0.00491   0.00448   0.00448   0.00451\n",
       "r2     20.57770  15.85658  23.47600  20.25198  20.24541  20.40927\n",
       "zpve    0.00112   0.00060   0.00147   0.00109   0.00108   0.00110\n",
       "cv      0.22297   0.15688   0.26568   0.22096   0.22068   0.22174\n",
       "u0      1.30714   0.77275   1.71336   1.28109   1.28308   1.29755\n",
       "u298    1.30441   0.77204   1.71331   1.27842   1.27763   1.29235\n",
       "h298    1.29865   0.77324   1.71370   1.27736   1.27577   1.29219\n",
       "g298    1.30281   0.77154   1.71330   1.27969   1.28084   1.29713\n",
       "AVG     2.24097   1.64970   2.63132   2.20513   2.20464   2.22385"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build df for absolute MAE\n",
    "results = np.array([\n",
    "    results_MAP,\n",
    "    results_ens,\n",
    "    results_MCdrop,\n",
    "    results_swagD,\n",
    "    results_swag,\n",
    "    results_sgld\n",
    "    ]).T\n",
    "averages = np.mean(results,0)\n",
    "df = pd.DataFrame(np.vstack([results,averages]), columns=col_names, index=row_names)\n",
    "df.round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP Ens</th>\n",
       "      <th>MC-drop</th>\n",
       "      <th>swagD</th>\n",
       "      <th>swag</th>\n",
       "      <th>sgld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>0.313</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>0.072</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homo</th>\n",
       "      <td>0.178</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lumo</th>\n",
       "      <td>0.090</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gap</th>\n",
       "      <td>0.115</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.116</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zpve</th>\n",
       "      <td>0.048</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv</th>\n",
       "      <td>0.084</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u0</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u298</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h298</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g298</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG</th>\n",
       "      <td>0.097</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         MAP  MAP Ens  MC-drop  swagD   swag   sgld\n",
       "mu     0.313    0.272    0.311  0.310  0.310  0.313\n",
       "alpha  0.072    0.052    0.087  0.071  0.071  0.072\n",
       "homo   0.178    0.144    0.191  0.176  0.176  0.177\n",
       "lumo   0.090    0.070    0.100  0.089  0.089  0.089\n",
       "gap    0.115    0.092    0.124  0.113  0.113  0.114\n",
       "r2     0.116    0.089    0.132  0.114  0.114  0.115\n",
       "zpve   0.048    0.026    0.063  0.047  0.047  0.047\n",
       "cv     0.084    0.059    0.100  0.083  0.083  0.083\n",
       "u0     0.038    0.023    0.050  0.037  0.037  0.038\n",
       "u298   0.038    0.022    0.050  0.037  0.037  0.038\n",
       "h298   0.038    0.023    0.050  0.037  0.037  0.038\n",
       "g298   0.038    0.022    0.050  0.037  0.037  0.038\n",
       "AVG    0.097    0.074    0.109  0.096  0.096  0.097"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build df for [MAE] / [MAE of test set]\n",
    "results_relative = results / np.array([MAE]).T\n",
    "averages_relative = np.mean(results_relative,0)\n",
    "df = pd.DataFrame(np.vstack([results_relative, averages_relative]), columns=col_names, index=row_names)\n",
    "df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
